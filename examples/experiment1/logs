/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousing:
    def __init__(n_examples, is_train):
        self.n_examples = n_examples
        self.is_train = is_train

        data = datasets.load_diabetes(return_X_y=True)

        if is_train:
            self.x = data[0][:self.n_examples, :]
            self.y = data[1][:self.n_examples]

        else:
            self.x = data[0][self.n_examples:, :]
            self.y = data[1][self.n_examples:]

    def __len__(self):
        return n_examples if is_train else 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousing(test_train_split, is_train=True),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousing(test_train_split, is_train=False),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hypernet_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousing:
    def __init__(n_examples, is_train):
        self.n_examples = n_examples
        self.is_train = is_train

        data = datasets.load_diabetes(return_X_y=True)

        if is_train:
            self.x = data[0][:self.n_examples, :]
            self.y = data[1][:self.n_examples]

        else:
            self.x = data[0][self.n_examples:, :]
            self.y = data[1][self.n_examples:]

    def __len__(self):
        return n_examples if is_train else 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousing(test_train_split, is_train=True),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousing(test_train_split, is_train=False),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(n_examples, is_train):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device).float()
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x))

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        pdb.set_trace()

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = loss(model, train_loader)
                test_loss = loss(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(model, train_loader)
                test_loss = compute_loss(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        criterion = nn.MSELoss().to(device)
        loss = criterion(model(x), y)

    return loss


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(model, train_loader)
                test_loss = compute_loss(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        mse_loss = nn.MSELoss().to(device)

    return mse_loss(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(model, train_loader)
                test_loss = compute_loss(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

    return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = torch.flatten(x).float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = np.reshape(x, -1).float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		print(x)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	pdb.set_trace()
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	pdb.set_trace()
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x)
		print(y)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x.shape)
		print(y.shape)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x.shape)
		print(y.shape)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x.shape)
		print(y.shape)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                print('Test')
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x.shape)
		print(y.shape)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.067 (0.067) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 355.6541 | Test Loss 56.1850
Epoch 0001 | Time 0.175 (0.071) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 597.8273 | Test Loss 1137.5107
Epoch 0002 | Time 0.202 (0.076) | NFE-F 25.8 | NFE-B 0.0 | Train Loss 739.0802 | Test Loss 1290.8634
Epoch 0003 | Time 0.214 (0.082) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 481.4865 | Test Loss 910.8774
Epoch 0004 | Time 0.237 (0.088) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 2860.4312 | Test Loss 1941.5103
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.065 (0.065) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 867.8470 | Test Loss 421.5237
Epoch 0001 | Time 2.964 (0.106) | NFE-F 40.7 | NFE-B 0.0 | Train Loss 6213.5518 | Test Loss 4903.3657
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.064 (0.064) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 824.1253 | Test Loss 312.4105
Epoch 0001 | Time 0.115 (0.066) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 83.2693 | Test Loss 219.5147
Epoch 0002 | Time 0.126 (0.069) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 435.5676 | Test Loss 844.2549
Epoch 0003 | Time 0.132 (0.071) | NFE-F 25.3 | NFE-B 0.0 | Train Loss 105.2414 | Test Loss 48.7483
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.068 (0.068) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 589.0634 | Test Loss 236.9714
Epoch 0001 | Time 0.063 (0.069) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 387.6355 | Test Loss 118.9518
Epoch 0002 | Time 0.069 (0.069) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 241.1600 | Test Loss 31.4426
Epoch 0003 | Time 0.083 (0.069) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 70.5097 | Test Loss 67.2300
Epoch 0004 | Time 0.087 (0.070) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 86.6430 | Test Loss 208.5830
Epoch 0005 | Time 0.077 (0.071) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 138.9497 | Test Loss 305.8057
Epoch 0006 | Time 0.082 (0.071) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 88.7137 | Test Loss 221.3272
Epoch 0007 | Time 0.077 (0.072) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 85.4428 | Test Loss 200.9950
Epoch 0008 | Time 0.077 (0.073) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 74.2993 | Test Loss 161.9343
Epoch 0009 | Time 0.083 (0.073) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 85.5876 | Test Loss 126.5085
Epoch 0010 | Time 0.086 (0.074) | NFE-F 25.5 | NFE-B 0.0 | Train Loss 65.7968 | Test Loss 43.0080
Epoch 0011 | Time 0.091 (0.075) | NFE-F 26.4 | NFE-B 0.0 | Train Loss 86.5743 | Test Loss 50.6766
Epoch 0012 | Time 0.094 (0.076) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 101.8955 | Test Loss 35.7532
Epoch 0013 | Time 0.094 (0.077) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 104.2713 | Test Loss 49.6196
Epoch 0014 | Time 0.091 (0.079) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 100.1036 | Test Loss 79.8774
Epoch 0015 | Time 0.118 (0.080) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 84.7195 | Test Loss 121.5119
Epoch 0016 | Time 0.117 (0.082) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 99.4694 | Test Loss 161.7673
Epoch 0017 | Time 0.122 (0.084) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 93.4290 | Test Loss 185.8327
Epoch 0018 | Time 0.118 (0.085) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 100.6226 | Test Loss 187.9783
Epoch 0019 | Time 0.113 (0.087) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 87.2376 | Test Loss 174.5251
Epoch 0020 | Time 0.124 (0.089) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 89.9293 | Test Loss 153.6800
Epoch 0021 | Time 0.131 (0.091) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 62.6415 | Test Loss 131.7206
Epoch 0022 | Time 0.128 (0.093) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 83.5940 | Test Loss 116.6610
Epoch 0023 | Time 0.136 (0.094) | NFE-F 40.1 | NFE-B 0.0 | Train Loss 81.4433 | Test Loss 109.1258
Epoch 0024 | Time 0.132 (0.096) | NFE-F 41.4 | NFE-B 0.0 | Train Loss 73.6758 | Test Loss 106.8242
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=1e-06)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.094 (0.094) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 601.9762 | Test Loss 194.6317
Epoch 0001 | Time 1.207 (0.114) | NFE-F 49.4 | NFE-B 0.0 | Train Loss 117.0133 | Test Loss 50.6959
Epoch 0002 | Time 2.012 (0.174) | NFE-F 87.4 | NFE-B 0.0 | Train Loss 385.4149 | Test Loss 776.1693
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=1e-06)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.164 (0.164) | NFE-F 32.0 | NFE-B 39.0 | Train Loss 593.6538 | Test Loss 203.9406
Epoch 0001 | Time 0.776 (0.176) | NFE-F 36.5 | NFE-B 42.5 | Train Loss 79.4602 | Test Loss 109.5346
Epoch 0002 | Time 1.094 (0.203) | NFE-F 49.3 | NFE-B 50.5 | Train Loss 316.5763 | Test Loss 692.1130
Epoch 0003 | Time 1.345 (0.241) | NFE-F 66.5 | NFE-B 61.0 | Train Loss 62.6532 | Test Loss 97.6874
Epoch 0004 | Time 1.621 (0.285) | NFE-F 87.0 | NFE-B 73.3 | Train Loss 212.7020 | Test Loss 31.6948
Epoch 0005 | Time 1.670 (0.332) | NFE-F 109.5 | NFE-B 86.8 | Train Loss 77.3815 | Test Loss 187.3872
Epoch 0006 | Time 1.836 (0.384) | NFE-F 133.5 | NFE-B 101.4 | Train Loss 135.2753 | Test Loss 333.3078
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hyerpenet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=1e-06)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.139 (0.139) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 685.4780 | Test Loss 238.7606
Epoch 0001 | Time 0.248 (0.142) | NFE-F 33.3 | NFE-B 33.6 | Train Loss 242.7415 | Test Loss 34.2863
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=64, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=1e-06)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Tanh()
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): Tanh()
        (8): Linear(in_features=64, out_features=64, bias=True)
        (9): Tanh()
        (10): Linear(in_features=64, out_features=64, bias=True)
        (11): Tanh()
        (12): Linear(in_features=64, out_features=64, bias=True)
        (13): Tanh()
        (14): Linear(in_features=64, out_features=64, bias=True)
        (15): Tanh()
        (16): Linear(in_features=64, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 41092
Epoch 0000 | Time 0.207 (0.207) | NFE-F 44.0 | NFE-B 39.0 | Train Loss 477.7968 | Test Loss 172.4750
Epoch 0001 | Time 0.927 (0.222) | NFE-F 49.1 | NFE-B 42.5 | Train Loss 339.7423 | Test Loss 58.1468
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=64, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Tanh()
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): Tanh()
        (8): Linear(in_features=64, out_features=64, bias=True)
        (9): Tanh()
        (10): Linear(in_features=64, out_features=64, bias=True)
        (11): Tanh()
        (12): Linear(in_features=64, out_features=64, bias=True)
        (13): Tanh()
        (14): Linear(in_features=64, out_features=64, bias=True)
        (15): Tanh()
        (16): Linear(in_features=64, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 41092
Epoch 0000 | Time 0.111 (0.111) | NFE-F 20.0 | NFE-B 21.0 | Train Loss 602.9485 | Test Loss 192.6642
Epoch 0001 | Time 0.232 (0.115) | NFE-F 21.1 | NFE-B 21.7 | Train Loss 152.2756 | Test Loss 382.7536
Epoch 0002 | Time 0.194 (0.118) | NFE-F 22.4 | NFE-B 22.2 | Train Loss 301.0821 | Test Loss 675.6415
Epoch 0003 | Time 0.255 (0.122) | NFE-F 24.0 | NFE-B 22.9 | Train Loss 213.1002 | Test Loss 68.2755
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=64, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Tanh()
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): Tanh()
        (8): Linear(in_features=64, out_features=64, bias=True)
        (9): Tanh()
        (10): Linear(in_features=64, out_features=64, bias=True)
        (11): Tanh()
        (12): Linear(in_features=64, out_features=64, bias=True)
        (13): Tanh()
        (14): Linear(in_features=64, out_features=64, bias=True)
        (15): Tanh()
        (16): Linear(in_features=64, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 41092
Epoch 0000 | Time 0.112 (0.112) | NFE-F 20.0 | NFE-B 21.0 | Train Loss 590.2124 | Test Loss 222.7938
Epoch 0001 | Time 0.120 (0.113) | NFE-F 20.4 | NFE-B 21.0 | Train Loss 471.2536 | Test Loss 133.3170
Epoch 0002 | Time 0.176 (0.115) | NFE-F 21.2 | NFE-B 21.4 | Train Loss 327.2334 | Test Loss 67.3502
Epoch 0003 | Time 0.283 (0.119) | NFE-F 22.8 | NFE-B 22.4 | Train Loss 288.9860 | Test Loss 56.3641
Epoch 0004 | Time 0.397 (0.127) | NFE-F 24.9 | NFE-B 24.0 | Train Loss 127.1284 | Test Loss 33.6874
Epoch 0005 | Time 0.381 (0.136) | NFE-F 27.1 | NFE-B 25.9 | Train Loss 84.7627 | Test Loss 141.3484
Epoch 0006 | Time 0.316 (0.142) | NFE-F 29.4 | NFE-B 27.4 | Train Loss 116.6623 | Test Loss 257.1414
Epoch 0007 | Time 0.317 (0.148) | NFE-F 31.5 | NFE-B 28.8 | Train Loss 78.4748 | Test Loss 119.3902
Epoch 0008 | Time 0.292 (0.154) | NFE-F 33.3 | NFE-B 30.1 | Train Loss 72.0251 | Test Loss 144.2831
Epoch 0009 | Time 0.263 (0.159) | NFE-F 34.8 | NFE-B 31.3 | Train Loss 93.1104 | Test Loss 159.0213
Epoch 0010 | Time 0.261 (0.164) | NFE-F 36.1 | NFE-B 32.3 | Train Loss 77.0690 | Test Loss 161.7787
Epoch 0011 | Time 0.269 (0.168) | NFE-F 37.2 | NFE-B 33.3 | Train Loss 74.2534 | Test Loss 156.7155
Epoch 0012 | Time 0.248 (0.171) | NFE-F 38.3 | NFE-B 34.0 | Train Loss 85.2066 | Test Loss 145.0602
Epoch 0013 | Time 0.299 (0.174) | NFE-F 39.3 | NFE-B 34.7 | Train Loss 71.8891 | Test Loss 131.3968
Epoch 0014 | Time 0.234 (0.178) | NFE-F 40.3 | NFE-B 35.5 | Train Loss 77.9770 | Test Loss 121.0180
Epoch 0015 | Time 0.237 (0.180) | NFE-F 41.3 | NFE-B 36.0 | Train Loss 64.2560 | Test Loss 114.3976
Epoch 0016 | Time 0.216 (0.182) | NFE-F 42.0 | NFE-B 36.4 | Train Loss 83.1498 | Test Loss 111.9668
Epoch 0017 | Time 0.230 (0.185) | NFE-F 43.0 | NFE-B 36.8 | Train Loss 1111.4124 | Test Loss 504.0420
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=64, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Tanh()
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): Tanh()
        (8): Linear(in_features=64, out_features=64, bias=True)
        (9): Tanh()
        (10): Linear(in_features=64, out_features=64, bias=True)
        (11): Tanh()
        (12): Linear(in_features=64, out_features=64, bias=True)
        (13): Tanh()
        (14): Linear(in_features=64, out_features=64, bias=True)
        (15): Tanh()
        (16): Linear(in_features=64, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 41092
Epoch 0000 | Time 0.120 (0.120) | NFE-F 26.0 | NFE-B 21.0 | Train Loss 660.4828 | Test Loss 244.5778
Epoch 0001 | Time 0.216 (0.123) | NFE-F 26.8 | NFE-B 21.5 | Train Loss 559.2910 | Test Loss 156.9040
Epoch 0002 | Time 0.254 (0.127) | NFE-F 28.0 | NFE-B 22.4 | Train Loss 396.5923 | Test Loss 111.7445
Epoch 0003 | Time 0.371 (0.132) | NFE-F 29.6 | NFE-B 23.6 | Train Loss 636.6080 | Test Loss 173.2100
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.232 (0.232) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 653.5259 | Test Loss 248.3483
Epoch 0001 | Time 0.388 (0.235) | NFE-F 33.9 | NFE-B 33.7 | Train Loss 508.5280 | Test Loss 159.0592
Epoch 0002 | Time 1.016 (0.254) | NFE-F 38.6 | NFE-B 37.4 | Train Loss 447.2726 | Test Loss 110.1176
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=3, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): Tanh()
        (6): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 178884
Epoch 0000 | Time 0.296 (0.296) | NFE-F 38.0 | NFE-B 39.0 | Train Loss 706.6361 | Test Loss 259.9602
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.207 (0.207) | NFE-F 38.0 | NFE-B 33.0 | Train Loss 613.5606 | Test Loss 234.4628
Epoch 0001 | Time 0.269 (0.208) | NFE-F 38.7 | NFE-B 33.2 | Train Loss 516.3296 | Test Loss 141.8881
Epoch 0002 | Time 0.392 (0.213) | NFE-F 40.2 | NFE-B 34.0 | Train Loss 362.4404 | Test Loss 76.0080
Epoch 0003 | Time 0.829 (0.231) | NFE-F 45.0 | NFE-B 37.1 | Train Loss 272.5395 | Test Loss 41.6826
Epoch 0004 | Time 0.714 (0.247) | NFE-F 50.0 | NFE-B 40.0 | Train Loss 201.6051 | Test Loss 30.6536
Epoch 0005 | Time 0.746 (0.263) | NFE-F 54.5 | NFE-B 42.9 | Train Loss 75.4349 | Test Loss 58.7194
Epoch 0006 | Time 0.764 (0.279) | NFE-F 58.9 | NFE-B 45.8 | Train Loss 109.3386 | Test Loss 312.9297
Epoch 0007 | Time 0.789 (0.295) | NFE-F 63.3 | NFE-B 48.7 | Train Loss 197.4277 | Test Loss 417.8729
Epoch 0008 | Time 0.767 (0.311) | NFE-F 67.8 | NFE-B 51.5 | Train Loss 173.7735 | Test Loss 413.9748
Epoch 0009 | Time 0.768 (0.326) | NFE-F 72.1 | NFE-B 54.1 | Train Loss 136.7833 | Test Loss 329.7649
Epoch 0010 | Time 0.742 (0.340) | NFE-F 76.2 | NFE-B 56.7 | Train Loss 95.9847 | Test Loss 225.0668
Epoch 0011 | Time 0.752 (0.354) | NFE-F 80.3 | NFE-B 59.2 | Train Loss 76.8428 | Test Loss 141.8363
Epoch 0012 | Time 0.771 (0.368) | NFE-F 84.2 | NFE-B 61.6 | Train Loss 91.1081 | Test Loss 93.2646
Epoch 0013 | Time 0.758 (0.381) | NFE-F 88.0 | NFE-B 64.0 | Train Loss 64.3261 | Test Loss 71.4707
Epoch 0014 | Time 0.775 (0.394) | NFE-F 91.7 | NFE-B 66.3 | Train Loss 113.6421 | Test Loss 66.7239
Epoch 0015 | Time 0.806 (0.406) | NFE-F 95.3 | NFE-B 68.6 | Train Loss 91.4769 | Test Loss 73.1053
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=1e-05)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.234 (0.234) | NFE-F 44.0 | NFE-B 39.0 | Train Loss 594.6319 | Test Loss 243.7785
Epoch 0001 | Time 0.314 (0.236) | NFE-F 44.9 | NFE-B 39.4 | Train Loss 499.4521 | Test Loss 138.2782
Epoch 0002 | Time 0.394 (0.240) | NFE-F 46.6 | NFE-B 40.1 | Train Loss 282.9223 | Test Loss 40.5354
Epoch 0003 | Time 0.467 (0.247) | NFE-F 49.2 | NFE-B 41.3 | Train Loss 121.6102 | Test Loss 52.4800
Epoch 0004 | Time 0.480 (0.255) | NFE-F 52.3 | NFE-B 42.7 | Train Loss 333.3083 | Test Loss 44.4251
Epoch 0005 | Time 2.230 (0.301) | NFE-F 63.8 | NFE-B 51.3 | Train Loss 955.1534 | Test Loss 400.6032
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.183 (0.183) | NFE-F 32.0 | NFE-B 27.0 | Train Loss 735.1126 | Test Loss 251.0374
Epoch 0001 | Time 0.189 (0.184) | NFE-F 32.6 | NFE-B 27.2 | Train Loss 603.7531 | Test Loss 213.9929
Epoch 0002 | Time 0.232 (0.185) | NFE-F 33.3 | NFE-B 27.4 | Train Loss 547.4236 | Test Loss 203.1864
Epoch 0003 | Time 0.387 (0.190) | NFE-F 34.8 | NFE-B 28.3 | Train Loss 584.7150 | Test Loss 185.3877
Epoch 0004 | Time 0.565 (0.200) | NFE-F 37.7 | NFE-B 30.2 | Train Loss 519.7109 | Test Loss 157.2892
Epoch 0005 | Time 0.656 (0.214) | NFE-F 41.5 | NFE-B 32.8 | Train Loss 446.4230 | Test Loss 128.7980
Epoch 0006 | Time 0.713 (0.230) | NFE-F 45.8 | NFE-B 35.8 | Train Loss 443.8549 | Test Loss 106.0300
Epoch 0007 | Time 0.791 (0.247) | NFE-F 50.3 | NFE-B 39.0 | Train Loss 350.0882 | Test Loss 84.2207
Epoch 0008 | Time 0.766 (0.264) | NFE-F 54.9 | NFE-B 42.2 | Train Loss 377.8639 | Test Loss 65.7390
Epoch 0009 | Time 0.811 (0.281) | NFE-F 59.4 | NFE-B 45.4 | Train Loss 293.3192 | Test Loss 51.2207
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y) / y.shape[0]


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.245 (0.245) | NFE-F 38.0 | NFE-B 45.0 | Train Loss 6.0786 | Test Loss 2.1825
Epoch 0001 | Time 0.209 (0.245) | NFE-F 38.5 | NFE-B 44.8 | Train Loss 5.3443 | Test Loss 1.8617
Epoch 0002 | Time 0.283 (0.246) | NFE-F 39.2 | NFE-B 44.7 | Train Loss 4.5985 | Test Loss 1.7586
Epoch 0003 | Time 0.303 (0.248) | NFE-F 39.9 | NFE-B 44.9 | Train Loss 4.1664 | Test Loss 1.5263
Epoch 0004 | Time 0.505 (0.253) | NFE-F 41.9 | NFE-B 45.7 | Train Loss 4.5380 | Test Loss 1.4357
Epoch 0005 | Time 0.496 (0.261) | NFE-F 45.1 | NFE-B 47.1 | Train Loss 4.0671 | Test Loss 1.3964
Epoch 0006 | Time 0.555 (0.272) | NFE-F 48.2 | NFE-B 48.9 | Train Loss 4.1890 | Test Loss 1.4152
Epoch 0007 | Time 0.531 (0.280) | NFE-F 51.3 | NFE-B 50.3 | Train Loss 3.8256 | Test Loss 1.1906
Epoch 0008 | Time 0.611 (0.291) | NFE-F 54.3 | NFE-B 52.2 | Train Loss 3.3621 | Test Loss 0.9687
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.212 (0.212) | NFE-F 38.0 | NFE-B 33.0 | Train Loss 618.9880 | Test Loss 243.6745
Epoch 0001 | Time 0.211 (0.213) | NFE-F 38.6 | NFE-B 33.0 | Train Loss 626.6074 | Test Loss 228.5138
Epoch 0002 | Time 0.236 (0.214) | NFE-F 39.5 | NFE-B 33.2 | Train Loss 617.7850 | Test Loss 204.6622
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.228 (0.228) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 662.3463 | Test Loss 263.5746
Epoch 0001 | Time 0.233 (0.228) | NFE-F 32.7 | NFE-B 33.1 | Train Loss 706.4435 | Test Loss 236.8170
Epoch 0002 | Time 0.207 (0.227) | NFE-F 33.4 | NFE-B 33.1 | Train Loss 573.1531 | Test Loss 213.9051
Epoch 0003 | Time 0.203 (0.228) | NFE-F 34.4 | NFE-B 33.1 | Train Loss 613.4490 | Test Loss 189.5848
Epoch 0004 | Time 0.231 (0.228) | NFE-F 35.3 | NFE-B 33.2 | Train Loss 567.4155 | Test Loss 160.6470
Epoch 0005 | Time 0.258 (0.229) | NFE-F 36.2 | NFE-B 33.5 | Train Loss 440.4854 | Test Loss 131.3923
Epoch 0006 | Time 0.234 (0.230) | NFE-F 37.3 | NFE-B 33.7 | Train Loss 434.6211 | Test Loss 104.3385
Epoch 0007 | Time 0.251 (0.231) | NFE-F 38.4 | NFE-B 34.0 | Train Loss 340.3719 | Test Loss 81.0502
Epoch 0008 | Time 0.248 (0.232) | NFE-F 39.4 | NFE-B 34.3 | Train Loss 265.4019 | Test Loss 62.2449
Epoch 0009 | Time 0.248 (0.233) | NFE-F 40.4 | NFE-B 34.7 | Train Loss 250.5754 | Test Loss 48.0350
Epoch 0010 | Time 0.260 (0.234) | NFE-F 41.4 | NFE-B 35.0 | Train Loss 252.3688 | Test Loss 38.2129
Epoch 0011 | Time 0.267 (0.236) | NFE-F 42.3 | NFE-B 35.3 | Train Loss 224.5994 | Test Loss 32.2826
Epoch 0012 | Time 0.257 (0.237) | NFE-F 43.2 | NFE-B 35.6 | Train Loss 179.5264 | Test Loss 29.6976
Epoch 0013 | Time 0.275 (0.239) | NFE-F 44.1 | NFE-B 35.8 | Train Loss 186.6126 | Test Loss 29.8234
Epoch 0014 | Time 0.292 (0.240) | NFE-F 45.0 | NFE-B 36.2 | Train Loss 147.6900 | Test Loss 32.0677
Epoch 0015 | Time 0.282 (0.242) | NFE-F 45.8 | NFE-B 36.6 | Train Loss 118.6263 | Test Loss 35.8670
Epoch 0016 | Time 0.289 (0.243) | NFE-F 46.7 | NFE-B 36.9 | Train Loss 139.3462 | Test Loss 40.8070
Epoch 0017 | Time 0.262 (0.244) | NFE-F 47.4 | NFE-B 37.0 | Train Loss 122.7762 | Test Loss 46.4658
Epoch 0018 | Time 0.279 (0.245) | NFE-F 48.3 | NFE-B 37.2 | Train Loss 104.9142 | Test Loss 52.5457
Epoch 0019 | Time 0.245 (0.246) | NFE-F 49.1 | NFE-B 37.4 | Train Loss 82.1630 | Test Loss 58.7456
Epoch 0020 | Time 0.260 (0.248) | NFE-F 49.8 | NFE-B 37.7 | Train Loss 98.7429 | Test Loss 64.9601
Epoch 0021 | Time 0.281 (0.249) | NFE-F 50.5 | NFE-B 38.0 | Train Loss 92.3196 | Test Loss 70.9592
Epoch 0022 | Time 0.251 (0.249) | NFE-F 51.1 | NFE-B 38.1 | Train Loss 64.9523 | Test Loss 76.6112
Epoch 0023 | Time 0.229 (0.250) | NFE-F 51.8 | NFE-B 38.2 | Train Loss 86.6711 | Test Loss 81.8900
Epoch 0024 | Time 0.232 (0.250) | NFE-F 52.5 | NFE-B 38.3 | Train Loss 60.4305 | Test Loss 86.8178
Epoch 0025 | Time 0.260 (0.251) | NFE-F 53.2 | NFE-B 38.5 | Train Loss 107.7512 | Test Loss 91.3833
Epoch 0026 | Time 0.236 (0.252) | NFE-F 53.8 | NFE-B 38.7 | Train Loss 65.6722 | Test Loss 95.4597
Epoch 0027 | Time 0.264 (0.252) | NFE-F 54.4 | NFE-B 38.8 | Train Loss 88.3076 | Test Loss 99.1928
Epoch 0028 | Time 0.266 (0.253) | NFE-F 55.0 | NFE-B 39.0 | Train Loss 75.5328 | Test Loss 102.5547
Epoch 0029 | Time 0.293 (0.254) | NFE-F 55.6 | NFE-B 39.3 | Train Loss 69.0020 | Test Loss 105.5478
Epoch 0030 | Time 0.272 (0.255) | NFE-F 56.1 | NFE-B 39.4 | Train Loss 88.5832 | Test Loss 108.1494
Epoch 0031 | Time 0.259 (0.255) | NFE-F 56.6 | NFE-B 39.5 | Train Loss 85.0921 | Test Loss 110.4409
Epoch 0032 | Time 0.273 (0.257) | NFE-F 57.2 | NFE-B 39.7 | Train Loss 76.9789 | Test Loss 112.4695
Epoch 0033 | Time 0.264 (0.257) | NFE-F 57.7 | NFE-B 39.8 | Train Loss 94.7499 | Test Loss 114.2755
Epoch 0034 | Time 0.249 (0.258) | NFE-F 58.1 | NFE-B 40.0 | Train Loss 71.1573 | Test Loss 115.8535
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.166 (0.166) | NFE-F 38.0 | NFE-B 33.0 | Train Loss 805.6329 | Test Loss 288.0281
Epoch 0001 | Time 0.197 (0.166) | NFE-F 38.4 | NFE-B 33.1 | Train Loss 719.9923 | Test Loss 257.7717
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.163 (0.163) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 670.6434 | Test Loss 244.3192
Epoch 0001 | Time 0.176 (0.164) | NFE-F 32.9 | NFE-B 33.0 | Train Loss 637.2859 | Test Loss 235.8933
Epoch 0002 | Time 0.173 (0.165) | NFE-F 33.9 | NFE-B 33.0 | Train Loss 591.8273 | Test Loss 224.9910
Epoch 0003 | Time 0.207 (0.166) | NFE-F 34.9 | NFE-B 33.1 | Train Loss 591.1779 | Test Loss 223.9792
Epoch 0004 | Time 0.201 (0.167) | NFE-F 36.2 | NFE-B 33.2 | Train Loss 688.8801 | Test Loss 222.7567
Epoch 0005 | Time 0.192 (0.169) | NFE-F 37.6 | NFE-B 33.5 | Train Loss 598.4707 | Test Loss 221.7400
Epoch 0006 | Time 0.217 (0.170) | NFE-F 38.9 | NFE-B 33.7 | Train Loss 738.0494 | Test Loss 223.3667
Epoch 0007 | Time 0.230 (0.173) | NFE-F 40.1 | NFE-B 34.3 | Train Loss 609.9858 | Test Loss 219.4405
Epoch 0008 | Time 0.208 (0.174) | NFE-F 41.3 | NFE-B 34.6 | Train Loss 708.5753 | Test Loss 218.3132
Epoch 0009 | Time 0.230 (0.176) | NFE-F 42.3 | NFE-B 34.8 | Train Loss 640.1066 | Test Loss 217.1814
Epoch 0010 | Time 0.241 (0.178) | NFE-F 43.3 | NFE-B 35.3 | Train Loss 661.9683 | Test Loss 216.0510
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.212 (0.212) | NFE-F 38.0 | NFE-B 45.0 | Train Loss 586.4349 | Test Loss 248.9915
Epoch 0001 | Time 0.181 (0.211) | NFE-F 38.4 | NFE-B 44.8 | Train Loss 666.5888 | Test Loss 247.6119
Epoch 0002 | Time 0.182 (0.210) | NFE-F 39.0 | NFE-B 44.5 | Train Loss 620.6733 | Test Loss 244.1149
Epoch 0003 | Time 0.168 (0.209) | NFE-F 39.4 | NFE-B 44.1 | Train Loss 675.4838 | Test Loss 242.2402
Epoch 0004 | Time 0.165 (0.208) | NFE-F 39.8 | NFE-B 43.9 | Train Loss 660.5200 | Test Loss 234.1936
Epoch 0005 | Time 0.179 (0.208) | NFE-F 40.3 | NFE-B 43.7 | Train Loss 657.0601 | Test Loss 229.4370
Epoch 0006 | Time 0.188 (0.208) | NFE-F 40.8 | NFE-B 43.6 | Train Loss 608.3486 | Test Loss 226.5303
Epoch 0007 | Time 0.168 (0.207) | NFE-F 41.1 | NFE-B 43.3 | Train Loss 632.8154 | Test Loss 225.5533
Epoch 0008 | Time 0.181 (0.206) | NFE-F 41.5 | NFE-B 43.0 | Train Loss 654.5818 | Test Loss 225.3658
Epoch 0009 | Time 0.143 (0.205) | NFE-F 41.8 | NFE-B 42.5 | Train Loss 688.2258 | Test Loss 225.2605
Epoch 0010 | Time 0.145 (0.203) | NFE-F 42.2 | NFE-B 42.1 | Train Loss 589.2280 | Test Loss 225.1471
Epoch 0011 | Time 0.144 (0.202) | NFE-F 42.5 | NFE-B 41.6 | Train Loss 583.5652 | Test Loss 224.8501
Epoch 0012 | Time 0.146 (0.201) | NFE-F 42.8 | NFE-B 41.2 | Train Loss 683.4775 | Test Loss 224.0823
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.191 (0.191) | NFE-F 38.0 | NFE-B 39.0 | Train Loss 507.6303 | Test Loss 595.0251
Epoch 0001 | Time 0.286 (0.192) | NFE-F 38.8 | NFE-B 39.1 | Train Loss 505.5565 | Test Loss 687.1700
Epoch 0002 | Time 0.282 (0.193) | NFE-F 39.5 | NFE-B 39.1 | Train Loss 505.1530 | Test Loss 692.4229
Epoch 0003 | Time 0.268 (0.194) | NFE-F 40.3 | NFE-B 39.1 | Train Loss 505.0921 | Test Loss 683.9657
Epoch 0004 | Time 0.288 (0.195) | NFE-F 41.0 | NFE-B 39.1 | Train Loss 505.0312 | Test Loss 650.8014
Epoch 0005 | Time 0.292 (0.196) | NFE-F 41.7 | NFE-B 39.2 | Train Loss 504.9669 | Test Loss 596.7054
Epoch 0006 | Time 0.300 (0.197) | NFE-F 42.4 | NFE-B 39.2 | Train Loss 504.7753 | Test Loss 631.6790
Epoch 0007 | Time 0.284 (0.198) | NFE-F 43.0 | NFE-B 39.3 | Train Loss 504.7142 | Test Loss 651.6755
Epoch 0008 | Time 0.293 (0.199) | NFE-F 43.6 | NFE-B 39.3 | Train Loss 504.6533 | Test Loss 618.3767
Epoch 0009 | Time 0.260 (0.199) | NFE-F 44.2 | NFE-B 39.3 | Train Loss 504.5923 | Test Loss 584.4711
Epoch 0010 | Time 0.266 (0.200) | NFE-F 44.8 | NFE-B 39.3 | Train Loss 504.5313 | Test Loss 595.1605
Epoch 0011 | Time 0.239 (0.200) | NFE-F 45.4 | NFE-B 39.3 | Train Loss 504.4700 | Test Loss 566.2782
Epoch 0012 | Time 0.246 (0.201) | NFE-F 45.9 | NFE-B 39.2 | Train Loss 504.3109 | Test Loss 634.9953
Epoch 0013 | Time 0.269 (0.201) | NFE-F 46.4 | NFE-B 39.2 | Train Loss 503.9293 | Test Loss 622.4167
Epoch 0014 | Time 0.273 (0.202) | NFE-F 47.0 | NFE-B 39.2 | Train Loss 493.8051 | Test Loss 590.0338
Epoch 0015 | Time 0.263 (0.203) | NFE-F 47.5 | NFE-B 39.2 | Train Loss 484.4832 | Test Loss 568.4072
Epoch 0016 | Time 0.251 (0.203) | NFE-F 48.0 | NFE-B 39.1 | Train Loss 484.4301 | Test Loss 622.2552
Epoch 0017 | Time 0.256 (0.204) | NFE-F 48.4 | NFE-B 39.1 | Train Loss 484.3766 | Test Loss 602.6579
Epoch 0018 | Time 0.255 (0.204) | NFE-F 48.9 | NFE-B 39.1 | Train Loss 484.3225 | Test Loss 586.0432
Epoch 0019 | Time 0.252 (0.205) | NFE-F 49.4 | NFE-B 39.1 | Train Loss 484.2678 | Test Loss 568.9924
Epoch 0020 | Time 0.263 (0.205) | NFE-F 49.9 | NFE-B 39.1 | Train Loss 484.2125 | Test Loss 577.6136
Epoch 0021 | Time 0.259 (0.206) | NFE-F 50.3 | NFE-B 39.1 | Train Loss 484.1568 | Test Loss 647.0449
Epoch 0022 | Time 0.243 (0.206) | NFE-F 50.8 | NFE-B 39.1 | Train Loss 484.1007 | Test Loss 576.3937
Epoch 0023 | Time 0.261 (0.207) | NFE-F 51.4 | NFE-B 39.0 | Train Loss 484.0442 | Test Loss 624.2147
Epoch 0024 | Time 0.253 (0.207) | NFE-F 52.0 | NFE-B 39.0 | Train Loss 483.9875 | Test Loss 600.7332
Epoch 0025 | Time 0.240 (0.208) | NFE-F 52.5 | NFE-B 38.9 | Train Loss 483.9304 | Test Loss 631.4043
Epoch 0026 | Time 0.256 (0.208) | NFE-F 53.1 | NFE-B 38.8 | Train Loss 483.8731 | Test Loss 573.6719
Epoch 0027 | Time 0.252 (0.208) | NFE-F 53.6 | NFE-B 38.8 | Train Loss 483.8156 | Test Loss 555.7769
Epoch 0028 | Time 0.239 (0.209) | NFE-F 54.2 | NFE-B 38.7 | Train Loss 483.7579 | Test Loss 596.5919
Epoch 0029 | Time 0.240 (0.209) | NFE-F 54.7 | NFE-B 38.7 | Train Loss 483.7000 | Test Loss 546.9241
Epoch 0030 | Time 0.246 (0.209) | NFE-F 55.3 | NFE-B 38.6 | Train Loss 483.6419 | Test Loss 602.1259
Epoch 0031 | Time 0.266 (0.210) | NFE-F 55.8 | NFE-B 38.6 | Train Loss 483.5838 | Test Loss 598.5539
Epoch 0032 | Time 0.275 (0.211) | NFE-F 56.4 | NFE-B 38.6 | Train Loss 483.5255 | Test Loss 566.3891
Epoch 0033 | Time 0.276 (0.211) | NFE-F 56.9 | NFE-B 38.6 | Train Loss 483.4671 | Test Loss 540.8224
Epoch 0034 | Time 0.277 (0.212) | NFE-F 57.4 | NFE-B 38.6 | Train Loss 483.4086 | Test Loss 528.3402
Epoch 0035 | Time 0.274 (0.213) | NFE-F 58.0 | NFE-B 38.6 | Train Loss 483.3500 | Test Loss 697.0054
Epoch 0036 | Time 0.266 (0.213) | NFE-F 58.6 | NFE-B 38.6 | Train Loss 483.2916 | Test Loss 624.5490
Epoch 0037 | Time 0.295 (0.214) | NFE-F 59.1 | NFE-B 38.6 | Train Loss 483.2347 | Test Loss 613.1321
Epoch 0038 | Time 0.287 (0.215) | NFE-F 59.7 | NFE-B 38.6 | Train Loss 483.1754 | Test Loss 558.5924
Epoch 0039 | Time 0.280 (0.215) | NFE-F 60.2 | NFE-B 38.6 | Train Loss 483.1152 | Test Loss 681.6421
Epoch 0040 | Time 0.275 (0.216) | NFE-F 60.8 | NFE-B 38.6 | Train Loss 483.0561 | Test Loss 584.2094
Epoch 0041 | Time 0.266 (0.216) | NFE-F 61.3 | NFE-B 38.7 | Train Loss 482.9971 | Test Loss 571.0095
Epoch 0042 | Time 0.286 (0.217) | NFE-F 61.9 | NFE-B 38.7 | Train Loss 482.9382 | Test Loss 576.1780
Epoch 0043 | Time 0.267 (0.218) | NFE-F 62.5 | NFE-B 38.7 | Train Loss 482.8791 | Test Loss 588.6858
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.148 (0.148) | NFE-F 38.0 | NFE-B 27.0 | Train Loss 470.5889 | Test Loss 583.4218
Epoch 0001 | Time 0.254 (0.149) | NFE-F 38.8 | NFE-B 27.1 | Train Loss 452.0190 | Test Loss 522.6471
Epoch 0002 | Time 0.294 (0.150) | NFE-F 39.3 | NFE-B 27.2 | Train Loss 466.2327 | Test Loss 568.9974
Epoch 0003 | Time 0.314 (0.152) | NFE-F 40.2 | NFE-B 27.4 | Train Loss 451.0554 | Test Loss 568.9382
Epoch 0004 | Time 0.273 (0.153) | NFE-F 40.9 | NFE-B 27.5 | Train Loss 450.5436 | Test Loss 551.1550
Epoch 0005 | Time 0.277 (0.154) | NFE-F 41.7 | NFE-B 27.6 | Train Loss 450.0181 | Test Loss 619.2804
Epoch 0006 | Time 0.311 (0.156) | NFE-F 42.4 | NFE-B 27.8 | Train Loss 449.4831 | Test Loss 539.1372
Epoch 0007 | Time 0.298 (0.157) | NFE-F 43.1 | NFE-B 28.0 | Train Loss 448.9412 | Test Loss 569.9101
Epoch 0008 | Time 0.287 (0.159) | NFE-F 43.9 | NFE-B 28.1 | Train Loss 448.4424 | Test Loss 592.9186
Epoch 0009 | Time 0.297 (0.160) | NFE-F 44.8 | NFE-B 28.2 | Train Loss 448.0094 | Test Loss 582.7579
Epoch 0010 | Time 0.350 (0.162) | NFE-F 45.9 | NFE-B 28.4 | Train Loss 447.4572 | Test Loss 605.1020
Epoch 0011 | Time 0.324 (0.164) | NFE-F 46.9 | NFE-B 28.6 | Train Loss 446.7353 | Test Loss 509.2994
Epoch 0012 | Time 0.323 (0.165) | NFE-F 47.8 | NFE-B 28.8 | Train Loss 446.1754 | Test Loss 487.1174
Epoch 0013 | Time 0.314 (0.167) | NFE-F 48.6 | NFE-B 28.9 | Train Loss 445.6159 | Test Loss 610.0585
Epoch 0014 | Time 0.312 (0.168) | NFE-F 49.4 | NFE-B 29.1 | Train Loss 445.0551 | Test Loss 556.0649
Epoch 0015 | Time 0.318 (0.170) | NFE-F 50.3 | NFE-B 29.3 | Train Loss 444.4935 | Test Loss 554.2475
Epoch 0016 | Time 0.320 (0.171) | NFE-F 51.1 | NFE-B 29.4 | Train Loss 443.9312 | Test Loss 495.4970
Epoch 0017 | Time 0.345 (0.173) | NFE-F 51.9 | NFE-B 29.6 | Train Loss 443.3682 | Test Loss 554.3510
Epoch 0018 | Time 0.369 (0.175) | NFE-F 52.9 | NFE-B 29.9 | Train Loss 442.8050 | Test Loss 631.5110
Epoch 0019 | Time 0.322 (0.176) | NFE-F 53.9 | NFE-B 30.0 | Train Loss 442.2414 | Test Loss 572.8087
Epoch 0020 | Time 0.338 (0.178) | NFE-F 54.9 | NFE-B 30.3 | Train Loss 441.6777 | Test Loss 571.7493
Epoch 0021 | Time 0.360 (0.180) | NFE-F 55.8 | NFE-B 30.5 | Train Loss 442.2476 | Test Loss 578.1234
Epoch 0022 | Time 0.365 (0.182) | NFE-F 56.8 | NFE-B 30.7 | Train Loss 441.7941 | Test Loss 599.5827
Epoch 0023 | Time 0.366 (0.184) | NFE-F 57.7 | NFE-B 31.0 | Train Loss 441.2367 | Test Loss 522.3663
Epoch 0024 | Time 0.395 (0.186) | NFE-F 58.6 | NFE-B 31.3 | Train Loss 441.7095 | Test Loss 485.7586
Epoch 0025 | Time 0.348 (0.187) | NFE-F 59.5 | NFE-B 31.5 | Train Loss 438.8637 | Test Loss 574.3048
Epoch 0026 | Time 0.357 (0.189) | NFE-F 60.4 | NFE-B 31.7 | Train Loss 438.3021 | Test Loss 511.2907
Epoch 0027 | Time 0.368 (0.191) | NFE-F 61.3 | NFE-B 32.0 | Train Loss 437.7405 | Test Loss 588.1959
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.160 (0.160) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 470.9152 | Test Loss 585.1619
Epoch 0001 | Time 0.314 (0.161) | NFE-F 33.0 | NFE-B 33.1 | Train Loss 457.7957 | Test Loss 496.6654
Epoch 0002 | Time 0.590 (0.166) | NFE-F 35.4 | NFE-B 33.8 | Train Loss 457.4163 | Test Loss 551.8701
Epoch 0003 | Time 0.744 (0.172) | NFE-F 38.7 | NFE-B 34.9 | Train Loss 452.9536 | Test Loss 608.8598
Epoch 0004 | Time 0.846 (0.178) | NFE-F 42.4 | NFE-B 36.0 | Train Loss 448.1871 | Test Loss 599.7330
Epoch 0005 | Time 0.887 (0.185) | NFE-F 46.1 | NFE-B 37.3 | Train Loss 443.2383 | Test Loss 572.1360
Epoch 0006 | Time 0.972 (0.193) | NFE-F 50.2 | NFE-B 38.7 | Train Loss 438.1751 | Test Loss 482.1218
Epoch 0007 | Time 1.002 (0.201) | NFE-F 54.5 | NFE-B 40.2 | Train Loss 433.0400 | Test Loss 502.3980
Epoch 0008 | Time 0.977 (0.209) | NFE-F 58.9 | NFE-B 41.5 | Train Loss 427.8608 | Test Loss 584.6394
Epoch 0009 | Time 0.980 (0.217) | NFE-F 63.4 | NFE-B 42.9 | Train Loss 422.6574 | Test Loss 578.6235
Epoch 0010 | Time 0.994 (0.225) | NFE-F 67.8 | NFE-B 44.2 | Train Loss 417.4440 | Test Loss 570.3111
Epoch 0011 | Time 0.982 (0.232) | NFE-F 72.4 | NFE-B 45.6 | Train Loss 435.1523 | Test Loss 590.7297
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.1, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.158 (0.158) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 451.2704 | Test Loss 525.6111
Epoch 0001 | Time 0.541 (0.161) | NFE-F 34.4 | NFE-B 33.6 | Train Loss 435.2757 | Test Loss 564.2970
Epoch 0002 | Time 0.692 (0.167) | NFE-F 37.7 | NFE-B 34.6 | Train Loss 391.5129 | Test Loss 471.4174
Epoch 0003 | Time 0.697 (0.172) | NFE-F 40.9 | NFE-B 35.4 | Train Loss 377.2156 | Test Loss 498.7661
Epoch 0004 | Time 0.693 (0.177) | NFE-F 43.8 | NFE-B 36.3 | Train Loss 685.9825 | Test Loss 667.5878
Epoch 0005 | Time 0.699 (0.183) | NFE-F 46.7 | NFE-B 37.2 | Train Loss 750.6708 | Test Loss 816.9667
Epoch 0006 | Time 0.664 (0.187) | NFE-F 49.3 | NFE-B 38.1 | Train Loss 752.7164 | Test Loss 838.8716
Epoch 0007 | Time 0.663 (0.192) | NFE-F 51.8 | NFE-B 38.9 | Train Loss 737.0453 | Test Loss 773.1858
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=3, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): Tanh()
        (6): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 178884
Epoch 0000 | Time 0.221 (0.221) | NFE-F 38.0 | NFE-B 27.0 | Train Loss 468.5544 | Test Loss 581.4703
Epoch 0001 | Time 0.462 (0.223) | NFE-F 39.4 | NFE-B 27.3 | Train Loss 464.7775 | Test Loss 637.5588
Epoch 0002 | Time 0.459 (0.226) | NFE-F 40.6 | NFE-B 27.6 | Train Loss 460.2292 | Test Loss 605.6552
Epoch 0003 | Time 0.526 (0.229) | NFE-F 42.1 | NFE-B 28.0 | Train Loss 455.3852 | Test Loss 623.1141
Epoch 0004 | Time 0.522 (0.232) | NFE-F 43.5 | NFE-B 28.4 | Train Loss 450.3484 | Test Loss 589.0645
Epoch 0005 | Time 0.511 (0.234) | NFE-F 45.0 | NFE-B 28.8 | Train Loss 445.1977 | Test Loss 532.2397
Epoch 0006 | Time 0.518 (0.237) | NFE-F 46.4 | NFE-B 29.2 | Train Loss 440.0753 | Test Loss 537.6068
Epoch 0007 | Time 0.541 (0.240) | NFE-F 47.8 | NFE-B 29.6 | Train Loss 434.8224 | Test Loss 595.6570
Epoch 0008 | Time 0.517 (0.243) | NFE-F 49.3 | NFE-B 30.0 | Train Loss 429.4773 | Test Loss 576.5524
Epoch 0009 | Time 0.536 (0.246) | NFE-F 50.9 | NFE-B 30.4 | Train Loss 424.3773 | Test Loss 516.5137
Epoch 0010 | Time 0.535 (0.249) | NFE-F 52.5 | NFE-B 30.8 | Train Loss 419.1143 | Test Loss 517.3851
Epoch 0011 | Time 0.522 (0.252) | NFE-F 54.0 | NFE-B 31.2 | Train Loss 413.8644 | Test Loss 519.8522
Epoch 0012 | Time 0.540 (0.255) | NFE-F 55.5 | NFE-B 31.6 | Train Loss 408.5424 | Test Loss 430.5510
Epoch 0013 | Time 0.542 (0.257) | NFE-F 57.1 | NFE-B 31.9 | Train Loss 401.8046 | Test Loss 485.5120
Epoch 0014 | Time 0.535 (0.260) | NFE-F 58.6 | NFE-B 32.3 | Train Loss 549.5036 | Test Loss 678.2479
Epoch 0015 | Time 0.537 (0.263) | NFE-F 60.1 | NFE-B 32.7 | Train Loss 563.0120 | Test Loss 810.4058
Epoch 0016 | Time 0.692 (0.267) | NFE-F 62.1 | NFE-B 33.3 | Train Loss 514.2405 | Test Loss 634.9647
Epoch 0017 | Time 0.740 (0.272) | NFE-F 64.4 | NFE-B 33.9 | Train Loss 443.5467 | Test Loss 550.6959
Epoch 0018 | Time 0.882 (0.278) | NFE-F 67.0 | NFE-B 34.8 | Train Loss 413.2216 | Test Loss 596.5928
Epoch 0019 | Time 0.810 (0.283) | NFE-F 69.5 | NFE-B 35.6 | Train Loss 449.6751 | Test Loss 570.7104
Epoch 0020 | Time 0.815 (0.289) | NFE-F 72.3 | NFE-B 36.3 | Train Loss 537.9579 | Test Loss 612.2883
Epoch 0021 | Time 0.840 (0.294) | NFE-F 75.1 | NFE-B 37.0 | Train Loss 574.1302 | Test Loss 705.8381
Epoch 0022 | Time 0.872 (0.300) | NFE-F 78.0 | NFE-B 37.8 | Train Loss 576.1250 | Test Loss 730.4046
Epoch 0023 | Time 0.855 (0.306) | NFE-F 81.0 | NFE-B 38.5 | Train Loss 566.3842 | Test Loss 761.0871
Epoch 0024 | Time 0.910 (0.312) | NFE-F 84.0 | NFE-B 39.3 | Train Loss 536.4471 | Test Loss 651.4460
Epoch 0025 | Time 0.997 (0.318) | NFE-F 87.0 | NFE-B 40.2 | Train Loss 474.0239 | Test Loss 675.3995
Epoch 0026 | Time 0.951 (0.325) | NFE-F 90.2 | NFE-B 41.1 | Train Loss 460.2142 | Test Loss 572.0376
Epoch 0027 | Time 0.880 (0.330) | NFE-F 93.3 | NFE-B 41.8 | Train Loss 428.0672 | Test Loss 541.5227
Epoch 0028 | Time 0.891 (0.336) | NFE-F 96.4 | NFE-B 42.6 | Train Loss 412.9895 | Test Loss 596.3389
Epoch 0029 | Time 0.961 (0.342) | NFE-F 99.6 | NFE-B 43.4 | Train Loss 424.2449 | Test Loss 505.2204
Epoch 0030 | Time 1.002 (0.349) | NFE-F 102.8 | NFE-B 44.3 | Train Loss 411.5221 | Test Loss 480.0962
Epoch 0031 | Time 0.995 (0.355) | NFE-F 106.0 | NFE-B 45.1 | Train Loss 440.6873 | Test Loss 555.2549
Epoch 0032 | Time 1.022 (0.362) | NFE-F 109.1 | NFE-B 46.1 | Train Loss 454.2672 | Test Loss 547.6797
Epoch 0033 | Time 0.966 (0.368) | NFE-F 112.4 | NFE-B 46.9 | Train Loss 494.0837 | Test Loss 598.6237
Epoch 0034 | Time 1.030 (0.375) | NFE-F 115.7 | NFE-B 47.8 | Train Loss 513.0227 | Test Loss 650.7415
