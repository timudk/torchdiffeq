/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousing:
    def __init__(n_examples, is_train):
        self.n_examples = n_examples
        self.is_train = is_train

        data = datasets.load_diabetes(return_X_y=True)

        if is_train:
            self.x = data[0][:self.n_examples, :]
            self.y = data[1][:self.n_examples]

        else:
            self.x = data[0][self.n_examples:, :]
            self.y = data[1][self.n_examples:]

    def __len__(self):
        return n_examples if is_train else 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousing(test_train_split, is_train=True),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousing(test_train_split, is_train=False),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hypernet_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousing:
    def __init__(n_examples, is_train):
        self.n_examples = n_examples
        self.is_train = is_train

        data = datasets.load_diabetes(return_X_y=True)

        if is_train:
            self.x = data[0][:self.n_examples, :]
            self.y = data[1][:self.n_examples]

        else:
            self.x = data[0][self.n_examples:, :]
            self.y = data[1][self.n_examples:]

    def __len__(self):
        return n_examples if is_train else 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousing(test_train_split, is_train=True),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousing(test_train_split, is_train=False),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(n_examples, is_train):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.to(device).float()
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [
        ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x))

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str,
                    choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)


parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, network='odenet', save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        pdb.set_trace()

        return self.activation(nn.functional.linear(x, w, b))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = accuracy(model, train_loader)
                test_loss = accuracy(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = loss(model, train_loader)
                test_loss = loss(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        criterion = nn.MSELoss().to(device)

    return criterion(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(model, train_loader)
                test_loss = compute_loss(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        criterion = nn.MSELoss().to(device)
        loss = criterion(model(x), y)

    return loss


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(model, train_loader)
                test_loss = compute_loss(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

        mse_loss = nn.MSELoss().to(device)

    return mse_loss(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(model, train_loader)
                test_loss = compute_loss(model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples 

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
    for x, y in dataset_loader:
        x = x.float().to(device)
        y = y.float().to(device)

    return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = torch.flatten(x).float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = np.reshape(x, -1).float().to(device)
        y = y.float().to(device)
        loss = criterion(model(x), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(model(x), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		print(x)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	pdb.set_trace()
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	pdb.set_trace()
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x)
		print(y)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=2, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=2, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x.shape)
		print(y.shape)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x.shape)
		print(y.shape)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=True)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=True)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x.shape)
		print(y.shape)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                print('Test')
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

		print(x.shape)
		print(y.shape)
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.067 (0.067) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 355.6541 | Test Loss 56.1850
Epoch 0001 | Time 0.175 (0.071) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 597.8273 | Test Loss 1137.5107
Epoch 0002 | Time 0.202 (0.076) | NFE-F 25.8 | NFE-B 0.0 | Train Loss 739.0802 | Test Loss 1290.8634
Epoch 0003 | Time 0.214 (0.082) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 481.4865 | Test Loss 910.8774
Epoch 0004 | Time 0.237 (0.088) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 2860.4312 | Test Loss 1941.5103
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.065 (0.065) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 867.8470 | Test Loss 421.5237
Epoch 0001 | Time 2.964 (0.106) | NFE-F 40.7 | NFE-B 0.0 | Train Loss 6213.5518 | Test Loss 4903.3657
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.064 (0.064) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 824.1253 | Test Loss 312.4105
Epoch 0001 | Time 0.115 (0.066) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 83.2693 | Test Loss 219.5147
Epoch 0002 | Time 0.126 (0.069) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 435.5676 | Test Loss 844.2549
Epoch 0003 | Time 0.132 (0.071) | NFE-F 25.3 | NFE-B 0.0 | Train Loss 105.2414 | Test Loss 48.7483
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.068 (0.068) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 589.0634 | Test Loss 236.9714
Epoch 0001 | Time 0.063 (0.069) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 387.6355 | Test Loss 118.9518
Epoch 0002 | Time 0.069 (0.069) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 241.1600 | Test Loss 31.4426
Epoch 0003 | Time 0.083 (0.069) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 70.5097 | Test Loss 67.2300
Epoch 0004 | Time 0.087 (0.070) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 86.6430 | Test Loss 208.5830
Epoch 0005 | Time 0.077 (0.071) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 138.9497 | Test Loss 305.8057
Epoch 0006 | Time 0.082 (0.071) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 88.7137 | Test Loss 221.3272
Epoch 0007 | Time 0.077 (0.072) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 85.4428 | Test Loss 200.9950
Epoch 0008 | Time 0.077 (0.073) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 74.2993 | Test Loss 161.9343
Epoch 0009 | Time 0.083 (0.073) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 85.5876 | Test Loss 126.5085
Epoch 0010 | Time 0.086 (0.074) | NFE-F 25.5 | NFE-B 0.0 | Train Loss 65.7968 | Test Loss 43.0080
Epoch 0011 | Time 0.091 (0.075) | NFE-F 26.4 | NFE-B 0.0 | Train Loss 86.5743 | Test Loss 50.6766
Epoch 0012 | Time 0.094 (0.076) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 101.8955 | Test Loss 35.7532
Epoch 0013 | Time 0.094 (0.077) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 104.2713 | Test Loss 49.6196
Epoch 0014 | Time 0.091 (0.079) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 100.1036 | Test Loss 79.8774
Epoch 0015 | Time 0.118 (0.080) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 84.7195 | Test Loss 121.5119
Epoch 0016 | Time 0.117 (0.082) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 99.4694 | Test Loss 161.7673
Epoch 0017 | Time 0.122 (0.084) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 93.4290 | Test Loss 185.8327
Epoch 0018 | Time 0.118 (0.085) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 100.6226 | Test Loss 187.9783
Epoch 0019 | Time 0.113 (0.087) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 87.2376 | Test Loss 174.5251
Epoch 0020 | Time 0.124 (0.089) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 89.9293 | Test Loss 153.6800
Epoch 0021 | Time 0.131 (0.091) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 62.6415 | Test Loss 131.7206
Epoch 0022 | Time 0.128 (0.093) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 83.5940 | Test Loss 116.6610
Epoch 0023 | Time 0.136 (0.094) | NFE-F 40.1 | NFE-B 0.0 | Train Loss 81.4433 | Test Loss 109.1258
Epoch 0024 | Time 0.132 (0.096) | NFE-F 41.4 | NFE-B 0.0 | Train Loss 73.6758 | Test Loss 106.8242
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=1e-06)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.094 (0.094) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 601.9762 | Test Loss 194.6317
Epoch 0001 | Time 1.207 (0.114) | NFE-F 49.4 | NFE-B 0.0 | Train Loss 117.0133 | Test Loss 50.6959
Epoch 0002 | Time 2.012 (0.174) | NFE-F 87.4 | NFE-B 0.0 | Train Loss 385.4149 | Test Loss 776.1693
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)
		
	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=1e-06)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.164 (0.164) | NFE-F 32.0 | NFE-B 39.0 | Train Loss 593.6538 | Test Loss 203.9406
Epoch 0001 | Time 0.776 (0.176) | NFE-F 36.5 | NFE-B 42.5 | Train Loss 79.4602 | Test Loss 109.5346
Epoch 0002 | Time 1.094 (0.203) | NFE-F 49.3 | NFE-B 50.5 | Train Loss 316.5763 | Test Loss 692.1130
Epoch 0003 | Time 1.345 (0.241) | NFE-F 66.5 | NFE-B 61.0 | Train Loss 62.6532 | Test Loss 97.6874
Epoch 0004 | Time 1.621 (0.285) | NFE-F 87.0 | NFE-B 73.3 | Train Loss 212.7020 | Test Loss 31.6948
Epoch 0005 | Time 1.670 (0.332) | NFE-F 109.5 | NFE-B 86.8 | Train Loss 77.3815 | Test Loss 187.3872
Epoch 0006 | Time 1.836 (0.384) | NFE-F 133.5 | NFE-B 101.4 | Train Loss 135.2753 | Test Loss 333.3078
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hyerpenet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=1e-06)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.139 (0.139) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 685.4780 | Test Loss 238.7606
Epoch 0001 | Time 0.248 (0.142) | NFE-F 33.3 | NFE-B 33.6 | Train Loss 242.7415 | Test Loss 34.2863
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=64, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=1e-06)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Tanh()
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): Tanh()
        (8): Linear(in_features=64, out_features=64, bias=True)
        (9): Tanh()
        (10): Linear(in_features=64, out_features=64, bias=True)
        (11): Tanh()
        (12): Linear(in_features=64, out_features=64, bias=True)
        (13): Tanh()
        (14): Linear(in_features=64, out_features=64, bias=True)
        (15): Tanh()
        (16): Linear(in_features=64, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 41092
Epoch 0000 | Time 0.207 (0.207) | NFE-F 44.0 | NFE-B 39.0 | Train Loss 477.7968 | Test Loss 172.4750
Epoch 0001 | Time 0.927 (0.222) | NFE-F 49.1 | NFE-B 42.5 | Train Loss 339.7423 | Test Loss 58.1468
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=64, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Tanh()
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): Tanh()
        (8): Linear(in_features=64, out_features=64, bias=True)
        (9): Tanh()
        (10): Linear(in_features=64, out_features=64, bias=True)
        (11): Tanh()
        (12): Linear(in_features=64, out_features=64, bias=True)
        (13): Tanh()
        (14): Linear(in_features=64, out_features=64, bias=True)
        (15): Tanh()
        (16): Linear(in_features=64, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 41092
Epoch 0000 | Time 0.111 (0.111) | NFE-F 20.0 | NFE-B 21.0 | Train Loss 602.9485 | Test Loss 192.6642
Epoch 0001 | Time 0.232 (0.115) | NFE-F 21.1 | NFE-B 21.7 | Train Loss 152.2756 | Test Loss 382.7536
Epoch 0002 | Time 0.194 (0.118) | NFE-F 22.4 | NFE-B 22.2 | Train Loss 301.0821 | Test Loss 675.6415
Epoch 0003 | Time 0.255 (0.122) | NFE-F 24.0 | NFE-B 22.9 | Train Loss 213.1002 | Test Loss 68.2755
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=64, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Tanh()
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): Tanh()
        (8): Linear(in_features=64, out_features=64, bias=True)
        (9): Tanh()
        (10): Linear(in_features=64, out_features=64, bias=True)
        (11): Tanh()
        (12): Linear(in_features=64, out_features=64, bias=True)
        (13): Tanh()
        (14): Linear(in_features=64, out_features=64, bias=True)
        (15): Tanh()
        (16): Linear(in_features=64, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 41092
Epoch 0000 | Time 0.112 (0.112) | NFE-F 20.0 | NFE-B 21.0 | Train Loss 590.2124 | Test Loss 222.7938
Epoch 0001 | Time 0.120 (0.113) | NFE-F 20.4 | NFE-B 21.0 | Train Loss 471.2536 | Test Loss 133.3170
Epoch 0002 | Time 0.176 (0.115) | NFE-F 21.2 | NFE-B 21.4 | Train Loss 327.2334 | Test Loss 67.3502
Epoch 0003 | Time 0.283 (0.119) | NFE-F 22.8 | NFE-B 22.4 | Train Loss 288.9860 | Test Loss 56.3641
Epoch 0004 | Time 0.397 (0.127) | NFE-F 24.9 | NFE-B 24.0 | Train Loss 127.1284 | Test Loss 33.6874
Epoch 0005 | Time 0.381 (0.136) | NFE-F 27.1 | NFE-B 25.9 | Train Loss 84.7627 | Test Loss 141.3484
Epoch 0006 | Time 0.316 (0.142) | NFE-F 29.4 | NFE-B 27.4 | Train Loss 116.6623 | Test Loss 257.1414
Epoch 0007 | Time 0.317 (0.148) | NFE-F 31.5 | NFE-B 28.8 | Train Loss 78.4748 | Test Loss 119.3902
Epoch 0008 | Time 0.292 (0.154) | NFE-F 33.3 | NFE-B 30.1 | Train Loss 72.0251 | Test Loss 144.2831
Epoch 0009 | Time 0.263 (0.159) | NFE-F 34.8 | NFE-B 31.3 | Train Loss 93.1104 | Test Loss 159.0213
Epoch 0010 | Time 0.261 (0.164) | NFE-F 36.1 | NFE-B 32.3 | Train Loss 77.0690 | Test Loss 161.7787
Epoch 0011 | Time 0.269 (0.168) | NFE-F 37.2 | NFE-B 33.3 | Train Loss 74.2534 | Test Loss 156.7155
Epoch 0012 | Time 0.248 (0.171) | NFE-F 38.3 | NFE-B 34.0 | Train Loss 85.2066 | Test Loss 145.0602
Epoch 0013 | Time 0.299 (0.174) | NFE-F 39.3 | NFE-B 34.7 | Train Loss 71.8891 | Test Loss 131.3968
Epoch 0014 | Time 0.234 (0.178) | NFE-F 40.3 | NFE-B 35.5 | Train Loss 77.9770 | Test Loss 121.0180
Epoch 0015 | Time 0.237 (0.180) | NFE-F 41.3 | NFE-B 36.0 | Train Loss 64.2560 | Test Loss 114.3976
Epoch 0016 | Time 0.216 (0.182) | NFE-F 42.0 | NFE-B 36.4 | Train Loss 83.1498 | Test Loss 111.9668
Epoch 0017 | Time 0.230 (0.185) | NFE-F 43.0 | NFE-B 36.8 | Train Loss 1111.4124 | Test Loss 504.0420
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=64, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=64, bias=True)
        (5): Tanh()
        (6): Linear(in_features=64, out_features=64, bias=True)
        (7): Tanh()
        (8): Linear(in_features=64, out_features=64, bias=True)
        (9): Tanh()
        (10): Linear(in_features=64, out_features=64, bias=True)
        (11): Tanh()
        (12): Linear(in_features=64, out_features=64, bias=True)
        (13): Tanh()
        (14): Linear(in_features=64, out_features=64, bias=True)
        (15): Tanh()
        (16): Linear(in_features=64, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 41092
Epoch 0000 | Time 0.120 (0.120) | NFE-F 26.0 | NFE-B 21.0 | Train Loss 660.4828 | Test Loss 244.5778
Epoch 0001 | Time 0.216 (0.123) | NFE-F 26.8 | NFE-B 21.5 | Train Loss 559.2910 | Test Loss 156.9040
Epoch 0002 | Time 0.254 (0.127) | NFE-F 28.0 | NFE-B 22.4 | Train Loss 396.5923 | Test Loss 111.7445
Epoch 0003 | Time 0.371 (0.132) | NFE-F 29.6 | NFE-B 23.6 | Train Loss 636.6080 | Test Loss 173.2100
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.232 (0.232) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 653.5259 | Test Loss 248.3483
Epoch 0001 | Time 0.388 (0.235) | NFE-F 33.9 | NFE-B 33.7 | Train Loss 508.5280 | Test Loss 159.0592
Epoch 0002 | Time 1.016 (0.254) | NFE-F 38.6 | NFE-B 37.4 | Train Loss 447.2726 | Test Loss 110.1176
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=3, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): Tanh()
        (6): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 178884
Epoch 0000 | Time 0.296 (0.296) | NFE-F 38.0 | NFE-B 39.0 | Train Loss 706.6361 | Test Loss 259.9602
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.207 (0.207) | NFE-F 38.0 | NFE-B 33.0 | Train Loss 613.5606 | Test Loss 234.4628
Epoch 0001 | Time 0.269 (0.208) | NFE-F 38.7 | NFE-B 33.2 | Train Loss 516.3296 | Test Loss 141.8881
Epoch 0002 | Time 0.392 (0.213) | NFE-F 40.2 | NFE-B 34.0 | Train Loss 362.4404 | Test Loss 76.0080
Epoch 0003 | Time 0.829 (0.231) | NFE-F 45.0 | NFE-B 37.1 | Train Loss 272.5395 | Test Loss 41.6826
Epoch 0004 | Time 0.714 (0.247) | NFE-F 50.0 | NFE-B 40.0 | Train Loss 201.6051 | Test Loss 30.6536
Epoch 0005 | Time 0.746 (0.263) | NFE-F 54.5 | NFE-B 42.9 | Train Loss 75.4349 | Test Loss 58.7194
Epoch 0006 | Time 0.764 (0.279) | NFE-F 58.9 | NFE-B 45.8 | Train Loss 109.3386 | Test Loss 312.9297
Epoch 0007 | Time 0.789 (0.295) | NFE-F 63.3 | NFE-B 48.7 | Train Loss 197.4277 | Test Loss 417.8729
Epoch 0008 | Time 0.767 (0.311) | NFE-F 67.8 | NFE-B 51.5 | Train Loss 173.7735 | Test Loss 413.9748
Epoch 0009 | Time 0.768 (0.326) | NFE-F 72.1 | NFE-B 54.1 | Train Loss 136.7833 | Test Loss 329.7649
Epoch 0010 | Time 0.742 (0.340) | NFE-F 76.2 | NFE-B 56.7 | Train Loss 95.9847 | Test Loss 225.0668
Epoch 0011 | Time 0.752 (0.354) | NFE-F 80.3 | NFE-B 59.2 | Train Loss 76.8428 | Test Loss 141.8363
Epoch 0012 | Time 0.771 (0.368) | NFE-F 84.2 | NFE-B 61.6 | Train Loss 91.1081 | Test Loss 93.2646
Epoch 0013 | Time 0.758 (0.381) | NFE-F 88.0 | NFE-B 64.0 | Train Loss 64.3261 | Test Loss 71.4707
Epoch 0014 | Time 0.775 (0.394) | NFE-F 91.7 | NFE-B 66.3 | Train Loss 113.6421 | Test Loss 66.7239
Epoch 0015 | Time 0.806 (0.406) | NFE-F 95.3 | NFE-B 68.6 | Train Loss 91.4769 | Test Loss 73.1053
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=1e-05)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.234 (0.234) | NFE-F 44.0 | NFE-B 39.0 | Train Loss 594.6319 | Test Loss 243.7785
Epoch 0001 | Time 0.314 (0.236) | NFE-F 44.9 | NFE-B 39.4 | Train Loss 499.4521 | Test Loss 138.2782
Epoch 0002 | Time 0.394 (0.240) | NFE-F 46.6 | NFE-B 40.1 | Train Loss 282.9223 | Test Loss 40.5354
Epoch 0003 | Time 0.467 (0.247) | NFE-F 49.2 | NFE-B 41.3 | Train Loss 121.6102 | Test Loss 52.4800
Epoch 0004 | Time 0.480 (0.255) | NFE-F 52.3 | NFE-B 42.7 | Train Loss 333.3083 | Test Loss 44.4251
Epoch 0005 | Time 2.230 (0.301) | NFE-F 63.8 | NFE-B 51.3 | Train Loss 955.1534 | Test Loss 400.6032
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.183 (0.183) | NFE-F 32.0 | NFE-B 27.0 | Train Loss 735.1126 | Test Loss 251.0374
Epoch 0001 | Time 0.189 (0.184) | NFE-F 32.6 | NFE-B 27.2 | Train Loss 603.7531 | Test Loss 213.9929
Epoch 0002 | Time 0.232 (0.185) | NFE-F 33.3 | NFE-B 27.4 | Train Loss 547.4236 | Test Loss 203.1864
Epoch 0003 | Time 0.387 (0.190) | NFE-F 34.8 | NFE-B 28.3 | Train Loss 584.7150 | Test Loss 185.3877
Epoch 0004 | Time 0.565 (0.200) | NFE-F 37.7 | NFE-B 30.2 | Train Loss 519.7109 | Test Loss 157.2892
Epoch 0005 | Time 0.656 (0.214) | NFE-F 41.5 | NFE-B 32.8 | Train Loss 446.4230 | Test Loss 128.7980
Epoch 0006 | Time 0.713 (0.230) | NFE-F 45.8 | NFE-B 35.8 | Train Loss 443.8549 | Test Loss 106.0300
Epoch 0007 | Time 0.791 (0.247) | NFE-F 50.3 | NFE-B 39.0 | Train Loss 350.0882 | Test Loss 84.2207
Epoch 0008 | Time 0.766 (0.264) | NFE-F 54.9 | NFE-B 42.2 | Train Loss 377.8639 | Test Loss 65.7390
Epoch 0009 | Time 0.811 (0.281) | NFE-F 59.4 | NFE-B 45.4 | Train Loss 293.3192 | Test Loss 51.2207
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y) / y.shape[0]


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.245 (0.245) | NFE-F 38.0 | NFE-B 45.0 | Train Loss 6.0786 | Test Loss 2.1825
Epoch 0001 | Time 0.209 (0.245) | NFE-F 38.5 | NFE-B 44.8 | Train Loss 5.3443 | Test Loss 1.8617
Epoch 0002 | Time 0.283 (0.246) | NFE-F 39.2 | NFE-B 44.7 | Train Loss 4.5985 | Test Loss 1.7586
Epoch 0003 | Time 0.303 (0.248) | NFE-F 39.9 | NFE-B 44.9 | Train Loss 4.1664 | Test Loss 1.5263
Epoch 0004 | Time 0.505 (0.253) | NFE-F 41.9 | NFE-B 45.7 | Train Loss 4.5380 | Test Loss 1.4357
Epoch 0005 | Time 0.496 (0.261) | NFE-F 45.1 | NFE-B 47.1 | Train Loss 4.0671 | Test Loss 1.3964
Epoch 0006 | Time 0.555 (0.272) | NFE-F 48.2 | NFE-B 48.9 | Train Loss 4.1890 | Test Loss 1.4152
Epoch 0007 | Time 0.531 (0.280) | NFE-F 51.3 | NFE-B 50.3 | Train Loss 3.8256 | Test Loss 1.1906
Epoch 0008 | Time 0.611 (0.291) | NFE-F 54.3 | NFE-B 52.2 | Train Loss 3.3621 | Test Loss 0.9687
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.212 (0.212) | NFE-F 38.0 | NFE-B 33.0 | Train Loss 618.9880 | Test Loss 243.6745
Epoch 0001 | Time 0.211 (0.213) | NFE-F 38.6 | NFE-B 33.0 | Train Loss 626.6074 | Test Loss 228.5138
Epoch 0002 | Time 0.236 (0.214) | NFE-F 39.5 | NFE-B 33.2 | Train Loss 617.7850 | Test Loss 204.6622
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.228 (0.228) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 662.3463 | Test Loss 263.5746
Epoch 0001 | Time 0.233 (0.228) | NFE-F 32.7 | NFE-B 33.1 | Train Loss 706.4435 | Test Loss 236.8170
Epoch 0002 | Time 0.207 (0.227) | NFE-F 33.4 | NFE-B 33.1 | Train Loss 573.1531 | Test Loss 213.9051
Epoch 0003 | Time 0.203 (0.228) | NFE-F 34.4 | NFE-B 33.1 | Train Loss 613.4490 | Test Loss 189.5848
Epoch 0004 | Time 0.231 (0.228) | NFE-F 35.3 | NFE-B 33.2 | Train Loss 567.4155 | Test Loss 160.6470
Epoch 0005 | Time 0.258 (0.229) | NFE-F 36.2 | NFE-B 33.5 | Train Loss 440.4854 | Test Loss 131.3923
Epoch 0006 | Time 0.234 (0.230) | NFE-F 37.3 | NFE-B 33.7 | Train Loss 434.6211 | Test Loss 104.3385
Epoch 0007 | Time 0.251 (0.231) | NFE-F 38.4 | NFE-B 34.0 | Train Loss 340.3719 | Test Loss 81.0502
Epoch 0008 | Time 0.248 (0.232) | NFE-F 39.4 | NFE-B 34.3 | Train Loss 265.4019 | Test Loss 62.2449
Epoch 0009 | Time 0.248 (0.233) | NFE-F 40.4 | NFE-B 34.7 | Train Loss 250.5754 | Test Loss 48.0350
Epoch 0010 | Time 0.260 (0.234) | NFE-F 41.4 | NFE-B 35.0 | Train Loss 252.3688 | Test Loss 38.2129
Epoch 0011 | Time 0.267 (0.236) | NFE-F 42.3 | NFE-B 35.3 | Train Loss 224.5994 | Test Loss 32.2826
Epoch 0012 | Time 0.257 (0.237) | NFE-F 43.2 | NFE-B 35.6 | Train Loss 179.5264 | Test Loss 29.6976
Epoch 0013 | Time 0.275 (0.239) | NFE-F 44.1 | NFE-B 35.8 | Train Loss 186.6126 | Test Loss 29.8234
Epoch 0014 | Time 0.292 (0.240) | NFE-F 45.0 | NFE-B 36.2 | Train Loss 147.6900 | Test Loss 32.0677
Epoch 0015 | Time 0.282 (0.242) | NFE-F 45.8 | NFE-B 36.6 | Train Loss 118.6263 | Test Loss 35.8670
Epoch 0016 | Time 0.289 (0.243) | NFE-F 46.7 | NFE-B 36.9 | Train Loss 139.3462 | Test Loss 40.8070
Epoch 0017 | Time 0.262 (0.244) | NFE-F 47.4 | NFE-B 37.0 | Train Loss 122.7762 | Test Loss 46.4658
Epoch 0018 | Time 0.279 (0.245) | NFE-F 48.3 | NFE-B 37.2 | Train Loss 104.9142 | Test Loss 52.5457
Epoch 0019 | Time 0.245 (0.246) | NFE-F 49.1 | NFE-B 37.4 | Train Loss 82.1630 | Test Loss 58.7456
Epoch 0020 | Time 0.260 (0.248) | NFE-F 49.8 | NFE-B 37.7 | Train Loss 98.7429 | Test Loss 64.9601
Epoch 0021 | Time 0.281 (0.249) | NFE-F 50.5 | NFE-B 38.0 | Train Loss 92.3196 | Test Loss 70.9592
Epoch 0022 | Time 0.251 (0.249) | NFE-F 51.1 | NFE-B 38.1 | Train Loss 64.9523 | Test Loss 76.6112
Epoch 0023 | Time 0.229 (0.250) | NFE-F 51.8 | NFE-B 38.2 | Train Loss 86.6711 | Test Loss 81.8900
Epoch 0024 | Time 0.232 (0.250) | NFE-F 52.5 | NFE-B 38.3 | Train Loss 60.4305 | Test Loss 86.8178
Epoch 0025 | Time 0.260 (0.251) | NFE-F 53.2 | NFE-B 38.5 | Train Loss 107.7512 | Test Loss 91.3833
Epoch 0026 | Time 0.236 (0.252) | NFE-F 53.8 | NFE-B 38.7 | Train Loss 65.6722 | Test Loss 95.4597
Epoch 0027 | Time 0.264 (0.252) | NFE-F 54.4 | NFE-B 38.8 | Train Loss 88.3076 | Test Loss 99.1928
Epoch 0028 | Time 0.266 (0.253) | NFE-F 55.0 | NFE-B 39.0 | Train Loss 75.5328 | Test Loss 102.5547
Epoch 0029 | Time 0.293 (0.254) | NFE-F 55.6 | NFE-B 39.3 | Train Loss 69.0020 | Test Loss 105.5478
Epoch 0030 | Time 0.272 (0.255) | NFE-F 56.1 | NFE-B 39.4 | Train Loss 88.5832 | Test Loss 108.1494
Epoch 0031 | Time 0.259 (0.255) | NFE-F 56.6 | NFE-B 39.5 | Train Loss 85.0921 | Test Loss 110.4409
Epoch 0032 | Time 0.273 (0.257) | NFE-F 57.2 | NFE-B 39.7 | Train Loss 76.9789 | Test Loss 112.4695
Epoch 0033 | Time 0.264 (0.257) | NFE-F 57.7 | NFE-B 39.8 | Train Loss 94.7499 | Test Loss 114.2755
Epoch 0034 | Time 0.249 (0.258) | NFE-F 58.1 | NFE-B 40.0 | Train Loss 71.1573 | Test Loss 115.8535
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.166 (0.166) | NFE-F 38.0 | NFE-B 33.0 | Train Loss 805.6329 | Test Loss 288.0281
Epoch 0001 | Time 0.197 (0.166) | NFE-F 38.4 | NFE-B 33.1 | Train Loss 719.9923 | Test Loss 257.7717
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.163 (0.163) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 670.6434 | Test Loss 244.3192
Epoch 0001 | Time 0.176 (0.164) | NFE-F 32.9 | NFE-B 33.0 | Train Loss 637.2859 | Test Loss 235.8933
Epoch 0002 | Time 0.173 (0.165) | NFE-F 33.9 | NFE-B 33.0 | Train Loss 591.8273 | Test Loss 224.9910
Epoch 0003 | Time 0.207 (0.166) | NFE-F 34.9 | NFE-B 33.1 | Train Loss 591.1779 | Test Loss 223.9792
Epoch 0004 | Time 0.201 (0.167) | NFE-F 36.2 | NFE-B 33.2 | Train Loss 688.8801 | Test Loss 222.7567
Epoch 0005 | Time 0.192 (0.169) | NFE-F 37.6 | NFE-B 33.5 | Train Loss 598.4707 | Test Loss 221.7400
Epoch 0006 | Time 0.217 (0.170) | NFE-F 38.9 | NFE-B 33.7 | Train Loss 738.0494 | Test Loss 223.3667
Epoch 0007 | Time 0.230 (0.173) | NFE-F 40.1 | NFE-B 34.3 | Train Loss 609.9858 | Test Loss 219.4405
Epoch 0008 | Time 0.208 (0.174) | NFE-F 41.3 | NFE-B 34.6 | Train Loss 708.5753 | Test Loss 218.3132
Epoch 0009 | Time 0.230 (0.176) | NFE-F 42.3 | NFE-B 34.8 | Train Loss 640.1066 | Test Loss 217.1814
Epoch 0010 | Time 0.241 (0.178) | NFE-F 43.3 | NFE-B 35.3 | Train Loss 661.9683 | Test Loss 216.0510
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=384):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.212 (0.212) | NFE-F 38.0 | NFE-B 45.0 | Train Loss 586.4349 | Test Loss 248.9915
Epoch 0001 | Time 0.181 (0.211) | NFE-F 38.4 | NFE-B 44.8 | Train Loss 666.5888 | Test Loss 247.6119
Epoch 0002 | Time 0.182 (0.210) | NFE-F 39.0 | NFE-B 44.5 | Train Loss 620.6733 | Test Loss 244.1149
Epoch 0003 | Time 0.168 (0.209) | NFE-F 39.4 | NFE-B 44.1 | Train Loss 675.4838 | Test Loss 242.2402
Epoch 0004 | Time 0.165 (0.208) | NFE-F 39.8 | NFE-B 43.9 | Train Loss 660.5200 | Test Loss 234.1936
Epoch 0005 | Time 0.179 (0.208) | NFE-F 40.3 | NFE-B 43.7 | Train Loss 657.0601 | Test Loss 229.4370
Epoch 0006 | Time 0.188 (0.208) | NFE-F 40.8 | NFE-B 43.6 | Train Loss 608.3486 | Test Loss 226.5303
Epoch 0007 | Time 0.168 (0.207) | NFE-F 41.1 | NFE-B 43.3 | Train Loss 632.8154 | Test Loss 225.5533
Epoch 0008 | Time 0.181 (0.206) | NFE-F 41.5 | NFE-B 43.0 | Train Loss 654.5818 | Test Loss 225.3658
Epoch 0009 | Time 0.143 (0.205) | NFE-F 41.8 | NFE-B 42.5 | Train Loss 688.2258 | Test Loss 225.2605
Epoch 0010 | Time 0.145 (0.203) | NFE-F 42.2 | NFE-B 42.1 | Train Loss 589.2280 | Test Loss 225.1471
Epoch 0011 | Time 0.144 (0.202) | NFE-F 42.5 | NFE-B 41.6 | Train Loss 583.5652 | Test Loss 224.8501
Epoch 0012 | Time 0.146 (0.201) | NFE-F 42.8 | NFE-B 41.2 | Train Loss 683.4775 | Test Loss 224.0823
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.0001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.191 (0.191) | NFE-F 38.0 | NFE-B 39.0 | Train Loss 507.6303 | Test Loss 595.0251
Epoch 0001 | Time 0.286 (0.192) | NFE-F 38.8 | NFE-B 39.1 | Train Loss 505.5565 | Test Loss 687.1700
Epoch 0002 | Time 0.282 (0.193) | NFE-F 39.5 | NFE-B 39.1 | Train Loss 505.1530 | Test Loss 692.4229
Epoch 0003 | Time 0.268 (0.194) | NFE-F 40.3 | NFE-B 39.1 | Train Loss 505.0921 | Test Loss 683.9657
Epoch 0004 | Time 0.288 (0.195) | NFE-F 41.0 | NFE-B 39.1 | Train Loss 505.0312 | Test Loss 650.8014
Epoch 0005 | Time 0.292 (0.196) | NFE-F 41.7 | NFE-B 39.2 | Train Loss 504.9669 | Test Loss 596.7054
Epoch 0006 | Time 0.300 (0.197) | NFE-F 42.4 | NFE-B 39.2 | Train Loss 504.7753 | Test Loss 631.6790
Epoch 0007 | Time 0.284 (0.198) | NFE-F 43.0 | NFE-B 39.3 | Train Loss 504.7142 | Test Loss 651.6755
Epoch 0008 | Time 0.293 (0.199) | NFE-F 43.6 | NFE-B 39.3 | Train Loss 504.6533 | Test Loss 618.3767
Epoch 0009 | Time 0.260 (0.199) | NFE-F 44.2 | NFE-B 39.3 | Train Loss 504.5923 | Test Loss 584.4711
Epoch 0010 | Time 0.266 (0.200) | NFE-F 44.8 | NFE-B 39.3 | Train Loss 504.5313 | Test Loss 595.1605
Epoch 0011 | Time 0.239 (0.200) | NFE-F 45.4 | NFE-B 39.3 | Train Loss 504.4700 | Test Loss 566.2782
Epoch 0012 | Time 0.246 (0.201) | NFE-F 45.9 | NFE-B 39.2 | Train Loss 504.3109 | Test Loss 634.9953
Epoch 0013 | Time 0.269 (0.201) | NFE-F 46.4 | NFE-B 39.2 | Train Loss 503.9293 | Test Loss 622.4167
Epoch 0014 | Time 0.273 (0.202) | NFE-F 47.0 | NFE-B 39.2 | Train Loss 493.8051 | Test Loss 590.0338
Epoch 0015 | Time 0.263 (0.203) | NFE-F 47.5 | NFE-B 39.2 | Train Loss 484.4832 | Test Loss 568.4072
Epoch 0016 | Time 0.251 (0.203) | NFE-F 48.0 | NFE-B 39.1 | Train Loss 484.4301 | Test Loss 622.2552
Epoch 0017 | Time 0.256 (0.204) | NFE-F 48.4 | NFE-B 39.1 | Train Loss 484.3766 | Test Loss 602.6579
Epoch 0018 | Time 0.255 (0.204) | NFE-F 48.9 | NFE-B 39.1 | Train Loss 484.3225 | Test Loss 586.0432
Epoch 0019 | Time 0.252 (0.205) | NFE-F 49.4 | NFE-B 39.1 | Train Loss 484.2678 | Test Loss 568.9924
Epoch 0020 | Time 0.263 (0.205) | NFE-F 49.9 | NFE-B 39.1 | Train Loss 484.2125 | Test Loss 577.6136
Epoch 0021 | Time 0.259 (0.206) | NFE-F 50.3 | NFE-B 39.1 | Train Loss 484.1568 | Test Loss 647.0449
Epoch 0022 | Time 0.243 (0.206) | NFE-F 50.8 | NFE-B 39.1 | Train Loss 484.1007 | Test Loss 576.3937
Epoch 0023 | Time 0.261 (0.207) | NFE-F 51.4 | NFE-B 39.0 | Train Loss 484.0442 | Test Loss 624.2147
Epoch 0024 | Time 0.253 (0.207) | NFE-F 52.0 | NFE-B 39.0 | Train Loss 483.9875 | Test Loss 600.7332
Epoch 0025 | Time 0.240 (0.208) | NFE-F 52.5 | NFE-B 38.9 | Train Loss 483.9304 | Test Loss 631.4043
Epoch 0026 | Time 0.256 (0.208) | NFE-F 53.1 | NFE-B 38.8 | Train Loss 483.8731 | Test Loss 573.6719
Epoch 0027 | Time 0.252 (0.208) | NFE-F 53.6 | NFE-B 38.8 | Train Loss 483.8156 | Test Loss 555.7769
Epoch 0028 | Time 0.239 (0.209) | NFE-F 54.2 | NFE-B 38.7 | Train Loss 483.7579 | Test Loss 596.5919
Epoch 0029 | Time 0.240 (0.209) | NFE-F 54.7 | NFE-B 38.7 | Train Loss 483.7000 | Test Loss 546.9241
Epoch 0030 | Time 0.246 (0.209) | NFE-F 55.3 | NFE-B 38.6 | Train Loss 483.6419 | Test Loss 602.1259
Epoch 0031 | Time 0.266 (0.210) | NFE-F 55.8 | NFE-B 38.6 | Train Loss 483.5838 | Test Loss 598.5539
Epoch 0032 | Time 0.275 (0.211) | NFE-F 56.4 | NFE-B 38.6 | Train Loss 483.5255 | Test Loss 566.3891
Epoch 0033 | Time 0.276 (0.211) | NFE-F 56.9 | NFE-B 38.6 | Train Loss 483.4671 | Test Loss 540.8224
Epoch 0034 | Time 0.277 (0.212) | NFE-F 57.4 | NFE-B 38.6 | Train Loss 483.4086 | Test Loss 528.3402
Epoch 0035 | Time 0.274 (0.213) | NFE-F 58.0 | NFE-B 38.6 | Train Loss 483.3500 | Test Loss 697.0054
Epoch 0036 | Time 0.266 (0.213) | NFE-F 58.6 | NFE-B 38.6 | Train Loss 483.2916 | Test Loss 624.5490
Epoch 0037 | Time 0.295 (0.214) | NFE-F 59.1 | NFE-B 38.6 | Train Loss 483.2347 | Test Loss 613.1321
Epoch 0038 | Time 0.287 (0.215) | NFE-F 59.7 | NFE-B 38.6 | Train Loss 483.1754 | Test Loss 558.5924
Epoch 0039 | Time 0.280 (0.215) | NFE-F 60.2 | NFE-B 38.6 | Train Loss 483.1152 | Test Loss 681.6421
Epoch 0040 | Time 0.275 (0.216) | NFE-F 60.8 | NFE-B 38.6 | Train Loss 483.0561 | Test Loss 584.2094
Epoch 0041 | Time 0.266 (0.216) | NFE-F 61.3 | NFE-B 38.7 | Train Loss 482.9971 | Test Loss 571.0095
Epoch 0042 | Time 0.286 (0.217) | NFE-F 61.9 | NFE-B 38.7 | Train Loss 482.9382 | Test Loss 576.1780
Epoch 0043 | Time 0.267 (0.218) | NFE-F 62.5 | NFE-B 38.7 | Train Loss 482.8791 | Test Loss 588.6858
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.148 (0.148) | NFE-F 38.0 | NFE-B 27.0 | Train Loss 470.5889 | Test Loss 583.4218
Epoch 0001 | Time 0.254 (0.149) | NFE-F 38.8 | NFE-B 27.1 | Train Loss 452.0190 | Test Loss 522.6471
Epoch 0002 | Time 0.294 (0.150) | NFE-F 39.3 | NFE-B 27.2 | Train Loss 466.2327 | Test Loss 568.9974
Epoch 0003 | Time 0.314 (0.152) | NFE-F 40.2 | NFE-B 27.4 | Train Loss 451.0554 | Test Loss 568.9382
Epoch 0004 | Time 0.273 (0.153) | NFE-F 40.9 | NFE-B 27.5 | Train Loss 450.5436 | Test Loss 551.1550
Epoch 0005 | Time 0.277 (0.154) | NFE-F 41.7 | NFE-B 27.6 | Train Loss 450.0181 | Test Loss 619.2804
Epoch 0006 | Time 0.311 (0.156) | NFE-F 42.4 | NFE-B 27.8 | Train Loss 449.4831 | Test Loss 539.1372
Epoch 0007 | Time 0.298 (0.157) | NFE-F 43.1 | NFE-B 28.0 | Train Loss 448.9412 | Test Loss 569.9101
Epoch 0008 | Time 0.287 (0.159) | NFE-F 43.9 | NFE-B 28.1 | Train Loss 448.4424 | Test Loss 592.9186
Epoch 0009 | Time 0.297 (0.160) | NFE-F 44.8 | NFE-B 28.2 | Train Loss 448.0094 | Test Loss 582.7579
Epoch 0010 | Time 0.350 (0.162) | NFE-F 45.9 | NFE-B 28.4 | Train Loss 447.4572 | Test Loss 605.1020
Epoch 0011 | Time 0.324 (0.164) | NFE-F 46.9 | NFE-B 28.6 | Train Loss 446.7353 | Test Loss 509.2994
Epoch 0012 | Time 0.323 (0.165) | NFE-F 47.8 | NFE-B 28.8 | Train Loss 446.1754 | Test Loss 487.1174
Epoch 0013 | Time 0.314 (0.167) | NFE-F 48.6 | NFE-B 28.9 | Train Loss 445.6159 | Test Loss 610.0585
Epoch 0014 | Time 0.312 (0.168) | NFE-F 49.4 | NFE-B 29.1 | Train Loss 445.0551 | Test Loss 556.0649
Epoch 0015 | Time 0.318 (0.170) | NFE-F 50.3 | NFE-B 29.3 | Train Loss 444.4935 | Test Loss 554.2475
Epoch 0016 | Time 0.320 (0.171) | NFE-F 51.1 | NFE-B 29.4 | Train Loss 443.9312 | Test Loss 495.4970
Epoch 0017 | Time 0.345 (0.173) | NFE-F 51.9 | NFE-B 29.6 | Train Loss 443.3682 | Test Loss 554.3510
Epoch 0018 | Time 0.369 (0.175) | NFE-F 52.9 | NFE-B 29.9 | Train Loss 442.8050 | Test Loss 631.5110
Epoch 0019 | Time 0.322 (0.176) | NFE-F 53.9 | NFE-B 30.0 | Train Loss 442.2414 | Test Loss 572.8087
Epoch 0020 | Time 0.338 (0.178) | NFE-F 54.9 | NFE-B 30.3 | Train Loss 441.6777 | Test Loss 571.7493
Epoch 0021 | Time 0.360 (0.180) | NFE-F 55.8 | NFE-B 30.5 | Train Loss 442.2476 | Test Loss 578.1234
Epoch 0022 | Time 0.365 (0.182) | NFE-F 56.8 | NFE-B 30.7 | Train Loss 441.7941 | Test Loss 599.5827
Epoch 0023 | Time 0.366 (0.184) | NFE-F 57.7 | NFE-B 31.0 | Train Loss 441.2367 | Test Loss 522.3663
Epoch 0024 | Time 0.395 (0.186) | NFE-F 58.6 | NFE-B 31.3 | Train Loss 441.7095 | Test Loss 485.7586
Epoch 0025 | Time 0.348 (0.187) | NFE-F 59.5 | NFE-B 31.5 | Train Loss 438.8637 | Test Loss 574.3048
Epoch 0026 | Time 0.357 (0.189) | NFE-F 60.4 | NFE-B 31.7 | Train Loss 438.3021 | Test Loss 511.2907
Epoch 0027 | Time 0.368 (0.191) | NFE-F 61.3 | NFE-B 32.0 | Train Loss 437.7405 | Test Loss 588.1959
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.160 (0.160) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 470.9152 | Test Loss 585.1619
Epoch 0001 | Time 0.314 (0.161) | NFE-F 33.0 | NFE-B 33.1 | Train Loss 457.7957 | Test Loss 496.6654
Epoch 0002 | Time 0.590 (0.166) | NFE-F 35.4 | NFE-B 33.8 | Train Loss 457.4163 | Test Loss 551.8701
Epoch 0003 | Time 0.744 (0.172) | NFE-F 38.7 | NFE-B 34.9 | Train Loss 452.9536 | Test Loss 608.8598
Epoch 0004 | Time 0.846 (0.178) | NFE-F 42.4 | NFE-B 36.0 | Train Loss 448.1871 | Test Loss 599.7330
Epoch 0005 | Time 0.887 (0.185) | NFE-F 46.1 | NFE-B 37.3 | Train Loss 443.2383 | Test Loss 572.1360
Epoch 0006 | Time 0.972 (0.193) | NFE-F 50.2 | NFE-B 38.7 | Train Loss 438.1751 | Test Loss 482.1218
Epoch 0007 | Time 1.002 (0.201) | NFE-F 54.5 | NFE-B 40.2 | Train Loss 433.0400 | Test Loss 502.3980
Epoch 0008 | Time 0.977 (0.209) | NFE-F 58.9 | NFE-B 41.5 | Train Loss 427.8608 | Test Loss 584.6394
Epoch 0009 | Time 0.980 (0.217) | NFE-F 63.4 | NFE-B 42.9 | Train Loss 422.6574 | Test Loss 578.6235
Epoch 0010 | Time 0.994 (0.225) | NFE-F 67.8 | NFE-B 44.2 | Train Loss 417.4440 | Test Loss 570.3111
Epoch 0011 | Time 0.982 (0.232) | NFE-F 72.4 | NFE-B 45.6 | Train Loss 435.1523 | Test Loss 590.7297
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.1, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.158 (0.158) | NFE-F 32.0 | NFE-B 33.0 | Train Loss 451.2704 | Test Loss 525.6111
Epoch 0001 | Time 0.541 (0.161) | NFE-F 34.4 | NFE-B 33.6 | Train Loss 435.2757 | Test Loss 564.2970
Epoch 0002 | Time 0.692 (0.167) | NFE-F 37.7 | NFE-B 34.6 | Train Loss 391.5129 | Test Loss 471.4174
Epoch 0003 | Time 0.697 (0.172) | NFE-F 40.9 | NFE-B 35.4 | Train Loss 377.2156 | Test Loss 498.7661
Epoch 0004 | Time 0.693 (0.177) | NFE-F 43.8 | NFE-B 36.3 | Train Loss 685.9825 | Test Loss 667.5878
Epoch 0005 | Time 0.699 (0.183) | NFE-F 46.7 | NFE-B 37.2 | Train Loss 750.6708 | Test Loss 816.9667
Epoch 0006 | Time 0.664 (0.187) | NFE-F 49.3 | NFE-B 38.1 | Train Loss 752.7164 | Test Loss 838.8716
Epoch 0007 | Time 0.663 (0.192) | NFE-F 51.8 | NFE-B 38.9 | Train Loss 737.0453 | Test Loss 773.1858
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.functional.linear(x, w, b)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Tanh(), nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=True, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=3, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): Tanh()
        (6): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 178884
Epoch 0000 | Time 0.221 (0.221) | NFE-F 38.0 | NFE-B 27.0 | Train Loss 468.5544 | Test Loss 581.4703
Epoch 0001 | Time 0.462 (0.223) | NFE-F 39.4 | NFE-B 27.3 | Train Loss 464.7775 | Test Loss 637.5588
Epoch 0002 | Time 0.459 (0.226) | NFE-F 40.6 | NFE-B 27.6 | Train Loss 460.2292 | Test Loss 605.6552
Epoch 0003 | Time 0.526 (0.229) | NFE-F 42.1 | NFE-B 28.0 | Train Loss 455.3852 | Test Loss 623.1141
Epoch 0004 | Time 0.522 (0.232) | NFE-F 43.5 | NFE-B 28.4 | Train Loss 450.3484 | Test Loss 589.0645
Epoch 0005 | Time 0.511 (0.234) | NFE-F 45.0 | NFE-B 28.8 | Train Loss 445.1977 | Test Loss 532.2397
Epoch 0006 | Time 0.518 (0.237) | NFE-F 46.4 | NFE-B 29.2 | Train Loss 440.0753 | Test Loss 537.6068
Epoch 0007 | Time 0.541 (0.240) | NFE-F 47.8 | NFE-B 29.6 | Train Loss 434.8224 | Test Loss 595.6570
Epoch 0008 | Time 0.517 (0.243) | NFE-F 49.3 | NFE-B 30.0 | Train Loss 429.4773 | Test Loss 576.5524
Epoch 0009 | Time 0.536 (0.246) | NFE-F 50.9 | NFE-B 30.4 | Train Loss 424.3773 | Test Loss 516.5137
Epoch 0010 | Time 0.535 (0.249) | NFE-F 52.5 | NFE-B 30.8 | Train Loss 419.1143 | Test Loss 517.3851
Epoch 0011 | Time 0.522 (0.252) | NFE-F 54.0 | NFE-B 31.2 | Train Loss 413.8644 | Test Loss 519.8522
Epoch 0012 | Time 0.540 (0.255) | NFE-F 55.5 | NFE-B 31.6 | Train Loss 408.5424 | Test Loss 430.5510
Epoch 0013 | Time 0.542 (0.257) | NFE-F 57.1 | NFE-B 31.9 | Train Loss 401.8046 | Test Loss 485.5120
Epoch 0014 | Time 0.535 (0.260) | NFE-F 58.6 | NFE-B 32.3 | Train Loss 549.5036 | Test Loss 678.2479
Epoch 0015 | Time 0.537 (0.263) | NFE-F 60.1 | NFE-B 32.7 | Train Loss 563.0120 | Test Loss 810.4058
Epoch 0016 | Time 0.692 (0.267) | NFE-F 62.1 | NFE-B 33.3 | Train Loss 514.2405 | Test Loss 634.9647
Epoch 0017 | Time 0.740 (0.272) | NFE-F 64.4 | NFE-B 33.9 | Train Loss 443.5467 | Test Loss 550.6959
Epoch 0018 | Time 0.882 (0.278) | NFE-F 67.0 | NFE-B 34.8 | Train Loss 413.2216 | Test Loss 596.5928
Epoch 0019 | Time 0.810 (0.283) | NFE-F 69.5 | NFE-B 35.6 | Train Loss 449.6751 | Test Loss 570.7104
Epoch 0020 | Time 0.815 (0.289) | NFE-F 72.3 | NFE-B 36.3 | Train Loss 537.9579 | Test Loss 612.2883
Epoch 0021 | Time 0.840 (0.294) | NFE-F 75.1 | NFE-B 37.0 | Train Loss 574.1302 | Test Loss 705.8381
Epoch 0022 | Time 0.872 (0.300) | NFE-F 78.0 | NFE-B 37.8 | Train Loss 576.1250 | Test Loss 730.4046
Epoch 0023 | Time 0.855 (0.306) | NFE-F 81.0 | NFE-B 38.5 | Train Loss 566.3842 | Test Loss 761.0871
Epoch 0024 | Time 0.910 (0.312) | NFE-F 84.0 | NFE-B 39.3 | Train Loss 536.4471 | Test Loss 651.4460
Epoch 0025 | Time 0.997 (0.318) | NFE-F 87.0 | NFE-B 40.2 | Train Loss 474.0239 | Test Loss 675.3995
Epoch 0026 | Time 0.951 (0.325) | NFE-F 90.2 | NFE-B 41.1 | Train Loss 460.2142 | Test Loss 572.0376
Epoch 0027 | Time 0.880 (0.330) | NFE-F 93.3 | NFE-B 41.8 | Train Loss 428.0672 | Test Loss 541.5227
Epoch 0028 | Time 0.891 (0.336) | NFE-F 96.4 | NFE-B 42.6 | Train Loss 412.9895 | Test Loss 596.3389
Epoch 0029 | Time 0.961 (0.342) | NFE-F 99.6 | NFE-B 43.4 | Train Loss 424.2449 | Test Loss 505.2204
Epoch 0030 | Time 1.002 (0.349) | NFE-F 102.8 | NFE-B 44.3 | Train Loss 411.5221 | Test Loss 480.0962
Epoch 0031 | Time 0.995 (0.355) | NFE-F 106.0 | NFE-B 45.1 | Train Loss 440.6873 | Test Loss 555.2549
Epoch 0032 | Time 1.022 (0.362) | NFE-F 109.1 | NFE-B 46.1 | Train Loss 454.2672 | Test Loss 547.6797
Epoch 0033 | Time 0.966 (0.368) | NFE-F 112.4 | NFE-B 46.9 | Train Loss 494.0837 | Test Loss 598.6237
Epoch 0034 | Time 1.030 (0.375) | NFE-F 115.7 | NFE-B 47.8 | Train Loss 513.0227 | Test Loss 650.7415
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return self.activation(0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b)))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # pdb.set_trace()

        return nn.Tanh(0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b)))


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = self.activation(0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b)))

        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))

        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.056 (0.056) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 17055.8438 | Test Loss 21669.3867
Epoch 0001 | Time 0.118 (0.056) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 16842.2441 | Test Loss 23555.5977
Epoch 0002 | Time 0.117 (0.057) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 16630.0742 | Test Loss 23044.0605
Epoch 0003 | Time 0.118 (0.058) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 16419.3574 | Test Loss 22363.6719
Epoch 0004 | Time 0.118 (0.058) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 16210.1064 | Test Loss 22742.4746
Epoch 0005 | Time 0.115 (0.059) | NFE-F 15.4 | NFE-B 0.0 | Train Loss 16002.3496 | Test Loss 22453.7012
Epoch 0006 | Time 0.124 (0.059) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 15796.1104 | Test Loss 22228.7285
Epoch 0007 | Time 0.125 (0.060) | NFE-F 15.9 | NFE-B 0.0 | Train Loss 15591.3994 | Test Loss 22603.9551
Epoch 0008 | Time 0.120 (0.061) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 15388.2451 | Test Loss 21418.2031
Epoch 0009 | Time 0.113 (0.061) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 15186.6572 | Test Loss 21093.1445
Epoch 0010 | Time 0.117 (0.062) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 14986.6592 | Test Loss 20107.5801
Epoch 0011 | Time 0.115 (0.062) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 14788.2607 | Test Loss 20052.5879
Epoch 0012 | Time 0.118 (0.063) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 14591.4805 | Test Loss 20224.2051
Epoch 0013 | Time 0.120 (0.063) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 14396.3271 | Test Loss 19277.9707
Epoch 0014 | Time 0.113 (0.064) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 14202.8252 | Test Loss 19518.1074
Epoch 0015 | Time 0.113 (0.064) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 14010.9746 | Test Loss 19586.5996
Epoch 0016 | Time 0.114 (0.065) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 13820.7949 | Test Loss 18505.0898
Epoch 0017 | Time 0.111 (0.065) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 13632.2861 | Test Loss 17959.0820
Epoch 0018 | Time 0.114 (0.066) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 13445.4668 | Test Loss 19359.7285
Epoch 0019 | Time 0.113 (0.066) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 13260.3350 | Test Loss 18336.4414
Epoch 0020 | Time 0.123 (0.067) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 13076.9072 | Test Loss 17780.9551
Epoch 0021 | Time 0.122 (0.067) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 12895.1855 | Test Loss 17448.9785
Epoch 0022 | Time 0.115 (0.068) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 12715.1758 | Test Loss 17611.4453
Epoch 0023 | Time 0.112 (0.068) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 12536.8799 | Test Loss 18031.8203
Epoch 0024 | Time 0.114 (0.069) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 12360.2988 | Test Loss 17762.7871
Epoch 0025 | Time 0.116 (0.069) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 12185.4434 | Test Loss 15883.4277
Epoch 0026 | Time 0.111 (0.070) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 12012.3164 | Test Loss 16260.5068
Epoch 0027 | Time 0.118 (0.070) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 11840.9023 | Test Loss 16218.2324
Epoch 0028 | Time 0.115 (0.071) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 11671.2178 | Test Loss 16225.4375
Epoch 0029 | Time 0.113 (0.071) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 11503.2500 | Test Loss 15750.5869
Epoch 0030 | Time 0.117 (0.072) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 11337.0088 | Test Loss 15610.2764
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))

        return nn.functional.tanh(out)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.046 (0.046) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 856.9441 | Test Loss 6181.7612
Epoch 0001 | Time 0.111 (0.046) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 820.0493 | Test Loss 7537.9609
Epoch 0002 | Time 0.117 (0.047) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 784.5643 | Test Loss 5644.5625
Epoch 0003 | Time 0.121 (0.048) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 750.5105 | Test Loss 6135.8569
Epoch 0004 | Time 0.118 (0.048) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 717.9019 | Test Loss 6148.6006
Epoch 0005 | Time 0.118 (0.049) | NFE-F 15.4 | NFE-B 0.0 | Train Loss 686.7537 | Test Loss 6214.1929
Epoch 0006 | Time 0.122 (0.050) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 657.0758 | Test Loss 5236.4233
Epoch 0007 | Time 0.120 (0.051) | NFE-F 15.9 | NFE-B 0.0 | Train Loss 628.8735 | Test Loss 6700.5176
Epoch 0008 | Time 0.120 (0.051) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 602.1486 | Test Loss 5866.9263
Epoch 0009 | Time 0.118 (0.052) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 576.9004 | Test Loss 5408.4297
Epoch 0010 | Time 0.115 (0.053) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 553.1202 | Test Loss 6813.9492
Epoch 0011 | Time 0.125 (0.053) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 530.7990 | Test Loss 5199.5288
Epoch 0012 | Time 0.116 (0.054) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 509.9183 | Test Loss 5915.8267
Epoch 0013 | Time 0.114 (0.055) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 490.4603 | Test Loss 5361.5645
Epoch 0014 | Time 0.118 (0.055) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 472.3979 | Test Loss 5524.5854
Epoch 0015 | Time 0.118 (0.056) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 455.7005 | Test Loss 5915.2065
Epoch 0016 | Time 0.116 (0.056) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 440.3346 | Test Loss 5478.7671
Epoch 0017 | Time 0.115 (0.057) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 426.2615 | Test Loss 4886.4341
Epoch 0018 | Time 0.116 (0.058) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 413.4324 | Test Loss 5556.5039
Epoch 0019 | Time 0.111 (0.058) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 401.8035 | Test Loss 5375.8794
Epoch 0020 | Time 0.121 (0.059) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 391.3209 | Test Loss 4494.4824
Epoch 0021 | Time 0.112 (0.059) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 381.9289 | Test Loss 4508.7949
Epoch 0022 | Time 0.115 (0.060) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 373.5667 | Test Loss 4333.5059
Epoch 0023 | Time 0.114 (0.060) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 366.1751 | Test Loss 4412.7754
Epoch 0024 | Time 0.114 (0.061) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 359.6885 | Test Loss 3975.8677
Epoch 0025 | Time 0.115 (0.061) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 354.0402 | Test Loss 3757.0278
Epoch 0026 | Time 0.115 (0.062) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 349.1648 | Test Loss 3831.1282
Epoch 0027 | Time 0.109 (0.062) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 344.9941 | Test Loss 4407.4209
Epoch 0028 | Time 0.114 (0.063) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 341.4613 | Test Loss 3491.0613
Epoch 0029 | Time 0.117 (0.064) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 338.4998 | Test Loss 4548.7007
Epoch 0030 | Time 0.116 (0.064) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 336.0451 | Test Loss 3839.7246
Epoch 0031 | Time 0.112 (0.065) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 334.0341 | Test Loss 4183.5708
Epoch 0032 | Time 0.113 (0.065) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 332.4053 | Test Loss 4470.4155
Epoch 0033 | Time 0.113 (0.065) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 331.1039 | Test Loss 4189.7266
Epoch 0034 | Time 0.118 (0.066) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 330.0735 | Test Loss 4303.4453
Epoch 0035 | Time 0.116 (0.066) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 329.2665 | Test Loss 4616.0225
Epoch 0036 | Time 0.119 (0.067) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 328.6359 | Test Loss 3634.9116
Epoch 0037 | Time 0.118 (0.068) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 328.1406 | Test Loss 3651.1377
Epoch 0038 | Time 0.119 (0.068) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 327.7443 | Test Loss 4058.6660
Epoch 0039 | Time 0.115 (0.069) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 327.4138 | Test Loss 4007.8564
Epoch 0040 | Time 0.112 (0.069) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 327.1222 | Test Loss 3613.0430
Epoch 0041 | Time 0.114 (0.069) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 326.8459 | Test Loss 3807.4333
Epoch 0042 | Time 0.114 (0.070) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 326.5646 | Test Loss 3324.5195
Epoch 0043 | Time 0.114 (0.070) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 326.2635 | Test Loss 3250.2192
Epoch 0044 | Time 0.112 (0.071) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 325.9317 | Test Loss 4082.6299
Epoch 0045 | Time 0.113 (0.071) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 325.5600 | Test Loss 3563.9597
Epoch 0046 | Time 0.116 (0.072) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 325.1423 | Test Loss 3594.2842
Epoch 0047 | Time 0.113 (0.072) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 324.6769 | Test Loss 3675.1646
Epoch 0048 | Time 0.115 (0.072) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 324.1621 | Test Loss 3376.7515
Epoch 0049 | Time 0.115 (0.073) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 323.5999 | Test Loss 3700.1665
Epoch 0050 | Time 0.114 (0.073) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 322.9925 | Test Loss 4625.3901
Epoch 0051 | Time 0.117 (0.074) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 322.3428 | Test Loss 4384.4463
Epoch 0052 | Time 0.110 (0.074) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 321.6560 | Test Loss 4210.4673
Epoch 0053 | Time 0.112 (0.074) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 320.9377 | Test Loss 3871.4883
Epoch 0054 | Time 0.115 (0.075) | NFE-F 25.7 | NFE-B 0.0 | Train Loss 320.1915 | Test Loss 3156.2695
Epoch 0055 | Time 0.122 (0.075) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 319.4245 | Test Loss 3695.7185
Epoch 0056 | Time 0.114 (0.076) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 318.6406 | Test Loss 3666.9639
Epoch 0057 | Time 0.116 (0.076) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 317.8456 | Test Loss 3334.0232
Epoch 0058 | Time 0.118 (0.077) | NFE-F 26.4 | NFE-B 0.0 | Train Loss 317.0434 | Test Loss 3451.2622
Epoch 0059 | Time 0.115 (0.077) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 316.2382 | Test Loss 3275.4065
Epoch 0060 | Time 0.115 (0.077) | NFE-F 26.7 | NFE-B 0.0 | Train Loss 315.4335 | Test Loss 3997.6421
Epoch 0061 | Time 0.114 (0.078) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 314.6333 | Test Loss 3785.7063
Epoch 0062 | Time 0.119 (0.078) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 313.8387 | Test Loss 3279.7034
Epoch 0063 | Time 0.129 (0.079) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 313.0522 | Test Loss 3976.6433
Epoch 0064 | Time 0.115 (0.079) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 312.2751 | Test Loss 3741.8066
Epoch 0065 | Time 0.118 (0.079) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 311.5078 | Test Loss 4100.1387
Epoch 0066 | Time 0.114 (0.080) | NFE-F 27.6 | NFE-B 0.0 | Train Loss 310.7516 | Test Loss 3574.8135
Epoch 0067 | Time 0.121 (0.080) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 310.0052 | Test Loss 3928.9795
Epoch 0068 | Time 0.116 (0.080) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 309.2690 | Test Loss 3114.1172
Epoch 0069 | Time 0.115 (0.081) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 308.5429 | Test Loss 3024.2278
Epoch 0070 | Time 0.116 (0.081) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 307.8252 | Test Loss 3125.5188
Epoch 0071 | Time 0.117 (0.082) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 307.1152 | Test Loss 3248.5422
Epoch 0072 | Time 0.118 (0.082) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 306.4120 | Test Loss 4032.0806
Epoch 0073 | Time 0.112 (0.082) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 305.7146 | Test Loss 2942.0618
Epoch 0074 | Time 0.112 (0.082) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 305.0215 | Test Loss 3742.4160
Epoch 0075 | Time 0.114 (0.083) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 304.3309 | Test Loss 3563.2673
Epoch 0076 | Time 0.110 (0.083) | NFE-F 29.0 | NFE-B 0.0 | Train Loss 303.6429 | Test Loss 3720.1399
Epoch 0077 | Time 0.115 (0.083) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 302.9559 | Test Loss 3344.2080
Epoch 0078 | Time 0.116 (0.084) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 302.2693 | Test Loss 3457.7004
Epoch 0079 | Time 0.112 (0.084) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 301.5821 | Test Loss 3297.0591
Epoch 0080 | Time 0.115 (0.084) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 300.8939 | Test Loss 3732.9082
Epoch 0081 | Time 0.115 (0.085) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 300.2038 | Test Loss 3521.8118
Epoch 0082 | Time 0.113 (0.085) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 299.5115 | Test Loss 3326.3242
Epoch 0083 | Time 0.114 (0.085) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 298.8167 | Test Loss 2929.8704
Epoch 0084 | Time 0.115 (0.085) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 298.1192 | Test Loss 3659.0913
Epoch 0085 | Time 0.112 (0.086) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 297.4190 | Test Loss 3678.3411
Epoch 0086 | Time 0.114 (0.086) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 296.7158 | Test Loss 3337.2371
Epoch 0087 | Time 0.111 (0.086) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 296.0099 | Test Loss 3836.9897
Epoch 0088 | Time 0.113 (0.087) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 295.3012 | Test Loss 3736.1545
Epoch 0089 | Time 0.113 (0.087) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 294.5899 | Test Loss 3890.0332
Epoch 0090 | Time 0.114 (0.087) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 293.8762 | Test Loss 3222.9802
Epoch 0091 | Time 0.115 (0.087) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 293.1600 | Test Loss 3703.5757
Epoch 0092 | Time 0.114 (0.088) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 292.4423 | Test Loss 3171.5344
Epoch 0093 | Time 0.114 (0.088) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 291.7223 | Test Loss 3316.2473
Epoch 0094 | Time 0.116 (0.088) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 291.0007 | Test Loss 3261.8347
Epoch 0095 | Time 0.108 (0.088) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 290.2778 | Test Loss 4002.0535
Epoch 0096 | Time 0.113 (0.089) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 289.5535 | Test Loss 3148.5730
Epoch 0097 | Time 0.110 (0.089) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 288.8281 | Test Loss 3070.2207
Epoch 0098 | Time 0.121 (0.089) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 288.1020 | Test Loss 3562.1289
Epoch 0099 | Time 0.113 (0.089) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 287.3750 | Test Loss 3565.0576
Epoch 0100 | Time 0.114 (0.090) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 286.6470 | Test Loss 3255.9084
Epoch 0101 | Time 0.116 (0.090) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 285.9184 | Test Loss 3478.3740
Epoch 0102 | Time 0.115 (0.090) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 285.1893 | Test Loss 3642.8811
Epoch 0103 | Time 0.119 (0.090) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 284.4594 | Test Loss 3516.1792
Epoch 0104 | Time 0.116 (0.091) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 283.7286 | Test Loss 2807.5271
Epoch 0105 | Time 0.115 (0.091) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 282.9973 | Test Loss 3608.3796
Epoch 0106 | Time 0.114 (0.091) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 282.2662 | Test Loss 3179.3081
Epoch 0107 | Time 0.117 (0.091) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 281.5338 | Test Loss 3199.0527
Epoch 0108 | Time 0.114 (0.092) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 280.8011 | Test Loss 3172.6838
Epoch 0109 | Time 0.112 (0.092) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 280.0680 | Test Loss 3350.9922
Epoch 0110 | Time 0.112 (0.092) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 279.3337 | Test Loss 3873.0857
Epoch 0111 | Time 0.110 (0.092) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 278.5992 | Test Loss 3023.0452
Epoch 0112 | Time 0.114 (0.092) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 277.8639 | Test Loss 3173.2976
Epoch 0113 | Time 0.115 (0.093) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 277.1286 | Test Loss 3245.0532
Epoch 0114 | Time 0.113 (0.093) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 276.3925 | Test Loss 2960.6060
Epoch 0115 | Time 0.116 (0.093) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 275.6554 | Test Loss 3262.7363
Epoch 0116 | Time 0.116 (0.093) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 274.9182 | Test Loss 3052.2771
Epoch 0117 | Time 0.113 (0.094) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 274.1798 | Test Loss 3382.0425
Epoch 0118 | Time 0.114 (0.094) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 273.4416 | Test Loss 3440.0964
Epoch 0119 | Time 0.116 (0.094) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 272.7025 | Test Loss 3275.1348
Epoch 0120 | Time 0.115 (0.094) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 271.9632 | Test Loss 2806.0083
Epoch 0121 | Time 0.111 (0.094) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 271.2231 | Test Loss 2977.9480
Epoch 0122 | Time 0.116 (0.095) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 270.4829 | Test Loss 3032.7959
Epoch 0123 | Time 0.116 (0.095) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 269.7422 | Test Loss 3022.9260
Epoch 0124 | Time 0.111 (0.095) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 269.0008 | Test Loss 2743.9309
Epoch 0125 | Time 0.114 (0.095) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 268.2594 | Test Loss 3027.2683
Epoch 0126 | Time 0.109 (0.095) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 267.5177 | Test Loss 3333.1860
Epoch 0127 | Time 0.111 (0.095) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 266.7759 | Test Loss 2671.7019
Epoch 0128 | Time 0.113 (0.096) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 266.0334 | Test Loss 2930.0432
Epoch 0129 | Time 0.114 (0.096) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 265.2914 | Test Loss 2772.8960
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))

        return nn.functional.tanh(out)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.047 (0.047) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 3734.4541 | Test Loss 6258.1226
Epoch 0001 | Time 0.120 (0.048) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 2822.0552 | Test Loss 5335.5874
Epoch 0002 | Time 0.118 (0.048) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 2050.4822 | Test Loss 3672.5583
Epoch 0003 | Time 0.112 (0.049) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 1419.1489 | Test Loss 2391.2024
Epoch 0004 | Time 0.120 (0.050) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 925.0993 | Test Loss 1604.5825
Epoch 0005 | Time 0.116 (0.050) | NFE-F 15.4 | NFE-B 0.0 | Train Loss 562.3342 | Test Loss 1257.8947
Epoch 0006 | Time 0.122 (0.051) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 321.1712 | Test Loss 689.6342
Epoch 0007 | Time 0.120 (0.052) | NFE-F 15.9 | NFE-B 0.0 | Train Loss 187.8229 | Test Loss 520.1868
Epoch 0008 | Time 0.120 (0.052) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 144.4008 | Test Loss 407.7477
Epoch 0009 | Time 0.110 (0.053) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 169.5176 | Test Loss 381.4659
Epoch 0010 | Time 0.116 (0.054) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 239.4362 | Test Loss 462.8770
Epoch 0011 | Time 0.118 (0.054) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 330.3842 | Test Loss 590.1279
Epoch 0012 | Time 0.119 (0.055) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 421.6180 | Test Loss 601.0133
Epoch 0013 | Time 0.112 (0.055) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 497.3746 | Test Loss 508.0674
Epoch 0014 | Time 0.113 (0.056) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 547.6579 | Test Loss 684.2333
Epoch 0015 | Time 0.115 (0.057) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 568.0838 | Test Loss 776.9158
Epoch 0016 | Time 0.116 (0.057) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 559.0465 | Test Loss 798.0682
Epoch 0017 | Time 0.116 (0.058) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 524.5221 | Test Loss 557.2278
Epoch 0018 | Time 0.113 (0.058) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 470.7793 | Test Loss 666.1635
Epoch 0019 | Time 0.114 (0.059) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 405.1820 | Test Loss 533.0026
Epoch 0020 | Time 0.117 (0.059) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 335.1826 | Test Loss 429.7344
Epoch 0021 | Time 0.126 (0.060) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 267.5355 | Test Loss 384.0300
Epoch 0022 | Time 0.115 (0.061) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 207.7286 | Test Loss 337.6125
Epoch 0023 | Time 0.113 (0.061) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 159.6256 | Test Loss 299.5046
Epoch 0024 | Time 0.116 (0.062) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 125.3042 | Test Loss 219.7613
Epoch 0025 | Time 0.112 (0.062) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 105.0803 | Test Loss 248.5813
Epoch 0026 | Time 0.111 (0.063) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 97.7005 | Test Loss 277.5457
Epoch 0027 | Time 0.113 (0.063) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 100.6761 | Test Loss 249.5014
Epoch 0028 | Time 0.115 (0.064) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 110.7198 | Test Loss 263.3167
Epoch 0029 | Time 0.118 (0.064) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 124.2401 | Test Loss 358.2531
Epoch 0030 | Time 0.117 (0.065) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 137.8418 | Test Loss 318.6550
Epoch 0031 | Time 0.116 (0.065) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 148.7467 | Test Loss 380.3041
Epoch 0032 | Time 0.117 (0.066) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 155.0676 | Test Loss 331.5815
Epoch 0033 | Time 0.114 (0.066) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 155.9085 | Test Loss 421.2668
Epoch 0034 | Time 0.121 (0.067) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 151.3168 | Test Loss 363.5756
Epoch 0035 | Time 0.113 (0.067) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 142.1134 | Test Loss 325.4690
Epoch 0036 | Time 0.116 (0.068) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 129.6523 | Test Loss 339.6398
Epoch 0037 | Time 0.115 (0.068) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 115.5505 | Test Loss 264.5906
Epoch 0038 | Time 0.118 (0.069) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 101.4267 | Test Loss 249.2245
Epoch 0039 | Time 0.114 (0.069) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 88.6813 | Test Loss 267.3077
Epoch 0040 | Time 0.115 (0.070) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 78.3373 | Test Loss 191.1854
Epoch 0041 | Time 0.119 (0.070) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 70.9532 | Test Loss 170.4284
Epoch 0042 | Time 0.114 (0.071) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 66.6108 | Test Loss 185.7821
Epoch 0043 | Time 0.112 (0.071) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 64.9702 | Test Loss 206.4894
Epoch 0044 | Time 0.112 (0.071) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 65.3776 | Test Loss 175.4263
Epoch 0045 | Time 0.119 (0.072) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 67.0045 | Test Loss 151.9841
Epoch 0046 | Time 0.116 (0.072) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 68.9962 | Test Loss 154.1498
Epoch 0047 | Time 0.121 (0.073) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 70.6042 | Test Loss 179.7133
Epoch 0048 | Time 0.114 (0.073) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 71.2840 | Test Loss 168.6917
Epoch 0049 | Time 0.115 (0.074) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 70.7482 | Test Loss 162.7255
Epoch 0050 | Time 0.116 (0.074) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 68.9692 | Test Loss 143.3098
Epoch 0051 | Time 0.112 (0.074) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 66.1411 | Test Loss 145.1582
Epoch 0052 | Time 0.116 (0.075) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 62.6117 | Test Loss 133.1511
Epoch 0053 | Time 0.120 (0.075) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 58.7999 | Test Loss 150.5663
Epoch 0054 | Time 0.114 (0.076) | NFE-F 25.7 | NFE-B 0.0 | Train Loss 55.1154 | Test Loss 171.0116
Epoch 0055 | Time 0.120 (0.076) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 51.8925 | Test Loss 118.9482
Epoch 0056 | Time 0.116 (0.077) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 49.3465 | Test Loss 115.3293
Epoch 0057 | Time 0.115 (0.077) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 47.5572 | Test Loss 127.4671
Epoch 0058 | Time 0.118 (0.077) | NFE-F 26.4 | NFE-B 0.0 | Train Loss 46.4767 | Test Loss 126.3942
Epoch 0059 | Time 0.116 (0.078) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 45.9575 | Test Loss 120.8769
Epoch 0060 | Time 0.114 (0.078) | NFE-F 26.7 | NFE-B 0.0 | Train Loss 45.7929 | Test Loss 135.3600
Epoch 0061 | Time 0.113 (0.078) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 45.7606 | Test Loss 162.6935
Epoch 0062 | Time 0.113 (0.079) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 45.6631 | Test Loss 118.6001
Epoch 0063 | Time 0.119 (0.079) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 45.3570 | Test Loss 131.9917
Epoch 0064 | Time 0.117 (0.080) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 44.7684 | Test Loss 121.6329
Epoch 0065 | Time 0.117 (0.080) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 43.8943 | Test Loss 117.4296
Epoch 0066 | Time 0.114 (0.080) | NFE-F 27.6 | NFE-B 0.0 | Train Loss 42.7911 | Test Loss 120.0143
Epoch 0067 | Time 0.118 (0.081) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 41.5540 | Test Loss 92.0001
Epoch 0068 | Time 0.116 (0.081) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 40.2937 | Test Loss 101.3684
Epoch 0069 | Time 0.110 (0.081) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 39.1129 | Test Loss 131.2841
Epoch 0070 | Time 0.115 (0.082) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 38.0888 | Test Loss 97.1318
Epoch 0071 | Time 0.111 (0.082) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 37.2628 | Test Loss 84.7571
Epoch 0072 | Time 0.120 (0.082) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 36.6381 | Test Loss 95.0880
Epoch 0073 | Time 0.119 (0.083) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 36.1852 | Test Loss 80.2779
Epoch 0074 | Time 0.113 (0.083) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 35.8525 | Test Loss 72.8442
Epoch 0075 | Time 0.115 (0.083) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 35.5794 | Test Loss 85.9912
Epoch 0076 | Time 0.115 (0.084) | NFE-F 29.0 | NFE-B 0.0 | Train Loss 35.3089 | Test Loss 113.9570
Epoch 0077 | Time 0.113 (0.084) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 34.9978 | Test Loss 80.9926
Epoch 0078 | Time 0.119 (0.084) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 34.6223 | Test Loss 96.7403
Epoch 0079 | Time 0.120 (0.085) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 34.1794 | Test Loss 78.1775
Epoch 0080 | Time 0.116 (0.085) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 33.6834 | Test Loss 86.9584
Epoch 0081 | Time 0.119 (0.085) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 33.1606 | Test Loss 104.5142
Epoch 0082 | Time 0.115 (0.086) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 32.6413 | Test Loss 97.4753
Epoch 0083 | Time 0.119 (0.086) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 32.1533 | Test Loss 93.4893
Epoch 0084 | Time 0.115 (0.086) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 31.7164 | Test Loss 60.6498
Epoch 0085 | Time 0.118 (0.087) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 31.3395 | Test Loss 99.9180
Epoch 0086 | Time 0.112 (0.087) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 31.0207 | Test Loss 101.0087
Epoch 0087 | Time 0.113 (0.087) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 30.7491 | Test Loss 77.6784
Epoch 0088 | Time 0.120 (0.087) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 30.5085 | Test Loss 71.8308
Epoch 0089 | Time 0.118 (0.088) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 30.2815 | Test Loss 89.6384
Epoch 0090 | Time 0.118 (0.088) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 30.0531 | Test Loss 89.2940
Epoch 0091 | Time 0.114 (0.088) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 29.8134 | Test Loss 92.6964
Epoch 0092 | Time 0.113 (0.089) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 29.5585 | Test Loss 72.0374
Epoch 0093 | Time 0.116 (0.089) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 29.2905 | Test Loss 76.7743
Epoch 0094 | Time 0.113 (0.089) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 29.0158 | Test Loss 81.6161
Epoch 0095 | Time 0.116 (0.089) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 28.7426 | Test Loss 75.4653
Epoch 0096 | Time 0.113 (0.090) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 28.4793 | Test Loss 71.0581
Epoch 0097 | Time 0.116 (0.090) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 28.2321 | Test Loss 69.9637
Epoch 0098 | Time 0.117 (0.090) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 28.0040 | Test Loss 82.7095
Epoch 0099 | Time 0.115 (0.090) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 27.7950 | Test Loss 82.4512
Epoch 0100 | Time 0.115 (0.091) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 27.6018 | Test Loss 105.7501
Epoch 0101 | Time 0.115 (0.091) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 27.4198 | Test Loss 70.4263
Epoch 0102 | Time 0.117 (0.091) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 27.2437 | Test Loss 75.8079
Epoch 0103 | Time 0.115 (0.091) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 27.0691 | Test Loss 81.0410
Epoch 0104 | Time 0.115 (0.092) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 26.8930 | Test Loss 109.2619
Epoch 0105 | Time 0.115 (0.092) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 26.7144 | Test Loss 75.2710
Epoch 0106 | Time 0.119 (0.092) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 26.5340 | Test Loss 87.1131
Epoch 0107 | Time 0.118 (0.092) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 26.3537 | Test Loss 79.5524
Epoch 0108 | Time 0.117 (0.093) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 26.1760 | Test Loss 98.0479
Epoch 0109 | Time 0.117 (0.093) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 26.0032 | Test Loss 67.5884
Epoch 0110 | Time 0.113 (0.093) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 25.8369 | Test Loss 66.2728
Epoch 0111 | Time 0.116 (0.093) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 25.6777 | Test Loss 92.3443
Epoch 0112 | Time 0.112 (0.093) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 25.5251 | Test Loss 64.6184
Epoch 0113 | Time 0.120 (0.094) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 25.3779 | Test Loss 91.8336
Epoch 0114 | Time 0.115 (0.094) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 25.2346 | Test Loss 75.0247
Epoch 0115 | Time 0.115 (0.094) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 25.0936 | Test Loss 94.0395
Epoch 0116 | Time 0.117 (0.094) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 24.9537 | Test Loss 55.2583
Epoch 0117 | Time 0.111 (0.095) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 24.8143 | Test Loss 73.3406
Epoch 0118 | Time 0.112 (0.095) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 24.6754 | Test Loss 58.0740
Epoch 0119 | Time 0.111 (0.095) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 24.5373 | Test Loss 85.5528
Epoch 0120 | Time 0.122 (0.095) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 24.4008 | Test Loss 90.7836
Epoch 0121 | Time 0.115 (0.095) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 24.2666 | Test Loss 88.6259
Epoch 0122 | Time 0.118 (0.096) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 24.1353 | Test Loss 85.7916
Epoch 0123 | Time 0.114 (0.096) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 24.0071 | Test Loss 81.0815
Epoch 0124 | Time 0.120 (0.096) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 23.8819 | Test Loss 77.2661
Epoch 0125 | Time 0.121 (0.096) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 23.7595 | Test Loss 69.8997
Epoch 0126 | Time 0.119 (0.096) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 23.6393 | Test Loss 82.3582
Epoch 0127 | Time 0.113 (0.097) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 23.5208 | Test Loss 78.6106
Epoch 0128 | Time 0.115 (0.097) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 23.4036 | Test Loss 109.0177
Epoch 0129 | Time 0.113 (0.097) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 23.2876 | Test Loss 78.8530
Epoch 0130 | Time 0.113 (0.097) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 23.1726 | Test Loss 78.5596
Epoch 0131 | Time 0.114 (0.097) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 23.0588 | Test Loss 84.2086
Epoch 0132 | Time 0.111 (0.097) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 22.9465 | Test Loss 86.2020
Epoch 0133 | Time 0.116 (0.098) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 22.8357 | Test Loss 111.1555
Epoch 0134 | Time 0.118 (0.098) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 22.7268 | Test Loss 59.9334
Epoch 0135 | Time 0.116 (0.098) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 22.6197 | Test Loss 108.7345
Epoch 0136 | Time 0.119 (0.098) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 22.5145 | Test Loss 62.0797
Epoch 0137 | Time 0.119 (0.098) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 22.4109 | Test Loss 93.4774
Epoch 0138 | Time 0.118 (0.099) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 22.3089 | Test Loss 104.8590
Epoch 0139 | Time 0.114 (0.099) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 22.2082 | Test Loss 81.7114
Epoch 0140 | Time 0.117 (0.099) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 22.1087 | Test Loss 87.9046
Epoch 0141 | Time 0.121 (0.099) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 22.0104 | Test Loss 84.0680
Epoch 0142 | Time 0.120 (0.099) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 21.9133 | Test Loss 101.7329
Epoch 0143 | Time 0.118 (0.100) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 21.8173 | Test Loss 81.7947
Epoch 0144 | Time 0.123 (0.100) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 21.7226 | Test Loss 59.0580
Epoch 0145 | Time 0.123 (0.100) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 21.6293 | Test Loss 90.8870
Epoch 0146 | Time 0.116 (0.100) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 21.5373 | Test Loss 97.7007
Epoch 0147 | Time 0.113 (0.100) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 21.4466 | Test Loss 91.2481
Epoch 0148 | Time 0.113 (0.100) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 21.3573 | Test Loss 64.4590
Epoch 0149 | Time 0.117 (0.101) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 21.2693 | Test Loss 98.2918
Epoch 0150 | Time 0.124 (0.101) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 21.1824 | Test Loss 96.2186
Epoch 0151 | Time 0.118 (0.101) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 21.0967 | Test Loss 106.4221
Epoch 0152 | Time 0.116 (0.101) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 21.0122 | Test Loss 100.0842
Epoch 0153 | Time 0.114 (0.101) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 20.9287 | Test Loss 125.5718
Epoch 0154 | Time 0.114 (0.101) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 20.8463 | Test Loss 85.1038
Epoch 0155 | Time 0.114 (0.102) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 20.7650 | Test Loss 94.8414
Epoch 0156 | Time 0.115 (0.102) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 20.6849 | Test Loss 89.0538
Epoch 0157 | Time 0.121 (0.102) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 20.6059 | Test Loss 114.7473
Epoch 0158 | Time 0.122 (0.102) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 20.5281 | Test Loss 79.6042
Epoch 0159 | Time 0.117 (0.102) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 20.4513 | Test Loss 90.0532
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))

        return nn.functional.tanh(out)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.048 (0.048) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 2129.3613 | Test Loss 1489.7811
Epoch 0001 | Time 0.113 (0.048) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 1457.2434 | Test Loss 923.8147
Epoch 0002 | Time 0.122 (0.049) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 924.9436 | Test Loss 535.1953
Epoch 0003 | Time 0.120 (0.050) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 529.8906 | Test Loss 491.2634
Epoch 0004 | Time 0.116 (0.050) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 265.3188 | Test Loss 580.7397
Epoch 0005 | Time 0.117 (0.051) | NFE-F 15.4 | NFE-B 0.0 | Train Loss 119.0153 | Test Loss 580.9415
Epoch 0006 | Time 0.140 (0.052) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 72.4790 | Test Loss 1024.8163
Epoch 0007 | Time 0.140 (0.053) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 101.1229 | Test Loss 1445.5314
Epoch 0008 | Time 0.135 (0.054) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 176.0351 | Test Loss 1632.1176
Epoch 0009 | Time 0.139 (0.055) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 267.8606 | Test Loss 1786.8794
Epoch 0010 | Time 0.140 (0.055) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 351.7747 | Test Loss 1855.3273
Epoch 0011 | Time 0.157 (0.056) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 410.8494 | Test Loss 2421.3760
Epoch 0012 | Time 0.135 (0.057) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 436.9923 | Test Loss 2420.5735
Epoch 0013 | Time 0.113 (0.058) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 429.8718 | Test Loss 2340.7246
Epoch 0014 | Time 0.121 (0.058) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 394.7551 | Test Loss 2171.9792
Epoch 0015 | Time 0.118 (0.059) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 340.1628 | Test Loss 2160.2371
Epoch 0016 | Time 0.118 (0.060) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 275.7919 | Test Loss 1962.7643
Epoch 0017 | Time 0.115 (0.060) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 210.8947 | Test Loss 1745.1481
Epoch 0018 | Time 0.134 (0.061) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 153.2060 | Test Loss 1355.2798
Epoch 0019 | Time 0.135 (0.062) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 108.2162 | Test Loss 1151.5853
Epoch 0020 | Time 0.135 (0.062) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 78.8540 | Test Loss 993.5287
Epoch 0021 | Time 0.115 (0.063) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 65.4690 | Test Loss 834.2971
Epoch 0022 | Time 0.131 (0.064) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 66.1129 | Test Loss 776.8950
Epoch 0023 | Time 0.128 (0.064) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 77.0999 | Test Loss 612.5797
Epoch 0024 | Time 0.118 (0.065) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 93.7741 | Test Loss 612.1415
Epoch 0025 | Time 0.113 (0.065) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 111.3701 | Test Loss 519.0402
Epoch 0026 | Time 0.134 (0.066) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 125.8054 | Test Loss 541.8020
Epoch 0027 | Time 0.116 (0.066) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 134.2620 | Test Loss 599.8651
Epoch 0028 | Time 0.121 (0.067) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 135.4676 | Test Loss 382.8885
Epoch 0029 | Time 0.115 (0.067) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 129.6758 | Test Loss 491.4824
Epoch 0030 | Time 0.134 (0.068) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 118.3797 | Test Loss 565.4503
Epoch 0031 | Time 0.135 (0.069) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 103.8536 | Test Loss 512.9849
Epoch 0032 | Time 0.121 (0.069) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 88.6169 | Test Loss 613.4462
Epoch 0033 | Time 0.137 (0.070) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 74.9365 | Test Loss 578.1222
Epoch 0034 | Time 0.143 (0.071) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 64.4722 | Test Loss 685.0947
Epoch 0035 | Time 0.134 (0.071) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 58.0803 | Test Loss 762.0689
Epoch 0036 | Time 0.138 (0.072) | NFE-F 25.3 | NFE-B 0.0 | Train Loss 55.7837 | Test Loss 922.3324
Epoch 0037 | Time 0.136 (0.073) | NFE-F 25.7 | NFE-B 0.0 | Train Loss 56.8853 | Test Loss 968.8876
Epoch 0038 | Time 0.133 (0.073) | NFE-F 26.0 | NFE-B 0.0 | Train Loss 60.1967 | Test Loss 908.6016
Epoch 0039 | Time 0.137 (0.074) | NFE-F 26.4 | NFE-B 0.0 | Train Loss 64.3298 | Test Loss 1210.1107
Epoch 0040 | Time 0.136 (0.075) | NFE-F 26.7 | NFE-B 0.0 | Train Loss 67.9901 | Test Loss 1025.5812
Epoch 0041 | Time 0.139 (0.075) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 70.2120 | Test Loss 1070.6825
Epoch 0042 | Time 0.136 (0.076) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 70.4956 | Test Loss 1057.9199
Epoch 0043 | Time 0.136 (0.076) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 68.8291 | Test Loss 1191.0227
Epoch 0044 | Time 0.135 (0.077) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 65.6118 | Test Loss 983.5908
Epoch 0045 | Time 0.134 (0.078) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 61.5068 | Test Loss 899.0159
Epoch 0046 | Time 0.140 (0.078) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 57.2641 | Test Loss 718.3838
Epoch 0047 | Time 0.137 (0.079) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 53.5538 | Test Loss 820.3326
Epoch 0048 | Time 0.131 (0.079) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 50.8400 | Test Loss 607.5114
Epoch 0049 | Time 0.123 (0.080) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 49.3162 | Test Loss 731.1661
Epoch 0050 | Time 0.134 (0.080) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 48.9035 | Test Loss 764.7480
Epoch 0051 | Time 0.143 (0.081) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 49.3105 | Test Loss 737.1818
Epoch 0052 | Time 0.122 (0.081) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 50.1255 | Test Loss 690.1641
Epoch 0053 | Time 0.123 (0.082) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 50.9241 | Test Loss 611.2908
Epoch 0054 | Time 0.118 (0.082) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 51.3647 | Test Loss 722.6766
Epoch 0055 | Time 0.118 (0.082) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 51.2481 | Test Loss 698.9101
Epoch 0056 | Time 0.117 (0.083) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 50.5411 | Test Loss 555.9871
Epoch 0057 | Time 0.121 (0.083) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 49.3560 | Test Loss 627.2491
Epoch 0058 | Time 0.113 (0.083) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 47.9002 | Test Loss 539.2686
Epoch 0059 | Time 0.119 (0.084) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 46.4147 | Test Loss 473.9568
Epoch 0060 | Time 0.115 (0.084) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 45.1113 | Test Loss 729.5179
Epoch 0061 | Time 0.114 (0.084) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 44.1305 | Test Loss 696.4622
Epoch 0062 | Time 0.116 (0.085) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 43.5164 | Test Loss 444.0814
Epoch 0063 | Time 0.115 (0.085) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 43.2242 | Test Loss 826.9396
Epoch 0064 | Time 0.135 (0.086) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 43.1443 | Test Loss 657.8523
Epoch 0065 | Time 0.122 (0.086) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 43.1380 | Test Loss 736.7547
Epoch 0066 | Time 0.121 (0.086) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 43.0757 | Test Loss 732.0214
Epoch 0067 | Time 0.114 (0.087) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 42.8667 | Test Loss 671.9899
Epoch 0068 | Time 0.139 (0.087) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 42.4748 | Test Loss 778.9247
Epoch 0069 | Time 0.112 (0.087) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 41.9178 | Test Loss 598.8862
Epoch 0070 | Time 0.115 (0.088) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 41.2545 | Test Loss 737.6476
Epoch 0071 | Time 0.114 (0.088) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 40.5625 | Test Loss 608.3638
Epoch 0072 | Time 0.112 (0.088) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 39.9165 | Test Loss 602.0372
Epoch 0073 | Time 0.113 (0.088) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 39.3686 | Test Loss 749.6068
Epoch 0074 | Time 0.116 (0.089) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 38.9397 | Test Loss 656.0437
Epoch 0075 | Time 0.112 (0.089) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 38.6178 | Test Loss 391.4352
Epoch 0076 | Time 0.114 (0.089) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 38.3684 | Test Loss 548.9087
Epoch 0077 | Time 0.116 (0.089) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 38.1447 | Test Loss 488.6372
Epoch 0078 | Time 0.112 (0.090) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 37.9040 | Test Loss 507.7856
Epoch 0079 | Time 0.116 (0.090) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 37.6165 | Test Loss 392.3261
Epoch 0080 | Time 0.112 (0.090) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 37.2724 | Test Loss 420.5079
Epoch 0081 | Time 0.123 (0.090) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 36.8797 | Test Loss 462.0229
Epoch 0082 | Time 0.124 (0.091) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 36.4607 | Test Loss 419.6326
Epoch 0083 | Time 0.116 (0.091) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 36.0417 | Test Loss 515.9718
Epoch 0084 | Time 0.135 (0.091) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 35.6465 | Test Loss 532.8699
Epoch 0085 | Time 0.116 (0.092) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 35.2897 | Test Loss 458.2527
Epoch 0086 | Time 0.120 (0.092) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 34.9743 | Test Loss 496.0473
Epoch 0087 | Time 0.116 (0.092) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 34.6926 | Test Loss 538.0010
Epoch 0088 | Time 0.118 (0.092) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 34.4305 | Test Loss 591.7415
Epoch 0089 | Time 0.113 (0.093) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 34.1718 | Test Loss 534.0621
Epoch 0090 | Time 0.114 (0.093) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 33.9041 | Test Loss 339.9252
Epoch 0091 | Time 0.118 (0.093) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 33.6211 | Test Loss 437.5777
Epoch 0092 | Time 0.113 (0.093) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 33.3225 | Test Loss 515.3316
Epoch 0093 | Time 0.112 (0.094) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 33.0147 | Test Loss 467.8389
Epoch 0094 | Time 0.118 (0.094) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 32.7069 | Test Loss 473.1646
Epoch 0095 | Time 0.113 (0.094) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 32.4077 | Test Loss 538.7273
Epoch 0096 | Time 0.112 (0.094) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 32.1229 | Test Loss 423.7697
Epoch 0097 | Time 0.112 (0.094) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 31.8551 | Test Loss 370.5995
Epoch 0098 | Time 0.117 (0.095) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 31.6018 | Test Loss 404.7144
Epoch 0099 | Time 0.118 (0.095) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 31.3584 | Test Loss 451.0840
Epoch 0100 | Time 0.125 (0.095) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 31.1192 | Test Loss 426.7521
Epoch 0101 | Time 0.117 (0.095) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 30.8797 | Test Loss 424.3005
Epoch 0102 | Time 0.116 (0.096) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 30.6374 | Test Loss 468.6212
Epoch 0103 | Time 0.113 (0.096) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 30.3924 | Test Loss 312.4962
Epoch 0104 | Time 0.118 (0.096) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 30.1467 | Test Loss 364.5421
Epoch 0105 | Time 0.114 (0.096) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 29.9035 | Test Loss 382.5720
Epoch 0106 | Time 0.111 (0.096) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 29.6659 | Test Loss 378.7284
Epoch 0107 | Time 0.115 (0.096) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 29.4358 | Test Loss 333.2831
Epoch 0108 | Time 0.122 (0.097) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 29.2137 | Test Loss 388.9512
Epoch 0109 | Time 0.114 (0.097) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 28.9988 | Test Loss 292.5926
Epoch 0110 | Time 0.114 (0.097) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 28.7894 | Test Loss 438.2260
Epoch 0111 | Time 0.113 (0.097) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 28.5833 | Test Loss 353.5450
Epoch 0112 | Time 0.117 (0.097) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 28.3789 | Test Loss 363.6624
Epoch 0113 | Time 0.118 (0.098) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 28.1758 | Test Loss 324.3816
Epoch 0114 | Time 0.117 (0.098) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 27.9740 | Test Loss 373.4469
Epoch 0115 | Time 0.117 (0.098) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 27.7744 | Test Loss 397.7280
Epoch 0116 | Time 0.117 (0.098) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 27.5784 | Test Loss 299.2343
Epoch 0117 | Time 0.120 (0.098) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 27.3866 | Test Loss 398.6498
Epoch 0118 | Time 0.124 (0.099) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 27.1999 | Test Loss 325.8583
Epoch 0119 | Time 0.112 (0.099) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 27.0180 | Test Loss 425.5589
Epoch 0120 | Time 0.117 (0.099) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 26.8402 | Test Loss 380.3995
Epoch 0121 | Time 0.114 (0.099) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 26.6658 | Test Loss 346.0967
Epoch 0122 | Time 0.117 (0.099) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 26.4946 | Test Loss 345.2208
Epoch 0123 | Time 0.114 (0.099) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 26.3256 | Test Loss 293.8589
Epoch 0124 | Time 0.117 (0.100) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 26.1591 | Test Loss 260.3936
Epoch 0125 | Time 0.118 (0.100) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 25.9950 | Test Loss 302.7075
Epoch 0126 | Time 0.113 (0.100) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 25.8338 | Test Loss 215.6454
Epoch 0127 | Time 0.113 (0.100) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 25.6759 | Test Loss 347.3821
Epoch 0128 | Time 0.114 (0.100) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 25.5216 | Test Loss 328.4137
Epoch 0129 | Time 0.119 (0.100) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 25.3707 | Test Loss 345.1237
Epoch 0130 | Time 0.113 (0.101) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 25.2232 | Test Loss 280.2717
Epoch 0131 | Time 0.117 (0.101) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 25.0787 | Test Loss 324.0482
Epoch 0132 | Time 0.111 (0.101) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 24.9370 | Test Loss 330.3755
Epoch 0133 | Time 0.137 (0.101) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 24.7977 | Test Loss 354.1570
Epoch 0134 | Time 0.123 (0.101) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 24.6610 | Test Loss 311.7822
Epoch 0135 | Time 0.117 (0.102) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 24.5269 | Test Loss 281.3916
Epoch 0136 | Time 0.117 (0.102) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 24.3952 | Test Loss 288.4852
Epoch 0137 | Time 0.113 (0.102) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 24.2662 | Test Loss 270.9306
Epoch 0138 | Time 0.118 (0.102) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 24.1401 | Test Loss 243.5367
Epoch 0139 | Time 0.117 (0.102) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 24.0166 | Test Loss 263.1422
Epoch 0140 | Time 0.117 (0.102) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 23.8959 | Test Loss 241.0153
Epoch 0141 | Time 0.113 (0.102) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 23.7777 | Test Loss 220.8764
Epoch 0142 | Time 0.118 (0.103) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 23.6619 | Test Loss 328.7286
Epoch 0143 | Time 0.119 (0.103) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 23.5484 | Test Loss 173.6413
Epoch 0144 | Time 0.118 (0.103) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 23.4372 | Test Loss 257.1947
Epoch 0145 | Time 0.117 (0.103) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 23.3281 | Test Loss 205.1318
Epoch 0146 | Time 0.114 (0.103) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 23.2214 | Test Loss 255.5354
Epoch 0147 | Time 0.119 (0.103) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 23.1169 | Test Loss 266.8467
Epoch 0148 | Time 0.118 (0.103) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 23.0147 | Test Loss 252.8778
Epoch 0149 | Time 0.115 (0.103) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 22.9147 | Test Loss 250.9120
Epoch 0150 | Time 0.116 (0.104) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 22.8168 | Test Loss 266.6347
Epoch 0151 | Time 0.115 (0.104) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 22.7211 | Test Loss 178.6438
Epoch 0152 | Time 0.119 (0.104) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 22.6275 | Test Loss 279.3817
Epoch 0153 | Time 0.119 (0.104) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 22.5357 | Test Loss 270.1897
Epoch 0154 | Time 0.113 (0.104) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 22.4458 | Test Loss 253.8589
Epoch 0155 | Time 0.118 (0.104) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 22.3579 | Test Loss 213.4508
Epoch 0156 | Time 0.117 (0.104) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 22.2719 | Test Loss 226.7503
Epoch 0157 | Time 0.117 (0.105) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 22.1877 | Test Loss 253.5166
Epoch 0158 | Time 0.114 (0.105) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 22.1055 | Test Loss 209.3452
Epoch 0159 | Time 0.112 (0.105) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 22.0250 | Test Loss 229.7135
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))

        return nn.functional.tanh(out)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.046 (0.046) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 153.5132 | Test Loss 992.8981
Epoch 0001 | Time 0.113 (0.047) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 163.4797 | Test Loss 621.6262
Epoch 0002 | Time 0.120 (0.048) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 188.5488 | Test Loss 511.4855
Epoch 0003 | Time 0.115 (0.048) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 177.1928 | Test Loss 551.2778
Epoch 0004 | Time 0.116 (0.049) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 148.3844 | Test Loss 584.7900
Epoch 0005 | Time 0.116 (0.050) | NFE-F 15.4 | NFE-B 0.0 | Train Loss 126.7775 | Test Loss 717.3641
Epoch 0006 | Time 0.122 (0.050) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 122.3567 | Test Loss 777.2460
Epoch 0007 | Time 0.122 (0.051) | NFE-F 15.9 | NFE-B 0.0 | Train Loss 128.5118 | Test Loss 762.0297
Epoch 0008 | Time 0.119 (0.052) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 131.5963 | Test Loss 969.9794
Epoch 0009 | Time 0.115 (0.052) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 124.7631 | Test Loss 974.0076
Epoch 0010 | Time 0.118 (0.053) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 111.2387 | Test Loss 824.5393
Epoch 0011 | Time 0.119 (0.054) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 98.4566 | Test Loss 618.5642
Epoch 0012 | Time 0.114 (0.054) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 91.7464 | Test Loss 539.0024
Epoch 0013 | Time 0.116 (0.055) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 91.1083 | Test Loss 574.7157
Epoch 0014 | Time 0.116 (0.056) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 92.1162 | Test Loss 471.6802
Epoch 0015 | Time 0.114 (0.056) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 90.1112 | Test Loss 476.4401
Epoch 0016 | Time 0.115 (0.057) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 83.8831 | Test Loss 442.3986
Epoch 0017 | Time 0.115 (0.057) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 75.8151 | Test Loss 380.9896
Epoch 0018 | Time 0.116 (0.058) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 69.3564 | Test Loss 511.8335
Epoch 0019 | Time 0.118 (0.058) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 66.3062 | Test Loss 495.0842
Epoch 0020 | Time 0.119 (0.059) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 65.7267 | Test Loss 604.4893
Epoch 0021 | Time 0.112 (0.060) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 65.0678 | Test Loss 608.6992
Epoch 0022 | Time 0.117 (0.060) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 62.4320 | Test Loss 529.5370
Epoch 0023 | Time 0.115 (0.061) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 57.9570 | Test Loss 534.6869
Epoch 0024 | Time 0.117 (0.061) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 53.3362 | Test Loss 527.2451
Epoch 0025 | Time 0.118 (0.062) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 50.2233 | Test Loss 431.8545
Epoch 0026 | Time 0.117 (0.062) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 48.9274 | Test Loss 377.1938
Epoch 0027 | Time 0.114 (0.063) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 48.3516 | Test Loss 358.2423
Epoch 0028 | Time 0.120 (0.063) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 47.0980 | Test Loss 395.3681
Epoch 0029 | Time 0.115 (0.064) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 44.6750 | Test Loss 311.6054
Epoch 0030 | Time 0.118 (0.065) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 41.7586 | Test Loss 289.5995
Epoch 0031 | Time 0.112 (0.065) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 39.4287 | Test Loss 275.9478
Epoch 0032 | Time 0.117 (0.066) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 38.1853 | Test Loss 369.9902
Epoch 0033 | Time 0.117 (0.066) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 37.5975 | Test Loss 368.9254
Epoch 0034 | Time 0.114 (0.067) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 36.8115 | Test Loss 313.0245
Epoch 0035 | Time 0.118 (0.067) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 35.3649 | Test Loss 326.0410
Epoch 0036 | Time 0.122 (0.068) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 33.5337 | Test Loss 303.9906
Epoch 0037 | Time 0.113 (0.068) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 31.9628 | Test Loss 286.8522
Epoch 0038 | Time 0.118 (0.069) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 31.0216 | Test Loss 228.7736
Epoch 0039 | Time 0.111 (0.069) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 30.5024 | Test Loss 220.1804
Epoch 0040 | Time 0.113 (0.069) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 29.8962 | Test Loss 228.5240
Epoch 0041 | Time 0.122 (0.070) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 28.9136 | Test Loss 258.2863
Epoch 0042 | Time 0.115 (0.070) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 27.7212 | Test Loss 213.8950
Epoch 0043 | Time 0.117 (0.071) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 26.7090 | Test Loss 250.1702
Epoch 0044 | Time 0.119 (0.071) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 26.0774 | Test Loss 251.6844
Epoch 0045 | Time 0.118 (0.072) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 25.6688 | Test Loss 219.7564
Epoch 0046 | Time 0.117 (0.072) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 25.1756 | Test Loss 234.8657
Epoch 0047 | Time 0.117 (0.073) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 24.4676 | Test Loss 234.4219
Epoch 0048 | Time 0.118 (0.073) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 23.6922 | Test Loss 199.2806
Epoch 0049 | Time 0.121 (0.074) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 23.0801 | Test Loss 157.7228
Epoch 0050 | Time 0.118 (0.074) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 22.6962 | Test Loss 140.2341
Epoch 0051 | Time 0.117 (0.075) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 22.3966 | Test Loss 170.3205
Epoch 0052 | Time 0.115 (0.075) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 22.0094 | Test Loss 176.7222
Epoch 0053 | Time 0.119 (0.075) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 21.5176 | Test Loss 103.4821
Epoch 0054 | Time 0.115 (0.076) | NFE-F 25.7 | NFE-B 0.0 | Train Loss 21.0506 | Test Loss 174.4022
Epoch 0055 | Time 0.111 (0.076) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 20.7217 | Test Loss 163.8098
Epoch 0056 | Time 0.118 (0.077) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 20.5076 | Test Loss 159.0195
Epoch 0057 | Time 0.113 (0.077) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 20.2938 | Test Loss 161.3266
Epoch 0058 | Time 0.113 (0.077) | NFE-F 26.4 | NFE-B 0.0 | Train Loss 20.0123 | Test Loss 159.9284
Epoch 0059 | Time 0.118 (0.078) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 19.7070 | Test Loss 132.6876
Epoch 0060 | Time 0.114 (0.078) | NFE-F 26.7 | NFE-B 0.0 | Train Loss 19.4655 | Test Loss 137.6630
Epoch 0061 | Time 0.116 (0.078) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 19.3126 | Test Loss 116.3024
Epoch 0062 | Time 0.121 (0.079) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 19.1909 | Test Loss 136.5573
Epoch 0063 | Time 0.121 (0.079) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 19.0374 | Test Loss 140.7605
Epoch 0064 | Time 0.116 (0.080) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 18.8547 | Test Loss 105.3895
Epoch 0065 | Time 0.118 (0.080) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 18.6954 | Test Loss 158.0077
Epoch 0066 | Time 0.118 (0.080) | NFE-F 27.6 | NFE-B 0.0 | Train Loss 18.5917 | Test Loss 158.5708
Epoch 0067 | Time 0.122 (0.081) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 18.5198 | Test Loss 115.1063
Epoch 0068 | Time 0.117 (0.081) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 18.4354 | Test Loss 141.1579
Epoch 0069 | Time 0.122 (0.082) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 18.3281 | Test Loss 138.1634
Epoch 0070 | Time 0.118 (0.082) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 18.2267 | Test Loss 118.9867
Epoch 0071 | Time 0.127 (0.082) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 18.1573 | Test Loss 114.8333
Epoch 0072 | Time 0.118 (0.083) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 18.1109 | Test Loss 149.7358
Epoch 0073 | Time 0.117 (0.083) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 18.0595 | Test Loss 121.0282
Epoch 0074 | Time 0.119 (0.083) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 17.9920 | Test Loss 97.5259
Epoch 0075 | Time 0.121 (0.084) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 17.9245 | Test Loss 151.5264
Epoch 0076 | Time 0.116 (0.084) | NFE-F 29.0 | NFE-B 0.0 | Train Loss 17.8749 | Test Loss 102.6734
Epoch 0077 | Time 0.115 (0.084) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 17.8399 | Test Loss 120.5310
Epoch 0078 | Time 0.118 (0.085) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 17.8018 | Test Loss 124.5192
Epoch 0079 | Time 0.115 (0.085) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 17.7530 | Test Loss 110.6021
Epoch 0080 | Time 0.134 (0.086) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 17.7031 | Test Loss 100.3895
Epoch 0081 | Time 0.116 (0.086) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 17.6637 | Test Loss 130.0415
Epoch 0082 | Time 0.119 (0.086) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 17.6327 | Test Loss 96.0196
Epoch 0083 | Time 0.115 (0.087) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 17.5992 | Test Loss 146.5281
Epoch 0084 | Time 0.113 (0.087) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 17.5586 | Test Loss 97.3499
Epoch 0085 | Time 0.117 (0.087) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 17.5176 | Test Loss 99.1454
Epoch 0086 | Time 0.116 (0.087) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 17.4831 | Test Loss 117.9416
Epoch 0087 | Time 0.131 (0.088) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 17.4531 | Test Loss 93.6828
Epoch 0088 | Time 0.119 (0.088) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 17.4207 | Test Loss 146.4868
Epoch 0089 | Time 0.120 (0.088) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 17.3840 | Test Loss 137.6819
Epoch 0090 | Time 0.115 (0.089) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 17.3478 | Test Loss 113.4132
Epoch 0091 | Time 0.137 (0.089) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 17.3158 | Test Loss 113.6852
Epoch 0092 | Time 0.113 (0.089) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 17.2858 | Test Loss 154.1332
Epoch 0093 | Time 0.118 (0.090) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 17.2536 | Test Loss 122.4206
Epoch 0094 | Time 0.119 (0.090) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 17.2191 | Test Loss 141.1663
Epoch 0095 | Time 0.120 (0.090) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 17.1855 | Test Loss 97.6608
Epoch 0096 | Time 0.125 (0.091) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 17.1545 | Test Loss 129.8847
Epoch 0097 | Time 0.116 (0.091) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 17.1240 | Test Loss 146.6073
Epoch 0098 | Time 0.118 (0.091) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 17.0918 | Test Loss 106.9888
Epoch 0099 | Time 0.121 (0.091) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 17.0585 | Test Loss 108.6930
Epoch 0100 | Time 0.115 (0.092) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 17.0262 | Test Loss 151.0569
Epoch 0101 | Time 0.132 (0.092) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 16.9954 | Test Loss 109.3573
Epoch 0102 | Time 0.124 (0.092) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 16.9641 | Test Loss 116.2906
Epoch 0103 | Time 0.141 (0.093) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 16.9315 | Test Loss 136.5824
Epoch 0104 | Time 0.125 (0.093) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 16.8984 | Test Loss 126.2787
Epoch 0105 | Time 0.117 (0.093) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 16.8660 | Test Loss 149.5258
Epoch 0106 | Time 0.117 (0.094) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 16.8337 | Test Loss 154.3597
Epoch 0107 | Time 0.132 (0.094) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 16.8003 | Test Loss 129.2132
Epoch 0108 | Time 0.134 (0.094) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 16.7657 | Test Loss 130.6384
Epoch 0109 | Time 0.134 (0.095) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 16.7307 | Test Loss 137.3664
Epoch 0110 | Time 0.139 (0.095) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 16.6955 | Test Loss 105.0712
Epoch 0111 | Time 0.140 (0.096) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 16.6590 | Test Loss 161.3974
Epoch 0112 | Time 0.137 (0.096) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 16.6206 | Test Loss 134.5371
Epoch 0113 | Time 0.141 (0.097) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 16.5805 | Test Loss 122.1878
Epoch 0114 | Time 0.141 (0.097) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 16.5396 | Test Loss 145.5410
Epoch 0115 | Time 0.136 (0.097) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 16.4979 | Test Loss 103.5733
Epoch 0116 | Time 0.134 (0.098) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 16.4550 | Test Loss 152.0727
Epoch 0117 | Time 0.134 (0.098) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 16.4114 | Test Loss 122.8124
Epoch 0118 | Time 0.133 (0.099) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 16.3677 | Test Loss 136.9200
Epoch 0119 | Time 0.140 (0.099) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 16.3242 | Test Loss 124.9144
Epoch 0120 | Time 0.133 (0.099) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 16.2808 | Test Loss 144.5036
Epoch 0121 | Time 0.133 (0.100) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 16.2378 | Test Loss 101.4796
Epoch 0122 | Time 0.132 (0.100) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 16.1955 | Test Loss 122.8975
Epoch 0123 | Time 0.147 (0.100) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 16.1537 | Test Loss 143.3226
Epoch 0124 | Time 0.137 (0.101) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 16.1121 | Test Loss 148.4821
Epoch 0125 | Time 0.134 (0.101) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 16.0708 | Test Loss 149.7590
Epoch 0126 | Time 0.138 (0.102) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 16.0301 | Test Loss 144.1919
Epoch 0127 | Time 0.140 (0.102) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 15.9898 | Test Loss 145.9245
Epoch 0128 | Time 0.139 (0.102) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 15.9498 | Test Loss 171.6232
Epoch 0129 | Time 0.137 (0.103) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 15.9102 | Test Loss 176.1629
Epoch 0130 | Time 0.141 (0.103) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 15.8709 | Test Loss 119.3063
Epoch 0131 | Time 0.137 (0.103) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 15.8319 | Test Loss 168.4870
Epoch 0132 | Time 0.145 (0.104) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 15.7933 | Test Loss 150.3272
Epoch 0133 | Time 0.140 (0.104) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 15.7548 | Test Loss 191.2602
Epoch 0134 | Time 0.139 (0.104) | NFE-F 40.0 | NFE-B 0.0 | Train Loss 15.7167 | Test Loss 181.9642
Epoch 0135 | Time 0.135 (0.105) | NFE-F 40.2 | NFE-B 0.0 | Train Loss 15.6789 | Test Loss 127.8416
Epoch 0136 | Time 0.132 (0.105) | NFE-F 40.4 | NFE-B 0.0 | Train Loss 15.6414 | Test Loss 221.2253
Epoch 0137 | Time 0.133 (0.105) | NFE-F 40.6 | NFE-B 0.0 | Train Loss 15.6042 | Test Loss 158.7776
Epoch 0138 | Time 0.139 (0.106) | NFE-F 40.8 | NFE-B 0.0 | Train Loss 15.5674 | Test Loss 138.6088
Epoch 0139 | Time 0.133 (0.106) | NFE-F 41.0 | NFE-B 0.0 | Train Loss 15.5310 | Test Loss 211.0741
Epoch 0140 | Time 0.136 (0.106) | NFE-F 41.1 | NFE-B 0.0 | Train Loss 15.4950 | Test Loss 103.7921
Epoch 0141 | Time 0.136 (0.107) | NFE-F 41.3 | NFE-B 0.0 | Train Loss 15.4594 | Test Loss 156.3222
Epoch 0142 | Time 0.137 (0.107) | NFE-F 41.5 | NFE-B 0.0 | Train Loss 15.4241 | Test Loss 165.0037
Epoch 0143 | Time 0.135 (0.107) | NFE-F 41.7 | NFE-B 0.0 | Train Loss 15.3893 | Test Loss 156.8493
Epoch 0144 | Time 0.135 (0.107) | NFE-F 41.9 | NFE-B 0.0 | Train Loss 15.3550 | Test Loss 97.1581
Epoch 0145 | Time 0.138 (0.108) | NFE-F 42.1 | NFE-B 0.0 | Train Loss 15.3211 | Test Loss 162.5163
Epoch 0146 | Time 0.134 (0.108) | NFE-F 42.2 | NFE-B 0.0 | Train Loss 15.2877 | Test Loss 111.5454
Epoch 0147 | Time 0.136 (0.108) | NFE-F 42.4 | NFE-B 0.0 | Train Loss 15.2547 | Test Loss 163.9398
Epoch 0148 | Time 0.136 (0.109) | NFE-F 42.6 | NFE-B 0.0 | Train Loss 15.2222 | Test Loss 111.3104
Epoch 0149 | Time 0.131 (0.109) | NFE-F 42.8 | NFE-B 0.0 | Train Loss 15.1901 | Test Loss 215.7056
Epoch 0150 | Time 0.137 (0.109) | NFE-F 42.9 | NFE-B 0.0 | Train Loss 15.1584 | Test Loss 211.3716
Epoch 0151 | Time 0.136 (0.109) | NFE-F 43.1 | NFE-B 0.0 | Train Loss 15.1271 | Test Loss 183.7491
Epoch 0152 | Time 0.131 (0.110) | NFE-F 43.3 | NFE-B 0.0 | Train Loss 15.0962 | Test Loss 172.6315
Epoch 0153 | Time 0.134 (0.110) | NFE-F 43.5 | NFE-B 0.0 | Train Loss 15.0658 | Test Loss 189.9580
Epoch 0154 | Time 0.134 (0.110) | NFE-F 43.6 | NFE-B 0.0 | Train Loss 15.0356 | Test Loss 186.1243
Epoch 0155 | Time 0.139 (0.110) | NFE-F 43.8 | NFE-B 0.0 | Train Loss 15.0059 | Test Loss 194.1199
Epoch 0156 | Time 0.138 (0.111) | NFE-F 43.9 | NFE-B 0.0 | Train Loss 14.9764 | Test Loss 228.3109
Epoch 0157 | Time 0.140 (0.111) | NFE-F 44.1 | NFE-B 0.0 | Train Loss 14.9473 | Test Loss 248.0499
Epoch 0158 | Time 0.135 (0.111) | NFE-F 44.3 | NFE-B 0.0 | Train Loss 14.9185 | Test Loss 178.4440
Epoch 0159 | Time 0.135 (0.111) | NFE-F 44.4 | NFE-B 0.0 | Train Loss 14.8899 | Test Loss 162.1500
Epoch 0160 | Time 0.138 (0.112) | NFE-F 44.6 | NFE-B 0.0 | Train Loss 14.8617 | Test Loss 196.1549
Epoch 0161 | Time 0.135 (0.112) | NFE-F 44.7 | NFE-B 0.0 | Train Loss 14.8337 | Test Loss 161.5859
Epoch 0162 | Time 0.137 (0.112) | NFE-F 44.9 | NFE-B 0.0 | Train Loss 14.8059 | Test Loss 183.4596
Epoch 0163 | Time 0.137 (0.112) | NFE-F 45.0 | NFE-B 0.0 | Train Loss 14.7784 | Test Loss 163.8597
Epoch 0164 | Time 0.137 (0.113) | NFE-F 45.2 | NFE-B 0.0 | Train Loss 14.7511 | Test Loss 206.4090
Epoch 0165 | Time 0.137 (0.113) | NFE-F 45.3 | NFE-B 0.0 | Train Loss 14.7241 | Test Loss 210.3535
Epoch 0166 | Time 0.136 (0.113) | NFE-F 45.5 | NFE-B 0.0 | Train Loss 14.6972 | Test Loss 216.2215
Epoch 0167 | Time 0.136 (0.113) | NFE-F 45.6 | NFE-B 0.0 | Train Loss 14.6705 | Test Loss 215.3764
Epoch 0168 | Time 0.136 (0.114) | NFE-F 45.8 | NFE-B 0.0 | Train Loss 14.6441 | Test Loss 251.8316
Epoch 0169 | Time 0.136 (0.114) | NFE-F 45.9 | NFE-B 0.0 | Train Loss 14.6178 | Test Loss 268.7873
Epoch 0170 | Time 0.142 (0.114) | NFE-F 46.1 | NFE-B 0.0 | Train Loss 14.5917 | Test Loss 266.5630
Epoch 0171 | Time 0.136 (0.114) | NFE-F 46.2 | NFE-B 0.0 | Train Loss 14.5658 | Test Loss 215.0193
Epoch 0172 | Time 0.135 (0.114) | NFE-F 46.3 | NFE-B 0.0 | Train Loss 14.5400 | Test Loss 201.0622
Epoch 0173 | Time 0.137 (0.115) | NFE-F 46.5 | NFE-B 0.0 | Train Loss 14.5144 | Test Loss 132.9853
Epoch 0174 | Time 0.139 (0.115) | NFE-F 46.6 | NFE-B 0.0 | Train Loss 14.4890 | Test Loss 167.9901
Epoch 0175 | Time 0.135 (0.115) | NFE-F 46.7 | NFE-B 0.0 | Train Loss 14.4637 | Test Loss 132.4104
Epoch 0176 | Time 0.139 (0.115) | NFE-F 46.9 | NFE-B 0.0 | Train Loss 14.4386 | Test Loss 196.8085
Epoch 0177 | Time 0.142 (0.116) | NFE-F 47.0 | NFE-B 0.0 | Train Loss 14.4137 | Test Loss 189.1275
Epoch 0178 | Time 0.134 (0.116) | NFE-F 47.1 | NFE-B 0.0 | Train Loss 14.3888 | Test Loss 247.0436
Epoch 0179 | Time 0.131 (0.116) | NFE-F 47.3 | NFE-B 0.0 | Train Loss 14.3642 | Test Loss 240.7561
Epoch 0180 | Time 0.138 (0.116) | NFE-F 47.4 | NFE-B 0.0 | Train Loss 14.3396 | Test Loss 248.3132
Epoch 0181 | Time 0.135 (0.116) | NFE-F 47.5 | NFE-B 0.0 | Train Loss 14.3153 | Test Loss 227.5426
Epoch 0182 | Time 0.136 (0.117) | NFE-F 47.6 | NFE-B 0.0 | Train Loss 14.2910 | Test Loss 206.7675
Epoch 0183 | Time 0.138 (0.117) | NFE-F 47.8 | NFE-B 0.0 | Train Loss 14.2669 | Test Loss 194.5277
Epoch 0184 | Time 0.137 (0.117) | NFE-F 47.9 | NFE-B 0.0 | Train Loss 14.2429 | Test Loss 203.5129
Epoch 0185 | Time 0.141 (0.117) | NFE-F 48.0 | NFE-B 0.0 | Train Loss 14.2191 | Test Loss 226.6188
Epoch 0186 | Time 0.135 (0.117) | NFE-F 48.1 | NFE-B 0.0 | Train Loss 14.1954 | Test Loss 207.0302
Epoch 0187 | Time 0.134 (0.118) | NFE-F 48.2 | NFE-B 0.0 | Train Loss 14.1718 | Test Loss 171.9094
Epoch 0188 | Time 0.140 (0.118) | NFE-F 48.4 | NFE-B 0.0 | Train Loss 14.1483 | Test Loss 340.6071
Epoch 0189 | Time 0.140 (0.118) | NFE-F 48.5 | NFE-B 0.0 | Train Loss 14.1250 | Test Loss 176.3072
Epoch 0190 | Time 0.137 (0.118) | NFE-F 48.6 | NFE-B 0.0 | Train Loss 14.1018 | Test Loss 358.2674
Epoch 0191 | Time 0.136 (0.118) | NFE-F 48.7 | NFE-B 0.0 | Train Loss 14.0788 | Test Loss 260.3460
Epoch 0192 | Time 0.138 (0.119) | NFE-F 48.8 | NFE-B 0.0 | Train Loss 14.0558 | Test Loss 196.6017
Epoch 0193 | Time 0.137 (0.119) | NFE-F 48.9 | NFE-B 0.0 | Train Loss 14.0330 | Test Loss 179.7778
Epoch 0194 | Time 0.135 (0.119) | NFE-F 49.0 | NFE-B 0.0 | Train Loss 14.0103 | Test Loss 176.9046
Epoch 0195 | Time 0.138 (0.119) | NFE-F 49.1 | NFE-B 0.0 | Train Loss 13.9877 | Test Loss 257.8525
Epoch 0196 | Time 0.139 (0.119) | NFE-F 49.3 | NFE-B 0.0 | Train Loss 13.9652 | Test Loss 208.3512
Epoch 0197 | Time 0.137 (0.120) | NFE-F 49.4 | NFE-B 0.0 | Train Loss 13.9429 | Test Loss 210.0702
Epoch 0198 | Time 0.133 (0.120) | NFE-F 49.5 | NFE-B 0.0 | Train Loss 13.9206 | Test Loss 179.5851
Epoch 0199 | Time 0.142 (0.120) | NFE-F 49.6 | NFE-B 0.0 | Train Loss 13.8985 | Test Loss 220.3618
Epoch 0200 | Time 0.139 (0.120) | NFE-F 49.7 | NFE-B 0.0 | Train Loss 13.8765 | Test Loss 276.0967
Epoch 0201 | Time 0.138 (0.120) | NFE-F 49.8 | NFE-B 0.0 | Train Loss 13.8546 | Test Loss 261.3120
Epoch 0202 | Time 0.140 (0.120) | NFE-F 49.9 | NFE-B 0.0 | Train Loss 13.8328 | Test Loss 170.5147
Epoch 0203 | Time 0.137 (0.121) | NFE-F 50.0 | NFE-B 0.0 | Train Loss 13.8112 | Test Loss 227.1675
Epoch 0204 | Time 0.135 (0.121) | NFE-F 50.1 | NFE-B 0.0 | Train Loss 13.7896 | Test Loss 269.7087
Epoch 0205 | Time 0.138 (0.121) | NFE-F 50.2 | NFE-B 0.0 | Train Loss 13.7682 | Test Loss 261.9095
Epoch 0206 | Time 0.139 (0.121) | NFE-F 50.3 | NFE-B 0.0 | Train Loss 13.7468 | Test Loss 155.6620
Epoch 0207 | Time 0.137 (0.121) | NFE-F 50.4 | NFE-B 0.0 | Train Loss 13.7256 | Test Loss 235.9526
Epoch 0208 | Time 0.137 (0.121) | NFE-F 50.5 | NFE-B 0.0 | Train Loss 13.7045 | Test Loss 231.1770
Epoch 0209 | Time 0.134 (0.122) | NFE-F 50.6 | NFE-B 0.0 | Train Loss 13.6835 | Test Loss 282.2812
Epoch 0210 | Time 0.136 (0.122) | NFE-F 50.7 | NFE-B 0.0 | Train Loss 13.6626 | Test Loss 177.7052
Epoch 0211 | Time 0.134 (0.122) | NFE-F 50.8 | NFE-B 0.0 | Train Loss 13.6418 | Test Loss 217.8707
Epoch 0212 | Time 0.135 (0.122) | NFE-F 50.9 | NFE-B 0.0 | Train Loss 13.6211 | Test Loss 241.4386
Epoch 0213 | Time 0.136 (0.122) | NFE-F 50.9 | NFE-B 0.0 | Train Loss 13.6005 | Test Loss 206.8333
Epoch 0214 | Time 0.144 (0.122) | NFE-F 51.0 | NFE-B 0.0 | Train Loss 13.5800 | Test Loss 331.1115
Epoch 0215 | Time 0.133 (0.122) | NFE-F 51.1 | NFE-B 0.0 | Train Loss 13.5596 | Test Loss 216.1056
Epoch 0216 | Time 0.135 (0.123) | NFE-F 51.2 | NFE-B 0.0 | Train Loss 13.5393 | Test Loss 259.0937
Epoch 0217 | Time 0.139 (0.123) | NFE-F 51.3 | NFE-B 0.0 | Train Loss 13.5191 | Test Loss 243.6058
Epoch 0218 | Time 0.134 (0.123) | NFE-F 51.4 | NFE-B 0.0 | Train Loss 13.4990 | Test Loss 166.1644
Epoch 0219 | Time 0.136 (0.123) | NFE-F 51.5 | NFE-B 0.0 | Train Loss 13.4790 | Test Loss 205.3961
Epoch 0220 | Time 0.136 (0.123) | NFE-F 51.6 | NFE-B 0.0 | Train Loss 13.4591 | Test Loss 299.6315
Epoch 0221 | Time 0.142 (0.123) | NFE-F 51.6 | NFE-B 0.0 | Train Loss 13.4393 | Test Loss 346.9103
Epoch 0222 | Time 0.133 (0.123) | NFE-F 51.7 | NFE-B 0.0 | Train Loss 13.4196 | Test Loss 343.9564
Epoch 0223 | Time 0.139 (0.124) | NFE-F 51.8 | NFE-B 0.0 | Train Loss 13.4000 | Test Loss 325.6118
Epoch 0224 | Time 0.134 (0.124) | NFE-F 51.9 | NFE-B 0.0 | Train Loss 13.3805 | Test Loss 313.8304
Epoch 0225 | Time 0.137 (0.124) | NFE-F 52.0 | NFE-B 0.0 | Train Loss 13.3611 | Test Loss 307.5553
Epoch 0226 | Time 0.135 (0.124) | NFE-F 52.1 | NFE-B 0.0 | Train Loss 13.3417 | Test Loss 361.5002
Epoch 0227 | Time 0.138 (0.124) | NFE-F 52.1 | NFE-B 0.0 | Train Loss 13.3225 | Test Loss 307.3385
Epoch 0228 | Time 0.139 (0.124) | NFE-F 52.2 | NFE-B 0.0 | Train Loss 13.3034 | Test Loss 355.8857
Epoch 0229 | Time 0.137 (0.124) | NFE-F 52.3 | NFE-B 0.0 | Train Loss 13.2843 | Test Loss 354.6995
Epoch 0230 | Time 0.143 (0.124) | NFE-F 52.4 | NFE-B 0.0 | Train Loss 13.2653 | Test Loss 411.2642
Epoch 0231 | Time 0.142 (0.125) | NFE-F 52.4 | NFE-B 0.0 | Train Loss 13.2465 | Test Loss 345.7177
Epoch 0232 | Time 0.136 (0.125) | NFE-F 52.5 | NFE-B 0.0 | Train Loss 13.2277 | Test Loss 373.6737
Epoch 0233 | Time 0.134 (0.125) | NFE-F 52.6 | NFE-B 0.0 | Train Loss 13.2090 | Test Loss 369.8510
Epoch 0234 | Time 0.134 (0.125) | NFE-F 52.7 | NFE-B 0.0 | Train Loss 13.1903 | Test Loss 339.4072
Epoch 0235 | Time 0.142 (0.125) | NFE-F 52.7 | NFE-B 0.0 | Train Loss 13.1718 | Test Loss 222.4539
Epoch 0236 | Time 0.144 (0.125) | NFE-F 52.8 | NFE-B 0.0 | Train Loss 13.1534 | Test Loss 387.6992
Epoch 0237 | Time 0.134 (0.125) | NFE-F 52.9 | NFE-B 0.0 | Train Loss 13.1350 | Test Loss 323.5158
Epoch 0238 | Time 0.137 (0.126) | NFE-F 53.0 | NFE-B 0.0 | Train Loss 13.1167 | Test Loss 269.7488
Epoch 0239 | Time 0.135 (0.126) | NFE-F 53.0 | NFE-B 0.0 | Train Loss 13.0985 | Test Loss 453.0997
Epoch 0240 | Time 0.141 (0.126) | NFE-F 53.1 | NFE-B 0.0 | Train Loss 13.0804 | Test Loss 243.5536
Epoch 0241 | Time 0.137 (0.126) | NFE-F 53.2 | NFE-B 0.0 | Train Loss 13.0624 | Test Loss 497.7697
Epoch 0242 | Time 0.134 (0.126) | NFE-F 53.2 | NFE-B 0.0 | Train Loss 13.0445 | Test Loss 301.8986
Epoch 0243 | Time 0.142 (0.126) | NFE-F 53.3 | NFE-B 0.0 | Train Loss 13.0266 | Test Loss 298.6540
Epoch 0244 | Time 0.135 (0.126) | NFE-F 53.4 | NFE-B 0.0 | Train Loss 13.0088 | Test Loss 368.3079
Epoch 0245 | Time 0.136 (0.126) | NFE-F 53.4 | NFE-B 0.0 | Train Loss 12.9911 | Test Loss 216.7827
Epoch 0246 | Time 0.142 (0.126) | NFE-F 53.5 | NFE-B 0.0 | Train Loss 12.9735 | Test Loss 370.4248
Epoch 0247 | Time 0.141 (0.127) | NFE-F 53.6 | NFE-B 0.0 | Train Loss 12.9559 | Test Loss 483.4407
Epoch 0248 | Time 0.134 (0.127) | NFE-F 53.6 | NFE-B 0.0 | Train Loss 12.9384 | Test Loss 417.7570
Epoch 0249 | Time 0.141 (0.127) | NFE-F 53.7 | NFE-B 0.0 | Train Loss 12.9210 | Test Loss 402.3730
Epoch 0250 | Time 0.145 (0.127) | NFE-F 53.8 | NFE-B 0.0 | Train Loss 12.9037 | Test Loss 277.0930
Epoch 0251 | Time 0.135 (0.127) | NFE-F 53.8 | NFE-B 0.0 | Train Loss 12.8865 | Test Loss 158.5873
Epoch 0252 | Time 0.139 (0.127) | NFE-F 53.9 | NFE-B 0.0 | Train Loss 12.8693 | Test Loss 257.3907
Epoch 0253 | Time 0.138 (0.127) | NFE-F 53.9 | NFE-B 0.0 | Train Loss 12.8522 | Test Loss 323.2426
Epoch 0254 | Time 0.132 (0.127) | NFE-F 54.0 | NFE-B 0.0 | Train Loss 12.8351 | Test Loss 431.8538
Epoch 0255 | Time 0.138 (0.127) | NFE-F 54.1 | NFE-B 0.0 | Train Loss 12.8182 | Test Loss 301.1288
Epoch 0256 | Time 0.137 (0.128) | NFE-F 54.1 | NFE-B 0.0 | Train Loss 12.8013 | Test Loss 368.2407
Epoch 0257 | Time 0.145 (0.128) | NFE-F 54.2 | NFE-B 0.0 | Train Loss 12.7845 | Test Loss 375.6692
Epoch 0258 | Time 0.135 (0.128) | NFE-F 54.2 | NFE-B 0.0 | Train Loss 12.7677 | Test Loss 199.7237
Epoch 0259 | Time 0.137 (0.128) | NFE-F 54.3 | NFE-B 0.0 | Train Loss 12.7511 | Test Loss 261.5137
Epoch 0260 | Time 0.142 (0.128) | NFE-F 54.4 | NFE-B 0.0 | Train Loss 12.7345 | Test Loss 333.8824
Epoch 0261 | Time 0.137 (0.128) | NFE-F 54.4 | NFE-B 0.0 | Train Loss 12.7179 | Test Loss 298.4735
Epoch 0262 | Time 0.138 (0.128) | NFE-F 54.5 | NFE-B 0.0 | Train Loss 12.7015 | Test Loss 320.6967
Epoch 0263 | Time 0.131 (0.128) | NFE-F 54.5 | NFE-B 0.0 | Train Loss 12.6851 | Test Loss 389.7100
Epoch 0264 | Time 0.134 (0.128) | NFE-F 54.6 | NFE-B 0.0 | Train Loss 12.6687 | Test Loss 438.4398
Epoch 0265 | Time 0.136 (0.128) | NFE-F 54.6 | NFE-B 0.0 | Train Loss 12.6525 | Test Loss 430.5076
Epoch 0266 | Time 0.131 (0.128) | NFE-F 54.7 | NFE-B 0.0 | Train Loss 12.6363 | Test Loss 302.5103
Epoch 0267 | Time 0.135 (0.128) | NFE-F 54.7 | NFE-B 0.0 | Train Loss 12.6201 | Test Loss 443.3076
Epoch 0268 | Time 0.134 (0.129) | NFE-F 54.8 | NFE-B 0.0 | Train Loss 12.6041 | Test Loss 339.5136
Epoch 0269 | Time 0.138 (0.129) | NFE-F 54.8 | NFE-B 0.0 | Train Loss 12.5881 | Test Loss 306.2935
Epoch 0270 | Time 0.139 (0.129) | NFE-F 54.9 | NFE-B 0.0 | Train Loss 12.5721 | Test Loss 423.0717
Epoch 0271 | Time 0.133 (0.129) | NFE-F 54.9 | NFE-B 0.0 | Train Loss 12.5562 | Test Loss 365.7951
Epoch 0272 | Time 0.137 (0.129) | NFE-F 55.0 | NFE-B 0.0 | Train Loss 12.5404 | Test Loss 219.1777
Epoch 0273 | Time 0.131 (0.129) | NFE-F 55.0 | NFE-B 0.0 | Train Loss 12.5247 | Test Loss 296.0374
Epoch 0274 | Time 0.133 (0.129) | NFE-F 55.1 | NFE-B 0.0 | Train Loss 12.5090 | Test Loss 293.6646
Epoch 0275 | Time 0.134 (0.129) | NFE-F 55.1 | NFE-B 0.0 | Train Loss 12.4933 | Test Loss 232.3232
Epoch 0276 | Time 0.137 (0.129) | NFE-F 55.2 | NFE-B 0.0 | Train Loss 12.4777 | Test Loss 307.9807
Epoch 0277 | Time 0.141 (0.129) | NFE-F 55.2 | NFE-B 0.0 | Train Loss 12.4622 | Test Loss 381.4330
Epoch 0278 | Time 0.135 (0.129) | NFE-F 55.3 | NFE-B 0.0 | Train Loss 12.4468 | Test Loss 524.2635
Epoch 0279 | Time 0.141 (0.129) | NFE-F 55.3 | NFE-B 0.0 | Train Loss 12.4314 | Test Loss 336.0979
Epoch 0280 | Time 0.141 (0.129) | NFE-F 55.4 | NFE-B 0.0 | Train Loss 12.4160 | Test Loss 222.5673
Epoch 0281 | Time 0.136 (0.130) | NFE-F 55.4 | NFE-B 0.0 | Train Loss 12.4008 | Test Loss 287.9249
Epoch 0282 | Time 0.136 (0.130) | NFE-F 55.5 | NFE-B 0.0 | Train Loss 12.3855 | Test Loss 562.5944
Epoch 0283 | Time 0.134 (0.130) | NFE-F 55.5 | NFE-B 0.0 | Train Loss 12.3704 | Test Loss 609.7853
Epoch 0284 | Time 0.135 (0.130) | NFE-F 55.6 | NFE-B 0.0 | Train Loss 12.3553 | Test Loss 514.3558
Epoch 0285 | Time 0.135 (0.130) | NFE-F 55.6 | NFE-B 0.0 | Train Loss 12.3402 | Test Loss 166.8006
Epoch 0286 | Time 0.139 (0.130) | NFE-F 55.7 | NFE-B 0.0 | Train Loss 12.3252 | Test Loss 305.6501
Epoch 0287 | Time 0.142 (0.130) | NFE-F 55.7 | NFE-B 0.0 | Train Loss 12.3103 | Test Loss 401.1326
Epoch 0288 | Time 0.139 (0.130) | NFE-F 55.7 | NFE-B 0.0 | Train Loss 12.2954 | Test Loss 333.5224
Epoch 0289 | Time 0.130 (0.130) | NFE-F 55.8 | NFE-B 0.0 | Train Loss 12.2805 | Test Loss 587.5925
Epoch 0290 | Time 0.133 (0.130) | NFE-F 55.8 | NFE-B 0.0 | Train Loss 12.2658 | Test Loss 308.6494
Epoch 0291 | Time 0.135 (0.130) | NFE-F 55.9 | NFE-B 0.0 | Train Loss 12.2510 | Test Loss 355.8640
Epoch 0292 | Time 0.139 (0.130) | NFE-F 55.9 | NFE-B 0.0 | Train Loss 12.2364 | Test Loss 442.0426
Epoch 0293 | Time 0.136 (0.130) | NFE-F 55.9 | NFE-B 0.0 | Train Loss 12.2217 | Test Loss 380.5471
Epoch 0294 | Time 0.137 (0.130) | NFE-F 56.0 | NFE-B 0.0 | Train Loss 12.2072 | Test Loss 532.0946
Epoch 0295 | Time 0.135 (0.130) | NFE-F 56.0 | NFE-B 0.0 | Train Loss 12.1926 | Test Loss 473.3021
Epoch 0296 | Time 0.139 (0.130) | NFE-F 56.1 | NFE-B 0.0 | Train Loss 12.1782 | Test Loss 476.7143
Epoch 0297 | Time 0.138 (0.131) | NFE-F 56.1 | NFE-B 0.0 | Train Loss 12.1637 | Test Loss 568.6664
Epoch 0298 | Time 0.139 (0.131) | NFE-F 56.1 | NFE-B 0.0 | Train Loss 12.1494 | Test Loss 269.1674
Epoch 0299 | Time 0.137 (0.131) | NFE-F 56.2 | NFE-B 0.0 | Train Loss 12.1350 | Test Loss 262.0704
Epoch 0300 | Time 0.137 (0.131) | NFE-F 56.2 | NFE-B 0.0 | Train Loss 12.1208 | Test Loss 232.5864
Epoch 0301 | Time 0.144 (0.131) | NFE-F 56.3 | NFE-B 0.0 | Train Loss 12.1066 | Test Loss 540.3907
Epoch 0302 | Time 0.138 (0.131) | NFE-F 56.3 | NFE-B 0.0 | Train Loss 12.0924 | Test Loss 584.7656
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))

        return nn.functional.tanh(out)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=128):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                test_loss = compute_loss(criterion, model, test_loader)
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f} | Test Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss, test_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.055 (0.055) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 3974.6509 | Test Loss 11044.3662
Epoch 0001 | Time 0.126 (0.055) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 3028.2930 | Test Loss 8272.4297
Epoch 0002 | Time 0.133 (0.056) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 2238.7600 | Test Loss 7991.3208
Epoch 0003 | Time 0.135 (0.057) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 1589.0742 | Test Loss 6031.1338
Epoch 0004 | Time 0.135 (0.058) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 1075.2616 | Test Loss 4665.9438
Epoch 0005 | Time 0.135 (0.058) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 691.5662 | Test Loss 4057.7480
Epoch 0006 | Time 0.137 (0.059) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 428.9112 | Test Loss 3733.9521
Epoch 0007 | Time 0.132 (0.060) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 274.4468 | Test Loss 2107.2324
Epoch 0008 | Time 0.133 (0.061) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 211.5186 | Test Loss 2022.9885
Epoch 0009 | Time 0.133 (0.061) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 220.2088 | Test Loss 1875.4774
Epoch 0010 | Time 0.131 (0.062) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 278.5558 | Test Loss 1410.4431
Epoch 0011 | Time 0.130 (0.063) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 364.3687 | Test Loss 1459.6903
Epoch 0012 | Time 0.129 (0.063) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 457.3006 | Test Loss 1270.7759
Epoch 0013 | Time 0.136 (0.064) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 540.6998 | Test Loss 1305.5338
Epoch 0014 | Time 0.132 (0.065) | NFE-F 25.3 | NFE-B 0.0 | Train Loss 602.8111 | Test Loss 1170.1125
Epoch 0015 | Time 0.135 (0.065) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 637.1377 | Test Loss 1139.1827
Epoch 0016 | Time 0.148 (0.066) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 642.0372 | Test Loss 1148.0331
Epoch 0017 | Time 0.129 (0.067) | NFE-F 26.3 | NFE-B 0.0 | Train Loss 619.8018 | Test Loss 1247.8845
Epoch 0018 | Time 0.129 (0.068) | NFE-F 26.6 | NFE-B 0.0 | Train Loss 575.5018 | Test Loss 1208.5110
Epoch 0019 | Time 0.131 (0.068) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 515.8196 | Test Loss 1244.0343
Epoch 0020 | Time 0.129 (0.069) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 448.0061 | Test Loss 1114.4559
Epoch 0021 | Time 0.130 (0.069) | NFE-F 27.6 | NFE-B 0.0 | Train Loss 379.0246 | Test Loss 1218.8745
Epoch 0022 | Time 0.129 (0.070) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 314.9002 | Test Loss 1265.1912
Epoch 0023 | Time 0.133 (0.071) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 260.2712 | Test Loss 1268.3770
Epoch 0024 | Time 0.130 (0.071) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 218.1324 | Test Loss 1379.6842
Epoch 0025 | Time 0.127 (0.072) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 189.7622 | Test Loss 1272.7328
Epoch 0026 | Time 0.131 (0.072) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 174.8190 | Test Loss 1244.8638
Epoch 0027 | Time 0.140 (0.073) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 171.5893 | Test Loss 1744.1052
Epoch 0028 | Time 0.129 (0.074) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 177.3547 | Test Loss 1679.4100
Epoch 0029 | Time 0.128 (0.074) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 188.8335 | Test Loss 1986.6448
Epoch 0030 | Time 0.132 (0.075) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 202.6410 | Test Loss 2096.1467
Epoch 0031 | Time 0.136 (0.075) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 215.7092 | Test Loss 2007.1106
Epoch 0032 | Time 0.132 (0.076) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 225.6162 | Test Loss 2502.6956
Epoch 0033 | Time 0.132 (0.076) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 230.7892 | Test Loss 2221.6682
Epoch 0034 | Time 0.131 (0.077) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 230.5706 | Test Loss 1955.1842
Epoch 0035 | Time 0.130 (0.078) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 225.1561 | Test Loss 1970.1086
Epoch 0036 | Time 0.133 (0.078) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 215.4319 | Test Loss 1688.7185
Epoch 0037 | Time 0.131 (0.079) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 202.7489 | Test Loss 1995.9161
Epoch 0038 | Time 0.134 (0.079) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 188.6703 | Test Loss 2080.9526
Epoch 0039 | Time 0.136 (0.080) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 174.7313 | Test Loss 1535.5089
Epoch 0040 | Time 0.134 (0.080) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 162.2371 | Test Loss 1704.1508
Epoch 0041 | Time 0.132 (0.081) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 152.1216 | Test Loss 1245.5575
Epoch 0042 | Time 0.131 (0.081) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 144.8741 | Test Loss 1342.9886
Epoch 0043 | Time 0.133 (0.082) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 140.5367 | Test Loss 1467.3734
Epoch 0044 | Time 0.129 (0.082) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 138.7627 | Test Loss 1321.7012
Epoch 0045 | Time 0.132 (0.083) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 138.9214 | Test Loss 1115.6255
Epoch 0046 | Time 0.134 (0.083) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 140.2283 | Test Loss 1292.9287
Epoch 0047 | Time 0.130 (0.084) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 141.8794 | Test Loss 1254.4844
Epoch 0048 | Time 0.128 (0.084) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 143.1700 | Test Loss 1231.3107
Epoch 0049 | Time 0.132 (0.085) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 143.5819 | Test Loss 985.5928
Epoch 0050 | Time 0.132 (0.085) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 142.8298 | Test Loss 1124.8243
Epoch 0051 | Time 0.132 (0.086) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 140.8669 | Test Loss 931.1823
Epoch 0052 | Time 0.132 (0.086) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 137.8537 | Test Loss 942.6462
Epoch 0053 | Time 0.134 (0.087) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 134.0991 | Test Loss 1101.7134
Epoch 0054 | Time 0.135 (0.087) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 129.9892 | Test Loss 1069.4647
Epoch 0055 | Time 0.128 (0.087) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 125.9139 | Test Loss 1057.3516
Epoch 0056 | Time 0.133 (0.088) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 122.2043 | Test Loss 1018.3228
Epoch 0057 | Time 0.131 (0.088) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 119.0895 | Test Loss 1202.7083
Epoch 0058 | Time 0.130 (0.089) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 116.6749 | Test Loss 1065.0895
Epoch 0059 | Time 0.131 (0.089) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 114.9446 | Test Loss 1023.7441
Epoch 0060 | Time 0.132 (0.090) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 113.7823 | Test Loss 1247.3309
Epoch 0061 | Time 0.139 (0.090) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 113.0059 | Test Loss 1113.3492
Epoch 0062 | Time 0.129 (0.090) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 112.4074 | Test Loss 920.9166
Epoch 0063 | Time 0.137 (0.091) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 111.7912 | Test Loss 1027.1110
Epoch 0064 | Time 0.133 (0.091) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 111.0046 | Test Loss 896.7073
Epoch 0065 | Time 0.131 (0.092) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 109.9562 | Test Loss 825.5303
Epoch 0066 | Time 0.130 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 108.6210 | Test Loss 780.4521
Epoch 0067 | Time 0.130 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 107.0340 | Test Loss 970.6930
Epoch 0068 | Time 0.130 (0.093) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 105.2738 | Test Loss 997.2402
Epoch 0069 | Time 0.133 (0.093) | NFE-F 40.0 | NFE-B 0.0 | Train Loss 103.4414 | Test Loss 1122.5527
Epoch 0070 | Time 0.130 (0.094) | NFE-F 40.2 | NFE-B 0.0 | Train Loss 101.6388 | Test Loss 916.3217
Epoch 0071 | Time 0.132 (0.094) | NFE-F 40.4 | NFE-B 0.0 | Train Loss 99.9500 | Test Loss 1003.4949
Epoch 0072 | Time 0.138 (0.094) | NFE-F 40.6 | NFE-B 0.0 | Train Loss 98.4295 | Test Loss 801.3734
Epoch 0073 | Time 0.140 (0.095) | NFE-F 40.8 | NFE-B 0.0 | Train Loss 97.0965 | Test Loss 823.1190
Epoch 0074 | Time 0.132 (0.095) | NFE-F 41.0 | NFE-B 0.0 | Train Loss 95.9371 | Test Loss 788.1843
Epoch 0075 | Time 0.133 (0.096) | NFE-F 41.2 | NFE-B 0.0 | Train Loss 94.9125 | Test Loss 839.6528
Epoch 0076 | Time 0.136 (0.096) | NFE-F 41.4 | NFE-B 0.0 | Train Loss 93.9694 | Test Loss 796.9817
Epoch 0077 | Time 0.133 (0.096) | NFE-F 41.6 | NFE-B 0.0 | Train Loss 93.0525 | Test Loss 720.4133
Epoch 0078 | Time 0.130 (0.097) | NFE-F 41.7 | NFE-B 0.0 | Train Loss 92.1150 | Test Loss 674.1712
Epoch 0079 | Time 0.129 (0.097) | NFE-F 41.9 | NFE-B 0.0 | Train Loss 91.1252 | Test Loss 824.9465
Epoch 0080 | Time 0.130 (0.097) | NFE-F 42.1 | NFE-B 0.0 | Train Loss 90.0705 | Test Loss 727.4182
Epoch 0081 | Time 0.129 (0.098) | NFE-F 42.3 | NFE-B 0.0 | Train Loss 88.9562 | Test Loss 579.8206
Epoch 0082 | Time 0.127 (0.098) | NFE-F 42.5 | NFE-B 0.0 | Train Loss 87.8012 | Test Loss 701.4849
Epoch 0083 | Time 0.131 (0.098) | NFE-F 42.6 | NFE-B 0.0 | Train Loss 86.6323 | Test Loss 713.5720
Epoch 0084 | Time 0.134 (0.099) | NFE-F 42.8 | NFE-B 0.0 | Train Loss 85.4773 | Test Loss 794.2795
Epoch 0085 | Time 0.130 (0.099) | NFE-F 43.0 | NFE-B 0.0 | Train Loss 84.3596 | Test Loss 625.2728
Epoch 0086 | Time 0.130 (0.099) | NFE-F 43.1 | NFE-B 0.0 | Train Loss 83.2941 | Test Loss 731.8358
Epoch 0087 | Time 0.129 (0.100) | NFE-F 43.3 | NFE-B 0.0 | Train Loss 82.2856 | Test Loss 712.4454
Epoch 0088 | Time 0.127 (0.100) | NFE-F 43.5 | NFE-B 0.0 | Train Loss 81.3295 | Test Loss 633.9770
Epoch 0089 | Time 0.129 (0.100) | NFE-F 43.6 | NFE-B 0.0 | Train Loss 80.4144 | Test Loss 702.3403
Epoch 0090 | Time 0.127 (0.100) | NFE-F 43.8 | NFE-B 0.0 | Train Loss 79.5250 | Test Loss 676.4481
Epoch 0091 | Time 0.124 (0.101) | NFE-F 44.0 | NFE-B 0.0 | Train Loss 78.6464 | Test Loss 493.0184
Epoch 0092 | Time 0.137 (0.101) | NFE-F 44.1 | NFE-B 0.0 | Train Loss 77.7663 | Test Loss 659.2747
Epoch 0093 | Time 0.134 (0.101) | NFE-F 44.3 | NFE-B 0.0 | Train Loss 76.8775 | Test Loss 603.7946
Epoch 0094 | Time 0.128 (0.102) | NFE-F 44.4 | NFE-B 0.0 | Train Loss 75.9784 | Test Loss 588.6409
Epoch 0095 | Time 0.130 (0.102) | NFE-F 44.6 | NFE-B 0.0 | Train Loss 75.0720 | Test Loss 520.6706
Epoch 0096 | Time 0.126 (0.102) | NFE-F 44.8 | NFE-B 0.0 | Train Loss 74.1648 | Test Loss 603.9645
Epoch 0097 | Time 0.129 (0.102) | NFE-F 44.9 | NFE-B 0.0 | Train Loss 73.2645 | Test Loss 608.2115
Epoch 0098 | Time 0.132 (0.103) | NFE-F 45.1 | NFE-B 0.0 | Train Loss 72.3786 | Test Loss 583.5048
Epoch 0099 | Time 0.130 (0.103) | NFE-F 45.2 | NFE-B 0.0 | Train Loss 71.5123 | Test Loss 560.8365
Epoch 0100 | Time 0.131 (0.103) | NFE-F 45.4 | NFE-B 0.0 | Train Loss 70.6679 | Test Loss 482.0425
Epoch 0101 | Time 0.131 (0.104) | NFE-F 45.5 | NFE-B 0.0 | Train Loss 69.8450 | Test Loss 497.6080
Epoch 0102 | Time 0.134 (0.104) | NFE-F 45.7 | NFE-B 0.0 | Train Loss 69.0409 | Test Loss 481.9249
Epoch 0103 | Time 0.130 (0.104) | NFE-F 45.8 | NFE-B 0.0 | Train Loss 68.2512 | Test Loss 529.9536
Epoch 0104 | Time 0.128 (0.104) | NFE-F 45.9 | NFE-B 0.0 | Train Loss 67.4717 | Test Loss 487.9244
Epoch 0105 | Time 0.133 (0.105) | NFE-F 46.1 | NFE-B 0.0 | Train Loss 66.6985 | Test Loss 459.3504
Epoch 0106 | Time 0.131 (0.105) | NFE-F 46.2 | NFE-B 0.0 | Train Loss 65.9293 | Test Loss 421.0076
Epoch 0107 | Time 0.133 (0.105) | NFE-F 46.4 | NFE-B 0.0 | Train Loss 65.1634 | Test Loss 485.9644
Epoch 0108 | Time 0.129 (0.105) | NFE-F 46.5 | NFE-B 0.0 | Train Loss 64.4016 | Test Loss 437.2971
Epoch 0109 | Time 0.128 (0.106) | NFE-F 46.6 | NFE-B 0.0 | Train Loss 63.6456 | Test Loss 491.9601
Epoch 0110 | Time 0.131 (0.106) | NFE-F 46.8 | NFE-B 0.0 | Train Loss 62.8978 | Test Loss 447.1112
Epoch 0111 | Time 0.129 (0.106) | NFE-F 46.9 | NFE-B 0.0 | Train Loss 62.1600 | Test Loss 464.8709
Epoch 0112 | Time 0.129 (0.106) | NFE-F 47.0 | NFE-B 0.0 | Train Loss 61.4339 | Test Loss 441.2606
Epoch 0113 | Time 0.126 (0.107) | NFE-F 47.2 | NFE-B 0.0 | Train Loss 60.7199 | Test Loss 383.9380
Epoch 0114 | Time 0.128 (0.107) | NFE-F 47.3 | NFE-B 0.0 | Train Loss 60.0179 | Test Loss 349.8250
Epoch 0115 | Time 0.135 (0.107) | NFE-F 47.4 | NFE-B 0.0 | Train Loss 59.3268 | Test Loss 410.1025
Epoch 0116 | Time 0.134 (0.107) | NFE-F 47.5 | NFE-B 0.0 | Train Loss 58.6455 | Test Loss 320.7768
Epoch 0117 | Time 0.131 (0.108) | NFE-F 47.7 | NFE-B 0.0 | Train Loss 57.9725 | Test Loss 371.6641
Epoch 0118 | Time 0.133 (0.108) | NFE-F 47.8 | NFE-B 0.0 | Train Loss 57.3069 | Test Loss 385.0712
Epoch 0119 | Time 0.131 (0.108) | NFE-F 47.9 | NFE-B 0.0 | Train Loss 56.6480 | Test Loss 408.4812
Epoch 0120 | Time 0.127 (0.108) | NFE-F 48.0 | NFE-B 0.0 | Train Loss 55.9958 | Test Loss 347.0033
Epoch 0121 | Time 0.134 (0.109) | NFE-F 48.1 | NFE-B 0.0 | Train Loss 55.3506 | Test Loss 338.5143
Epoch 0122 | Time 0.134 (0.109) | NFE-F 48.3 | NFE-B 0.0 | Train Loss 54.7129 | Test Loss 375.0708
Epoch 0123 | Time 0.128 (0.109) | NFE-F 48.4 | NFE-B 0.0 | Train Loss 54.0835 | Test Loss 350.5860
Epoch 0124 | Time 0.134 (0.109) | NFE-F 48.5 | NFE-B 0.0 | Train Loss 53.4627 | Test Loss 317.9450
Epoch 0125 | Time 0.129 (0.109) | NFE-F 48.6 | NFE-B 0.0 | Train Loss 52.8509 | Test Loss 253.0780
Epoch 0126 | Time 0.126 (0.110) | NFE-F 48.7 | NFE-B 0.0 | Train Loss 52.2482 | Test Loss 331.3339
Epoch 0127 | Time 0.127 (0.110) | NFE-F 48.8 | NFE-B 0.0 | Train Loss 51.6542 | Test Loss 288.0540
Epoch 0128 | Time 0.137 (0.110) | NFE-F 48.9 | NFE-B 0.0 | Train Loss 51.0686 | Test Loss 304.3519
Epoch 0129 | Time 0.133 (0.110) | NFE-F 49.1 | NFE-B 0.0 | Train Loss 50.4909 | Test Loss 323.8646
Epoch 0130 | Time 0.131 (0.110) | NFE-F 49.2 | NFE-B 0.0 | Train Loss 49.9208 | Test Loss 248.4314
Epoch 0131 | Time 0.135 (0.111) | NFE-F 49.3 | NFE-B 0.0 | Train Loss 49.3579 | Test Loss 235.1638
Epoch 0132 | Time 0.133 (0.111) | NFE-F 49.4 | NFE-B 0.0 | Train Loss 48.8023 | Test Loss 303.6152
Epoch 0133 | Time 0.135 (0.111) | NFE-F 49.5 | NFE-B 0.0 | Train Loss 48.2538 | Test Loss 256.0359
Epoch 0134 | Time 0.130 (0.111) | NFE-F 49.6 | NFE-B 0.0 | Train Loss 47.7127 | Test Loss 306.8456
Epoch 0135 | Time 0.128 (0.112) | NFE-F 49.7 | NFE-B 0.0 | Train Loss 47.1791 | Test Loss 250.1359
Epoch 0136 | Time 0.129 (0.112) | NFE-F 49.8 | NFE-B 0.0 | Train Loss 46.6531 | Test Loss 237.6217
Epoch 0137 | Time 0.133 (0.112) | NFE-F 49.9 | NFE-B 0.0 | Train Loss 46.1347 | Test Loss 254.8885
Epoch 0138 | Time 0.133 (0.112) | NFE-F 50.0 | NFE-B 0.0 | Train Loss 45.6240 | Test Loss 249.4025
Epoch 0139 | Time 0.128 (0.112) | NFE-F 50.1 | NFE-B 0.0 | Train Loss 45.1209 | Test Loss 241.8829
Epoch 0140 | Time 0.129 (0.112) | NFE-F 50.2 | NFE-B 0.0 | Train Loss 44.6252 | Test Loss 241.7929
Epoch 0141 | Time 0.128 (0.113) | NFE-F 50.3 | NFE-B 0.0 | Train Loss 44.1366 | Test Loss 226.7741
Epoch 0142 | Time 0.136 (0.113) | NFE-F 50.4 | NFE-B 0.0 | Train Loss 43.6551 | Test Loss 243.2479
Epoch 0143 | Time 0.132 (0.113) | NFE-F 50.5 | NFE-B 0.0 | Train Loss 43.1806 | Test Loss 227.5562
Epoch 0144 | Time 0.132 (0.113) | NFE-F 50.6 | NFE-B 0.0 | Train Loss 42.7129 | Test Loss 235.6718
Epoch 0145 | Time 0.135 (0.113) | NFE-F 50.7 | NFE-B 0.0 | Train Loss 42.2520 | Test Loss 240.9865
Epoch 0146 | Time 0.135 (0.114) | NFE-F 50.8 | NFE-B 0.0 | Train Loss 41.7980 | Test Loss 218.4070
Epoch 0147 | Time 0.136 (0.114) | NFE-F 50.9 | NFE-B 0.0 | Train Loss 41.3508 | Test Loss 196.5119
Epoch 0148 | Time 0.127 (0.114) | NFE-F 51.0 | NFE-B 0.0 | Train Loss 40.9103 | Test Loss 210.5553
Epoch 0149 | Time 0.129 (0.114) | NFE-F 51.1 | NFE-B 0.0 | Train Loss 40.4767 | Test Loss 207.0935
Epoch 0150 | Time 0.132 (0.114) | NFE-F 51.1 | NFE-B 0.0 | Train Loss 40.0498 | Test Loss 206.7244
Epoch 0151 | Time 0.130 (0.114) | NFE-F 51.2 | NFE-B 0.0 | Train Loss 39.6296 | Test Loss 165.1682
Epoch 0152 | Time 0.132 (0.115) | NFE-F 51.3 | NFE-B 0.0 | Train Loss 39.2159 | Test Loss 153.3850
Epoch 0153 | Time 0.135 (0.115) | NFE-F 51.4 | NFE-B 0.0 | Train Loss 38.8087 | Test Loss 187.6769
Epoch 0154 | Time 0.133 (0.115) | NFE-F 51.5 | NFE-B 0.0 | Train Loss 38.4078 | Test Loss 146.8549
Epoch 0155 | Time 0.130 (0.115) | NFE-F 51.6 | NFE-B 0.0 | Train Loss 38.0132 | Test Loss 167.0796
Epoch 0156 | Time 0.128 (0.115) | NFE-F 51.7 | NFE-B 0.0 | Train Loss 37.6249 | Test Loss 180.4824
Epoch 0157 | Time 0.130 (0.115) | NFE-F 51.7 | NFE-B 0.0 | Train Loss 37.2427 | Test Loss 180.3896
Epoch 0158 | Time 0.131 (0.116) | NFE-F 51.8 | NFE-B 0.0 | Train Loss 36.8666 | Test Loss 160.8995
Epoch 0159 | Time 0.129 (0.116) | NFE-F 51.9 | NFE-B 0.0 | Train Loss 36.4966 | Test Loss 144.4588
Epoch 0160 | Time 0.138 (0.116) | NFE-F 52.0 | NFE-B 0.0 | Train Loss 36.1326 | Test Loss 158.6570
Epoch 0161 | Time 0.130 (0.116) | NFE-F 52.1 | NFE-B 0.0 | Train Loss 35.7746 | Test Loss 137.8806
Epoch 0162 | Time 0.130 (0.116) | NFE-F 52.1 | NFE-B 0.0 | Train Loss 35.4225 | Test Loss 136.9081
Epoch 0163 | Time 0.133 (0.116) | NFE-F 52.2 | NFE-B 0.0 | Train Loss 35.0762 | Test Loss 148.3993
Epoch 0164 | Time 0.132 (0.117) | NFE-F 52.3 | NFE-B 0.0 | Train Loss 34.7357 | Test Loss 154.1214
Epoch 0165 | Time 0.132 (0.117) | NFE-F 52.4 | NFE-B 0.0 | Train Loss 34.4008 | Test Loss 158.2670
Epoch 0166 | Time 0.127 (0.117) | NFE-F 52.5 | NFE-B 0.0 | Train Loss 34.0716 | Test Loss 137.6872
Epoch 0167 | Time 0.133 (0.117) | NFE-F 52.5 | NFE-B 0.0 | Train Loss 33.7479 | Test Loss 143.2397
Epoch 0168 | Time 0.134 (0.117) | NFE-F 52.6 | NFE-B 0.0 | Train Loss 33.4296 | Test Loss 119.3722
Epoch 0169 | Time 0.133 (0.117) | NFE-F 52.7 | NFE-B 0.0 | Train Loss 33.1167 | Test Loss 142.0383
Epoch 0170 | Time 0.132 (0.117) | NFE-F 52.8 | NFE-B 0.0 | Train Loss 32.8092 | Test Loss 121.4249
Epoch 0171 | Time 0.137 (0.118) | NFE-F 52.8 | NFE-B 0.0 | Train Loss 32.5070 | Test Loss 131.8604
Epoch 0172 | Time 0.137 (0.118) | NFE-F 52.9 | NFE-B 0.0 | Train Loss 32.2100 | Test Loss 146.3206
Epoch 0173 | Time 0.132 (0.118) | NFE-F 53.0 | NFE-B 0.0 | Train Loss 31.9182 | Test Loss 141.2691
Epoch 0174 | Time 0.131 (0.118) | NFE-F 53.0 | NFE-B 0.0 | Train Loss 31.6315 | Test Loss 135.5003
Epoch 0175 | Time 0.134 (0.118) | NFE-F 53.1 | NFE-B 0.0 | Train Loss 31.3498 | Test Loss 131.4547
Epoch 0176 | Time 0.129 (0.118) | NFE-F 53.2 | NFE-B 0.0 | Train Loss 31.0730 | Test Loss 121.1425
Epoch 0177 | Time 0.133 (0.119) | NFE-F 53.2 | NFE-B 0.0 | Train Loss 30.8012 | Test Loss 116.1828
Epoch 0178 | Time 0.136 (0.119) | NFE-F 53.3 | NFE-B 0.0 | Train Loss 30.5341 | Test Loss 132.9424
Epoch 0179 | Time 0.130 (0.119) | NFE-F 53.4 | NFE-B 0.0 | Train Loss 30.2719 | Test Loss 121.9065
Epoch 0180 | Time 0.126 (0.119) | NFE-F 53.4 | NFE-B 0.0 | Train Loss 30.0143 | Test Loss 125.2523
Epoch 0181 | Time 0.128 (0.119) | NFE-F 53.5 | NFE-B 0.0 | Train Loss 29.7613 | Test Loss 113.1008
Epoch 0182 | Time 0.127 (0.119) | NFE-F 53.6 | NFE-B 0.0 | Train Loss 29.5129 | Test Loss 109.3797
Epoch 0183 | Time 0.136 (0.119) | NFE-F 53.6 | NFE-B 0.0 | Train Loss 29.2691 | Test Loss 108.1406
Epoch 0184 | Time 0.130 (0.119) | NFE-F 53.7 | NFE-B 0.0 | Train Loss 29.0297 | Test Loss 103.8872
Epoch 0185 | Time 0.131 (0.119) | NFE-F 53.8 | NFE-B 0.0 | Train Loss 28.7946 | Test Loss 119.3485
Epoch 0186 | Time 0.132 (0.120) | NFE-F 53.8 | NFE-B 0.0 | Train Loss 28.5639 | Test Loss 114.5167
Epoch 0187 | Time 0.131 (0.120) | NFE-F 53.9 | NFE-B 0.0 | Train Loss 28.3375 | Test Loss 90.9049
Epoch 0188 | Time 0.128 (0.120) | NFE-F 54.0 | NFE-B 0.0 | Train Loss 28.1152 | Test Loss 126.7118
Epoch 0189 | Time 0.128 (0.120) | NFE-F 54.0 | NFE-B 0.0 | Train Loss 27.8971 | Test Loss 124.0725
Epoch 0190 | Time 0.128 (0.120) | NFE-F 54.1 | NFE-B 0.0 | Train Loss 27.6830 | Test Loss 103.3789
Epoch 0191 | Time 0.214 (0.121) | NFE-F 54.1 | NFE-B 0.0 | Train Loss 27.4730 | Test Loss 89.6644
Epoch 0192 | Time 0.140 (0.121) | NFE-F 54.2 | NFE-B 0.0 | Train Loss 27.2669 | Test Loss 100.1144
Epoch 0193 | Time 0.134 (0.121) | NFE-F 54.3 | NFE-B 0.0 | Train Loss 27.0647 | Test Loss 91.6409
Epoch 0194 | Time 0.125 (0.121) | NFE-F 54.3 | NFE-B 0.0 | Train Loss 26.8663 | Test Loss 116.8396
Epoch 0195 | Time 0.132 (0.121) | NFE-F 54.4 | NFE-B 0.0 | Train Loss 26.6718 | Test Loss 114.4017
Epoch 0196 | Time 0.131 (0.121) | NFE-F 54.4 | NFE-B 0.0 | Train Loss 26.4809 | Test Loss 83.9071
Epoch 0197 | Time 0.137 (0.122) | NFE-F 54.5 | NFE-B 0.0 | Train Loss 26.2937 | Test Loss 88.7746
Epoch 0198 | Time 0.140 (0.122) | NFE-F 54.5 | NFE-B 0.0 | Train Loss 26.1101 | Test Loss 78.9787
Epoch 0199 | Time 0.140 (0.122) | NFE-F 54.6 | NFE-B 0.0 | Train Loss 25.9301 | Test Loss 101.9828
Epoch 0200 | Time 0.135 (0.122) | NFE-F 54.6 | NFE-B 0.0 | Train Loss 25.7535 | Test Loss 99.3083
Epoch 0201 | Time 0.128 (0.122) | NFE-F 54.7 | NFE-B 0.0 | Train Loss 25.5804 | Test Loss 95.1580
Epoch 0202 | Time 0.128 (0.122) | NFE-F 54.7 | NFE-B 0.0 | Train Loss 25.4106 | Test Loss 95.3848
Epoch 0203 | Time 0.126 (0.122) | NFE-F 54.8 | NFE-B 0.0 | Train Loss 25.2442 | Test Loss 105.7844
Epoch 0204 | Time 0.125 (0.122) | NFE-F 54.9 | NFE-B 0.0 | Train Loss 25.0811 | Test Loss 92.8037
Epoch 0205 | Time 0.134 (0.122) | NFE-F 54.9 | NFE-B 0.0 | Train Loss 24.9211 | Test Loss 89.6597
Epoch 0206 | Time 0.124 (0.122) | NFE-F 55.0 | NFE-B 0.0 | Train Loss 24.7644 | Test Loss 78.6052
Epoch 0207 | Time 0.126 (0.122) | NFE-F 55.0 | NFE-B 0.0 | Train Loss 24.6107 | Test Loss 107.1892
Epoch 0208 | Time 0.127 (0.122) | NFE-F 55.1 | NFE-B 0.0 | Train Loss 24.4601 | Test Loss 83.9586
Epoch 0209 | Time 0.127 (0.123) | NFE-F 55.1 | NFE-B 0.0 | Train Loss 24.3125 | Test Loss 93.9348
Epoch 0210 | Time 0.132 (0.123) | NFE-F 55.2 | NFE-B 0.0 | Train Loss 24.1679 | Test Loss 96.4671
Epoch 0211 | Time 0.128 (0.123) | NFE-F 55.2 | NFE-B 0.0 | Train Loss 24.0262 | Test Loss 108.9443
Epoch 0212 | Time 0.131 (0.123) | NFE-F 55.2 | NFE-B 0.0 | Train Loss 23.8873 | Test Loss 74.0679
Epoch 0213 | Time 0.136 (0.123) | NFE-F 55.3 | NFE-B 0.0 | Train Loss 23.7512 | Test Loss 65.6348
Epoch 0214 | Time 0.150 (0.123) | NFE-F 55.3 | NFE-B 0.0 | Train Loss 23.6179 | Test Loss 70.5160
Epoch 0215 | Time 0.138 (0.123) | NFE-F 55.4 | NFE-B 0.0 | Train Loss 23.4873 | Test Loss 88.5693
Epoch 0216 | Time 0.137 (0.123) | NFE-F 55.4 | NFE-B 0.0 | Train Loss 23.3594 | Test Loss 75.0492
Epoch 0217 | Time 0.139 (0.124) | NFE-F 55.5 | NFE-B 0.0 | Train Loss 23.2340 | Test Loss 63.7908
Epoch 0218 | Time 0.167 (0.124) | NFE-F 55.5 | NFE-B 0.0 | Train Loss 23.1113 | Test Loss 77.6862
Epoch 0219 | Time 0.154 (0.124) | NFE-F 55.6 | NFE-B 0.0 | Train Loss 22.9911 | Test Loss 77.0869
Epoch 0220 | Time 0.140 (0.125) | NFE-F 55.6 | NFE-B 0.0 | Train Loss 22.8733 | Test Loss 75.2090
Epoch 0221 | Time 0.146 (0.125) | NFE-F 55.7 | NFE-B 0.0 | Train Loss 22.7580 | Test Loss 83.5992
Epoch 0222 | Time 0.138 (0.125) | NFE-F 55.7 | NFE-B 0.0 | Train Loss 22.6450 | Test Loss 92.6252
Epoch 0223 | Time 0.133 (0.125) | NFE-F 55.7 | NFE-B 0.0 | Train Loss 22.5345 | Test Loss 84.7132
Epoch 0224 | Time 0.139 (0.125) | NFE-F 55.8 | NFE-B 0.0 | Train Loss 22.4262 | Test Loss 92.6026
Epoch 0225 | Time 0.182 (0.126) | NFE-F 55.8 | NFE-B 0.0 | Train Loss 22.3201 | Test Loss 82.9476
Epoch 0226 | Time 0.141 (0.126) | NFE-F 55.9 | NFE-B 0.0 | Train Loss 22.2163 | Test Loss 88.6625
Epoch 0227 | Time 0.148 (0.126) | NFE-F 55.9 | NFE-B 0.0 | Train Loss 22.1147 | Test Loss 102.5823
Epoch 0228 | Time 0.139 (0.126) | NFE-F 56.0 | NFE-B 0.0 | Train Loss 22.0152 | Test Loss 79.7101
Epoch 0229 | Time 0.131 (0.126) | NFE-F 56.0 | NFE-B 0.0 | Train Loss 21.9178 | Test Loss 74.0192
Epoch 0230 | Time 0.135 (0.126) | NFE-F 56.0 | NFE-B 0.0 | Train Loss 21.8224 | Test Loss 91.5137
Epoch 0231 | Time 0.132 (0.126) | NFE-F 56.1 | NFE-B 0.0 | Train Loss 21.7291 | Test Loss 56.1171
Epoch 0232 | Time 0.133 (0.126) | NFE-F 56.1 | NFE-B 0.0 | Train Loss 21.6377 | Test Loss 94.3109
Epoch 0233 | Time 0.137 (0.127) | NFE-F 56.2 | NFE-B 0.0 | Train Loss 21.5483 | Test Loss 89.3850
Epoch 0234 | Time 0.135 (0.127) | NFE-F 56.2 | NFE-B 0.0 | Train Loss 21.4607 | Test Loss 85.9229
Epoch 0235 | Time 0.136 (0.127) | NFE-F 56.2 | NFE-B 0.0 | Train Loss 21.3751 | Test Loss 79.9440
Epoch 0236 | Time 0.138 (0.127) | NFE-F 56.3 | NFE-B 0.0 | Train Loss 21.2912 | Test Loss 83.1977
Epoch 0237 | Time 0.133 (0.127) | NFE-F 56.3 | NFE-B 0.0 | Train Loss 21.2092 | Test Loss 126.8684
Epoch 0238 | Time 0.131 (0.127) | NFE-F 56.3 | NFE-B 0.0 | Train Loss 21.1289 | Test Loss 86.3403
Epoch 0239 | Time 0.132 (0.127) | NFE-F 56.4 | NFE-B 0.0 | Train Loss 21.0503 | Test Loss 71.1533
Epoch 0240 | Time 0.137 (0.127) | NFE-F 56.4 | NFE-B 0.0 | Train Loss 20.9734 | Test Loss 65.0094
Epoch 0241 | Time 0.135 (0.127) | NFE-F 56.5 | NFE-B 0.0 | Train Loss 20.8982 | Test Loss 93.6039
Epoch 0242 | Time 0.140 (0.127) | NFE-F 56.5 | NFE-B 0.0 | Train Loss 20.8246 | Test Loss 87.4064
Epoch 0243 | Time 0.187 (0.128) | NFE-F 56.5 | NFE-B 0.0 | Train Loss 20.7526 | Test Loss 53.5572
Epoch 0244 | Time 0.150 (0.128) | NFE-F 56.6 | NFE-B 0.0 | Train Loss 20.6821 | Test Loss 96.4371
Epoch 0245 | Time 0.172 (0.129) | NFE-F 56.6 | NFE-B 0.0 | Train Loss 20.6131 | Test Loss 64.9972
Epoch 0246 | Time 0.135 (0.129) | NFE-F 56.6 | NFE-B 0.0 | Train Loss 20.5457 | Test Loss 101.0349
Epoch 0247 | Time 0.127 (0.129) | NFE-F 56.7 | NFE-B 0.0 | Train Loss 20.4797 | Test Loss 70.8732
Epoch 0248 | Time 0.138 (0.129) | NFE-F 56.7 | NFE-B 0.0 | Train Loss 20.4151 | Test Loss 82.0218
Epoch 0249 | Time 0.137 (0.129) | NFE-F 56.7 | NFE-B 0.0 | Train Loss 20.3519 | Test Loss 96.0817
Epoch 0250 | Time 0.132 (0.129) | NFE-F 56.8 | NFE-B 0.0 | Train Loss 20.2901 | Test Loss 95.3386
Epoch 0251 | Time 0.132 (0.129) | NFE-F 56.8 | NFE-B 0.0 | Train Loss 20.2297 | Test Loss 96.3964
Epoch 0252 | Time 0.130 (0.129) | NFE-F 56.8 | NFE-B 0.0 | Train Loss 20.1706 | Test Loss 81.8195
Epoch 0253 | Time 0.134 (0.129) | NFE-F 56.9 | NFE-B 0.0 | Train Loss 20.1127 | Test Loss 81.4949
Epoch 0254 | Time 0.129 (0.129) | NFE-F 56.9 | NFE-B 0.0 | Train Loss 20.0561 | Test Loss 103.7154
Epoch 0255 | Time 0.125 (0.129) | NFE-F 56.9 | NFE-B 0.0 | Train Loss 20.0007 | Test Loss 72.4380
Epoch 0256 | Time 0.138 (0.129) | NFE-F 56.9 | NFE-B 0.0 | Train Loss 19.9466 | Test Loss 93.0687
Epoch 0257 | Time 0.135 (0.129) | NFE-F 57.0 | NFE-B 0.0 | Train Loss 19.8936 | Test Loss 77.4657
Epoch 0258 | Time 0.178 (0.129) | NFE-F 57.0 | NFE-B 0.0 | Train Loss 19.8418 | Test Loss 101.5806
Epoch 0259 | Time 0.160 (0.130) | NFE-F 57.0 | NFE-B 0.0 | Train Loss 19.7911 | Test Loss 99.8391
Epoch 0260 | Time 0.166 (0.130) | NFE-F 57.1 | NFE-B 0.0 | Train Loss 19.7415 | Test Loss 84.3770
Epoch 0261 | Time 0.132 (0.130) | NFE-F 57.1 | NFE-B 0.0 | Train Loss 19.6930 | Test Loss 84.3965
Epoch 0262 | Time 0.159 (0.130) | NFE-F 57.1 | NFE-B 0.0 | Train Loss 19.6455 | Test Loss 106.7373
Epoch 0263 | Time 0.175 (0.131) | NFE-F 57.2 | NFE-B 0.0 | Train Loss 19.5991 | Test Loss 91.0421
Epoch 0264 | Time 0.150 (0.131) | NFE-F 57.2 | NFE-B 0.0 | Train Loss 19.5537 | Test Loss 88.7161
Epoch 0265 | Time 0.156 (0.131) | NFE-F 57.2 | NFE-B 0.0 | Train Loss 19.5093 | Test Loss 96.9970
Epoch 0266 | Time 0.131 (0.131) | NFE-F 57.2 | NFE-B 0.0 | Train Loss 19.4658 | Test Loss 86.1042
Epoch 0267 | Time 0.130 (0.131) | NFE-F 57.3 | NFE-B 0.0 | Train Loss 19.4233 | Test Loss 100.0826
Epoch 0268 | Time 0.160 (0.132) | NFE-F 57.3 | NFE-B 0.0 | Train Loss 19.3817 | Test Loss 109.3877
Epoch 0269 | Time 0.159 (0.132) | NFE-F 57.3 | NFE-B 0.0 | Train Loss 19.3410 | Test Loss 113.2355
Epoch 0270 | Time 0.130 (0.132) | NFE-F 57.3 | NFE-B 0.0 | Train Loss 19.3012 | Test Loss 89.1595
Epoch 0271 | Time 0.133 (0.132) | NFE-F 57.4 | NFE-B 0.0 | Train Loss 19.2623 | Test Loss 87.5013
Epoch 0272 | Time 0.128 (0.132) | NFE-F 57.4 | NFE-B 0.0 | Train Loss 19.2242 | Test Loss 115.5161
Epoch 0273 | Time 0.130 (0.132) | NFE-F 57.4 | NFE-B 0.0 | Train Loss 19.1869 | Test Loss 120.9782
Epoch 0274 | Time 0.128 (0.132) | NFE-F 57.5 | NFE-B 0.0 | Train Loss 19.1504 | Test Loss 128.0844
Epoch 0275 | Time 0.135 (0.132) | NFE-F 57.5 | NFE-B 0.0 | Train Loss 19.1147 | Test Loss 110.0368
Epoch 0276 | Time 0.203 (0.133) | NFE-F 57.5 | NFE-B 0.0 | Train Loss 19.0798 | Test Loss 120.1070
Epoch 0277 | Time 0.152 (0.133) | NFE-F 57.5 | NFE-B 0.0 | Train Loss 19.0456 | Test Loss 103.9562
Epoch 0278 | Time 0.136 (0.133) | NFE-F 57.6 | NFE-B 0.0 | Train Loss 19.0122 | Test Loss 112.8895
Epoch 0279 | Time 0.124 (0.133) | NFE-F 57.6 | NFE-B 0.0 | Train Loss 18.9795 | Test Loss 111.9013
Epoch 0280 | Time 0.128 (0.133) | NFE-F 57.6 | NFE-B 0.0 | Train Loss 18.9474 | Test Loss 125.4176
Epoch 0281 | Time 0.136 (0.133) | NFE-F 57.6 | NFE-B 0.0 | Train Loss 18.9161 | Test Loss 120.2576
Epoch 0282 | Time 0.135 (0.133) | NFE-F 57.6 | NFE-B 0.0 | Train Loss 18.8854 | Test Loss 109.6821
Epoch 0283 | Time 0.133 (0.133) | NFE-F 57.7 | NFE-B 0.0 | Train Loss 18.8554 | Test Loss 88.5386
Epoch 0284 | Time 0.131 (0.133) | NFE-F 57.7 | NFE-B 0.0 | Train Loss 18.8260 | Test Loss 97.8403
Epoch 0285 | Time 0.130 (0.133) | NFE-F 57.7 | NFE-B 0.0 | Train Loss 18.7972 | Test Loss 73.4228
Epoch 0286 | Time 0.131 (0.133) | NFE-F 57.7 | NFE-B 0.0 | Train Loss 18.7690 | Test Loss 112.9297
Epoch 0287 | Time 0.131 (0.133) | NFE-F 57.8 | NFE-B 0.0 | Train Loss 18.7414 | Test Loss 107.9699
Epoch 0288 | Time 0.131 (0.133) | NFE-F 57.8 | NFE-B 0.0 | Train Loss 18.7144 | Test Loss 115.5080
Epoch 0289 | Time 0.130 (0.133) | NFE-F 57.8 | NFE-B 0.0 | Train Loss 18.6879 | Test Loss 94.3715
Epoch 0290 | Time 0.131 (0.133) | NFE-F 57.8 | NFE-B 0.0 | Train Loss 18.6620 | Test Loss 107.6193
Epoch 0291 | Time 0.131 (0.133) | NFE-F 57.9 | NFE-B 0.0 | Train Loss 18.6366 | Test Loss 116.0086
Epoch 0292 | Time 0.130 (0.133) | NFE-F 57.9 | NFE-B 0.0 | Train Loss 18.6118 | Test Loss 88.2563
Epoch 0293 | Time 0.133 (0.133) | NFE-F 57.9 | NFE-B 0.0 | Train Loss 18.5874 | Test Loss 92.9993
Epoch 0294 | Time 0.134 (0.133) | NFE-F 57.9 | NFE-B 0.0 | Train Loss 18.5636 | Test Loss 70.3064
Epoch 0295 | Time 0.126 (0.132) | NFE-F 57.9 | NFE-B 0.0 | Train Loss 18.5402 | Test Loss 100.7730
Epoch 0296 | Time 0.129 (0.132) | NFE-F 58.0 | NFE-B 0.0 | Train Loss 18.5173 | Test Loss 98.8577
Epoch 0297 | Time 0.130 (0.132) | NFE-F 58.0 | NFE-B 0.0 | Train Loss 18.4949 | Test Loss 89.1438
Epoch 0298 | Time 0.132 (0.132) | NFE-F 58.0 | NFE-B 0.0 | Train Loss 18.4729 | Test Loss 121.5344
Epoch 0299 | Time 0.135 (0.132) | NFE-F 58.0 | NFE-B 0.0 | Train Loss 18.4513 | Test Loss 124.2953
Epoch 0300 | Time 0.131 (0.132) | NFE-F 58.0 | NFE-B 0.0 | Train Loss 18.4302 | Test Loss 88.7511
Epoch 0301 | Time 0.133 (0.132) | NFE-F 58.1 | NFE-B 0.0 | Train Loss 18.4095 | Test Loss 93.1258
Epoch 0302 | Time 0.130 (0.132) | NFE-F 58.1 | NFE-B 0.0 | Train Loss 18.3892 | Test Loss 105.0310
Epoch 0303 | Time 0.130 (0.132) | NFE-F 58.1 | NFE-B 0.0 | Train Loss 18.3692 | Test Loss 124.4151
Epoch 0304 | Time 0.132 (0.132) | NFE-F 58.1 | NFE-B 0.0 | Train Loss 18.3497 | Test Loss 112.4251
Epoch 0305 | Time 0.129 (0.132) | NFE-F 58.1 | NFE-B 0.0 | Train Loss 18.3306 | Test Loss 87.3764
Epoch 0306 | Time 0.133 (0.132) | NFE-F 58.2 | NFE-B 0.0 | Train Loss 18.3118 | Test Loss 145.3605
Epoch 0307 | Time 0.127 (0.132) | NFE-F 58.2 | NFE-B 0.0 | Train Loss 18.2933 | Test Loss 136.1299
Epoch 0308 | Time 0.131 (0.132) | NFE-F 58.2 | NFE-B 0.0 | Train Loss 18.2752 | Test Loss 142.7055
Epoch 0309 | Time 0.131 (0.132) | NFE-F 58.2 | NFE-B 0.0 | Train Loss 18.2575 | Test Loss 116.8659
Epoch 0310 | Time 0.125 (0.132) | NFE-F 58.2 | NFE-B 0.0 | Train Loss 18.2400 | Test Loss 135.5306
Epoch 0311 | Time 0.128 (0.132) | NFE-F 58.2 | NFE-B 0.0 | Train Loss 18.2229 | Test Loss 129.2876
Epoch 0312 | Time 0.129 (0.132) | NFE-F 58.3 | NFE-B 0.0 | Train Loss 18.2061 | Test Loss 105.3519
Epoch 0313 | Time 0.130 (0.132) | NFE-F 58.3 | NFE-B 0.0 | Train Loss 18.1896 | Test Loss 129.4296
Epoch 0314 | Time 0.139 (0.132) | NFE-F 58.3 | NFE-B 0.0 | Train Loss 18.1734 | Test Loss 137.0910
Epoch 0315 | Time 0.131 (0.132) | NFE-F 58.3 | NFE-B 0.0 | Train Loss 18.1575 | Test Loss 123.3288
Epoch 0316 | Time 0.128 (0.132) | NFE-F 58.3 | NFE-B 0.0 | Train Loss 18.1419 | Test Loss 108.1540
Epoch 0317 | Time 0.133 (0.132) | NFE-F 58.3 | NFE-B 0.0 | Train Loss 18.1265 | Test Loss 96.8059
Epoch 0318 | Time 0.132 (0.132) | NFE-F 58.4 | NFE-B 0.0 | Train Loss 18.1114 | Test Loss 117.6131
Epoch 0319 | Time 0.130 (0.132) | NFE-F 58.4 | NFE-B 0.0 | Train Loss 18.0966 | Test Loss 135.0197
Epoch 0320 | Time 0.128 (0.132) | NFE-F 58.4 | NFE-B 0.0 | Train Loss 18.0820 | Test Loss 107.3360
Epoch 0321 | Time 0.127 (0.132) | NFE-F 58.4 | NFE-B 0.0 | Train Loss 18.0677 | Test Loss 109.2430
Epoch 0322 | Time 0.133 (0.132) | NFE-F 58.4 | NFE-B 0.0 | Train Loss 18.0536 | Test Loss 124.0886
Epoch 0323 | Time 0.127 (0.132) | NFE-F 58.4 | NFE-B 0.0 | Train Loss 18.0397 | Test Loss 113.2696
Epoch 0324 | Time 0.132 (0.132) | NFE-F 58.5 | NFE-B 0.0 | Train Loss 18.0261 | Test Loss 138.2788
Epoch 0325 | Time 0.129 (0.132) | NFE-F 58.5 | NFE-B 0.0 | Train Loss 18.0126 | Test Loss 116.4304
Epoch 0326 | Time 0.138 (0.132) | NFE-F 58.5 | NFE-B 0.0 | Train Loss 17.9994 | Test Loss 127.2636
Epoch 0327 | Time 0.178 (0.132) | NFE-F 58.5 | NFE-B 0.0 | Train Loss 17.9864 | Test Loss 145.7116
Epoch 0328 | Time 0.133 (0.132) | NFE-F 58.5 | NFE-B 0.0 | Train Loss 17.9736 | Test Loss 145.8316
Epoch 0329 | Time 0.137 (0.132) | NFE-F 58.5 | NFE-B 0.0 | Train Loss 17.9610 | Test Loss 134.1461
Epoch 0330 | Time 0.132 (0.132) | NFE-F 58.5 | NFE-B 0.0 | Train Loss 17.9486 | Test Loss 121.7657
Epoch 0331 | Time 0.134 (0.133) | NFE-F 58.6 | NFE-B 0.0 | Train Loss 17.9363 | Test Loss 143.1539
Epoch 0332 | Time 0.131 (0.132) | NFE-F 58.6 | NFE-B 0.0 | Train Loss 17.9243 | Test Loss 100.8351
Epoch 0333 | Time 0.198 (0.133) | NFE-F 58.6 | NFE-B 0.0 | Train Loss 17.9124 | Test Loss 143.2451
Epoch 0334 | Time 0.133 (0.133) | NFE-F 58.6 | NFE-B 0.0 | Train Loss 17.9007 | Test Loss 148.9559
Epoch 0335 | Time 0.141 (0.133) | NFE-F 58.6 | NFE-B 0.0 | Train Loss 17.8891 | Test Loss 119.2066
Epoch 0336 | Time 0.196 (0.134) | NFE-F 58.6 | NFE-B 0.0 | Train Loss 17.8778 | Test Loss 137.5518
Epoch 0337 | Time 0.148 (0.134) | NFE-F 58.6 | NFE-B 0.0 | Train Loss 17.8665 | Test Loss 126.2864
Epoch 0338 | Time 0.145 (0.134) | NFE-F 58.7 | NFE-B 0.0 | Train Loss 17.8555 | Test Loss 118.8856
Epoch 0339 | Time 0.129 (0.134) | NFE-F 58.7 | NFE-B 0.0 | Train Loss 17.8445 | Test Loss 115.7482
Epoch 0340 | Time 0.126 (0.134) | NFE-F 58.7 | NFE-B 0.0 | Train Loss 17.8338 | Test Loss 162.5198
Epoch 0341 | Time 0.133 (0.134) | NFE-F 58.7 | NFE-B 0.0 | Train Loss 17.8231 | Test Loss 157.3777
Epoch 0342 | Time 0.141 (0.134) | NFE-F 58.7 | NFE-B 0.0 | Train Loss 17.8126 | Test Loss 121.3559
Epoch 0343 | Time 0.135 (0.134) | NFE-F 58.7 | NFE-B 0.0 | Train Loss 17.8022 | Test Loss 125.6699
Epoch 0344 | Time 0.135 (0.134) | NFE-F 58.7 | NFE-B 0.0 | Train Loss 17.7920 | Test Loss 140.4693
Epoch 0345 | Time 0.130 (0.134) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7819 | Test Loss 143.3229
Epoch 0346 | Time 0.126 (0.134) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7719 | Test Loss 122.0831
Epoch 0347 | Time 0.154 (0.134) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7620 | Test Loss 140.4600
Epoch 0348 | Time 0.186 (0.135) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7522 | Test Loss 155.5207
Epoch 0349 | Time 0.136 (0.135) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7426 | Test Loss 138.6029
Epoch 0350 | Time 0.135 (0.135) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7330 | Test Loss 146.7015
Epoch 0351 | Time 0.146 (0.135) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7236 | Test Loss 131.2293
Epoch 0352 | Time 0.159 (0.135) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7142 | Test Loss 118.0031
Epoch 0353 | Time 0.147 (0.135) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 17.7050 | Test Loss 145.3508
Epoch 0354 | Time 0.130 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6958 | Test Loss 141.4035
Epoch 0355 | Time 0.130 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6868 | Test Loss 189.6273
Epoch 0356 | Time 0.133 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6778 | Test Loss 128.6858
Epoch 0357 | Time 0.143 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6689 | Test Loss 115.4118
Epoch 0358 | Time 0.133 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6601 | Test Loss 146.5391
Epoch 0359 | Time 0.132 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6514 | Test Loss 116.8985
Epoch 0360 | Time 0.134 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6428 | Test Loss 149.2176
Epoch 0361 | Time 0.129 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6342 | Test Loss 107.7339
Epoch 0362 | Time 0.139 (0.135) | NFE-F 58.9 | NFE-B 0.0 | Train Loss 17.6257 | Test Loss 170.3021
Epoch 0363 | Time 0.160 (0.135) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.6173 | Test Loss 156.4577
Epoch 0364 | Time 0.145 (0.135) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.6090 | Test Loss 134.2447
Epoch 0365 | Time 0.146 (0.135) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.6007 | Test Loss 142.7549
Epoch 0366 | Time 0.140 (0.136) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.5925 | Test Loss 122.9056
Epoch 0367 | Time 0.138 (0.136) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.5844 | Test Loss 152.0501
Epoch 0368 | Time 0.133 (0.136) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.5763 | Test Loss 130.8948
Epoch 0369 | Time 0.133 (0.135) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.5683 | Test Loss 136.8807
Epoch 0370 | Time 0.145 (0.136) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.5604 | Test Loss 134.1396
Epoch 0371 | Time 0.148 (0.136) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.5525 | Test Loss 125.0438
Epoch 0372 | Time 0.135 (0.136) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 17.5446 | Test Loss 154.3512
Epoch 0373 | Time 0.140 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.5368 | Test Loss 132.3746
Epoch 0374 | Time 0.134 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.5291 | Test Loss 174.8441
Epoch 0375 | Time 0.133 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.5214 | Test Loss 125.2003
Epoch 0376 | Time 0.131 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.5138 | Test Loss 116.3349
Epoch 0377 | Time 0.131 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.5062 | Test Loss 141.5166
Epoch 0378 | Time 0.146 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.4987 | Test Loss 101.2383
Epoch 0379 | Time 0.148 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.4912 | Test Loss 152.8629
Epoch 0380 | Time 0.155 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.4837 | Test Loss 131.1795
Epoch 0381 | Time 0.136 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.4763 | Test Loss 136.5749
Epoch 0382 | Time 0.138 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.4690 | Test Loss 152.3631
Epoch 0383 | Time 0.136 (0.136) | NFE-F 59.1 | NFE-B 0.0 | Train Loss 17.4616 | Test Loss 175.1129
Epoch 0384 | Time 0.134 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.4544 | Test Loss 154.7548
Epoch 0385 | Time 0.137 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.4471 | Test Loss 125.1813
Epoch 0386 | Time 0.133 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.4399 | Test Loss 142.3932
Epoch 0387 | Time 0.133 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.4327 | Test Loss 127.4581
Epoch 0388 | Time 0.131 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.4256 | Test Loss 124.4632
Epoch 0389 | Time 0.140 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.4185 | Test Loss 172.9387
Epoch 0390 | Time 0.135 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.4114 | Test Loss 122.6822
Epoch 0391 | Time 0.137 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.4043 | Test Loss 131.9982
Epoch 0392 | Time 0.139 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.3973 | Test Loss 121.4084
Epoch 0393 | Time 0.139 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.3903 | Test Loss 116.0377
Epoch 0394 | Time 0.131 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.3834 | Test Loss 155.2687
Epoch 0395 | Time 0.135 (0.136) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 17.3764 | Test Loss 183.4649
Epoch 0396 | Time 0.137 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3695 | Test Loss 130.6143
Epoch 0397 | Time 0.135 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3627 | Test Loss 137.3187
Epoch 0398 | Time 0.137 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3558 | Test Loss 146.5420
Epoch 0399 | Time 0.135 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3490 | Test Loss 118.8140
Epoch 0400 | Time 0.140 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3422 | Test Loss 134.3138
Epoch 0401 | Time 0.140 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3354 | Test Loss 199.3526
Epoch 0402 | Time 0.134 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3286 | Test Loss 173.6196
Epoch 0403 | Time 0.133 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3219 | Test Loss 182.8298
Epoch 0404 | Time 0.137 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3152 | Test Loss 162.2857
Epoch 0405 | Time 0.133 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3085 | Test Loss 141.8661
Epoch 0406 | Time 0.136 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.3018 | Test Loss 179.8309
Epoch 0407 | Time 0.133 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.2951 | Test Loss 166.1969
Epoch 0408 | Time 0.141 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.2885 | Test Loss 143.0565
Epoch 0409 | Time 0.133 (0.136) | NFE-F 59.3 | NFE-B 0.0 | Train Loss 17.2819 | Test Loss 133.3403
Epoch 0410 | Time 0.141 (0.136) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2753 | Test Loss 115.1403
Epoch 0411 | Time 0.138 (0.136) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2687 | Test Loss 131.1650
Epoch 0412 | Time 0.138 (0.136) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2621 | Test Loss 136.1323
Epoch 0413 | Time 0.181 (0.137) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2556 | Test Loss 174.5430
Epoch 0414 | Time 0.143 (0.137) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2490 | Test Loss 144.4038
Epoch 0415 | Time 0.132 (0.137) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2425 | Test Loss 172.4156
Epoch 0416 | Time 0.133 (0.136) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2360 | Test Loss 148.6226
Epoch 0417 | Time 0.172 (0.137) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2295 | Test Loss 161.6999
Epoch 0418 | Time 0.170 (0.137) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2230 | Test Loss 130.4010
Epoch 0419 | Time 0.137 (0.137) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2165 | Test Loss 116.7362
Epoch 0420 | Time 0.184 (0.138) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2101 | Test Loss 205.8319
Epoch 0421 | Time 0.138 (0.138) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.2037 | Test Loss 177.1778
Epoch 0422 | Time 0.139 (0.138) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.1972 | Test Loss 154.2093
Epoch 0423 | Time 0.136 (0.138) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.1908 | Test Loss 144.5148
Epoch 0424 | Time 0.134 (0.138) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.1844 | Test Loss 170.8518
Epoch 0425 | Time 0.141 (0.138) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.1780 | Test Loss 170.5079
Epoch 0426 | Time 0.135 (0.138) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 17.1716 | Test Loss 130.9320
Epoch 0427 | Time 0.131 (0.138) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1653 | Test Loss 157.5929
Epoch 0428 | Time 0.137 (0.138) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1589 | Test Loss 155.5508
Epoch 0429 | Time 0.139 (0.138) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1525 | Test Loss 178.5474
Epoch 0430 | Time 0.140 (0.138) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1462 | Test Loss 137.9550
Epoch 0431 | Time 0.142 (0.138) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1399 | Test Loss 107.2403
Epoch 0432 | Time 0.130 (0.138) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1335 | Test Loss 137.1426
Epoch 0433 | Time 0.133 (0.138) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1272 | Test Loss 165.1015
Epoch 0434 | Time 0.134 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1209 | Test Loss 157.7017
Epoch 0435 | Time 0.135 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1146 | Test Loss 134.5656
Epoch 0436 | Time 0.135 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1083 | Test Loss 110.2635
Epoch 0437 | Time 0.130 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.1021 | Test Loss 169.5483
Epoch 0438 | Time 0.129 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0958 | Test Loss 130.9619
Epoch 0439 | Time 0.136 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0895 | Test Loss 150.3038
Epoch 0440 | Time 0.132 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0833 | Test Loss 111.6930
Epoch 0441 | Time 0.135 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0770 | Test Loss 175.7972
Epoch 0442 | Time 0.135 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0708 | Test Loss 156.3423
Epoch 0443 | Time 0.140 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0646 | Test Loss 170.3728
Epoch 0444 | Time 0.133 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0583 | Test Loss 122.8772
Epoch 0445 | Time 0.135 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0521 | Test Loss 195.9344
Epoch 0446 | Time 0.130 (0.137) | NFE-F 59.5 | NFE-B 0.0 | Train Loss 17.0459 | Test Loss 137.3758
Epoch 0447 | Time 0.130 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 17.0397 | Test Loss 169.8685
Epoch 0448 | Time 0.137 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 17.0335 | Test Loss 139.7376
Epoch 0449 | Time 0.137 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 17.0273 | Test Loss 165.5316
Epoch 0450 | Time 0.140 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 17.0211 | Test Loss 139.6266
Epoch 0451 | Time 0.146 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 17.0150 | Test Loss 166.6582
Epoch 0452 | Time 0.144 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 17.0088 | Test Loss 135.3487
Epoch 0453 | Time 0.134 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 17.0026 | Test Loss 168.1092
Epoch 0454 | Time 0.134 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9965 | Test Loss 156.5404
Epoch 0455 | Time 0.131 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9903 | Test Loss 190.1479
Epoch 0456 | Time 0.129 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9841 | Test Loss 168.0762
Epoch 0457 | Time 0.130 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9780 | Test Loss 149.6018
Epoch 0458 | Time 0.151 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9719 | Test Loss 203.0204
Epoch 0459 | Time 0.131 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9657 | Test Loss 158.4240
Epoch 0460 | Time 0.138 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9596 | Test Loss 114.4621
Epoch 0461 | Time 0.137 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9535 | Test Loss 166.9471
Epoch 0462 | Time 0.140 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9474 | Test Loss 168.1821
Epoch 0463 | Time 0.138 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9412 | Test Loss 133.0844
Epoch 0464 | Time 0.131 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9351 | Test Loss 153.8335
Epoch 0465 | Time 0.137 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9290 | Test Loss 134.8111
Epoch 0466 | Time 0.130 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9229 | Test Loss 173.5668
Epoch 0467 | Time 0.136 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9168 | Test Loss 146.3714
Epoch 0468 | Time 0.131 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9107 | Test Loss 160.5336
Epoch 0469 | Time 0.144 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.9047 | Test Loss 193.8503
Epoch 0470 | Time 0.136 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.8986 | Test Loss 171.0585
Epoch 0471 | Time 0.133 (0.137) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 16.8925 | Test Loss 164.5196
Epoch 0472 | Time 0.126 (0.137) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8864 | Test Loss 154.9466
Epoch 0473 | Time 0.135 (0.137) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8804 | Test Loss 171.2496
Epoch 0474 | Time 0.176 (0.137) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8743 | Test Loss 168.9458
Epoch 0475 | Time 0.162 (0.137) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8682 | Test Loss 144.0542
Epoch 0476 | Time 0.136 (0.137) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8622 | Test Loss 173.0412
Epoch 0477 | Time 0.184 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8561 | Test Loss 158.0357
Epoch 0478 | Time 0.147 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8501 | Test Loss 172.5826
Epoch 0479 | Time 0.150 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8440 | Test Loss 178.1251
Epoch 0480 | Time 0.134 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8380 | Test Loss 164.2541
Epoch 0481 | Time 0.139 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8320 | Test Loss 203.0951
Epoch 0482 | Time 0.135 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8259 | Test Loss 186.3353
Epoch 0483 | Time 0.141 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8199 | Test Loss 226.2394
Epoch 0484 | Time 0.148 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8139 | Test Loss 156.3151
Epoch 0485 | Time 0.157 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8079 | Test Loss 189.3750
Epoch 0486 | Time 0.161 (0.139) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.8018 | Test Loss 123.3547
Epoch 0487 | Time 0.132 (0.138) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7958 | Test Loss 196.0622
Epoch 0488 | Time 0.152 (0.139) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7898 | Test Loss 189.8717
Epoch 0489 | Time 0.167 (0.139) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7838 | Test Loss 183.5939
Epoch 0490 | Time 0.161 (0.139) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7778 | Test Loss 181.9025
Epoch 0491 | Time 0.133 (0.139) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7718 | Test Loss 215.3847
Epoch 0492 | Time 0.136 (0.139) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7658 | Test Loss 129.5240
Epoch 0493 | Time 0.194 (0.140) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7598 | Test Loss 194.2796
Epoch 0494 | Time 0.148 (0.140) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7538 | Test Loss 151.4607
Epoch 0495 | Time 0.143 (0.140) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7479 | Test Loss 190.8577
Epoch 0496 | Time 0.154 (0.140) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7419 | Test Loss 196.2754
Epoch 0497 | Time 0.191 (0.140) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7359 | Test Loss 157.9795
Epoch 0498 | Time 0.177 (0.141) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7299 | Test Loss 200.0860
Epoch 0499 | Time 0.193 (0.141) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7240 | Test Loss 146.1699
Epoch 0500 | Time 0.141 (0.141) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7180 | Test Loss 167.0097
Epoch 0501 | Time 0.127 (0.141) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7120 | Test Loss 171.3568
Epoch 0502 | Time 0.130 (0.141) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7061 | Test Loss 211.7854
Epoch 0503 | Time 0.127 (0.141) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.7001 | Test Loss 183.2177
Epoch 0504 | Time 0.133 (0.141) | NFE-F 59.7 | NFE-B 0.0 | Train Loss 16.6942 | Test Loss 169.3860
Epoch 0505 | Time 0.146 (0.141) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6882 | Test Loss 168.9165
Epoch 0506 | Time 0.198 (0.141) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6823 | Test Loss 199.8494
Epoch 0507 | Time 0.143 (0.141) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6763 | Test Loss 218.9052
Epoch 0508 | Time 0.149 (0.141) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6704 | Test Loss 196.1257
Epoch 0509 | Time 0.145 (0.141) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6645 | Test Loss 167.7911
Epoch 0510 | Time 0.131 (0.141) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6585 | Test Loss 174.0312
Epoch 0511 | Time 0.158 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6526 | Test Loss 161.4595
Epoch 0512 | Time 0.157 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6467 | Test Loss 177.4973
Epoch 0513 | Time 0.158 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6408 | Test Loss 214.4953
Epoch 0514 | Time 0.165 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6348 | Test Loss 188.5526
Epoch 0515 | Time 0.140 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6289 | Test Loss 169.4945
Epoch 0516 | Time 0.157 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6230 | Test Loss 183.4460
Epoch 0517 | Time 0.140 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6171 | Test Loss 192.2220
Epoch 0518 | Time 0.135 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6112 | Test Loss 147.1859
Epoch 0519 | Time 0.154 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.6053 | Test Loss 129.7015
Epoch 0520 | Time 0.162 (0.142) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5994 | Test Loss 185.1256
Epoch 0521 | Time 0.155 (0.143) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5935 | Test Loss 188.6422
Epoch 0522 | Time 0.173 (0.143) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5876 | Test Loss 183.8183
Epoch 0523 | Time 0.144 (0.143) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5817 | Test Loss 158.3066
Epoch 0524 | Time 0.160 (0.143) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5759 | Test Loss 147.5057
Epoch 0525 | Time 0.160 (0.143) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5700 | Test Loss 176.2554
Epoch 0526 | Time 0.157 (0.143) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5641 | Test Loss 185.8609
Epoch 0527 | Time 0.183 (0.144) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5582 | Test Loss 159.1892
Epoch 0528 | Time 0.242 (0.145) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5524 | Test Loss 194.7226
Epoch 0529 | Time 0.160 (0.145) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5465 | Test Loss 156.7179
Epoch 0530 | Time 0.180 (0.145) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5406 | Test Loss 190.4727
Epoch 0531 | Time 0.230 (0.146) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5348 | Test Loss 220.7910
Epoch 0532 | Time 0.270 (0.147) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5289 | Test Loss 195.0414
Epoch 0533 | Time 0.198 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5231 | Test Loss 208.0681
Epoch 0534 | Time 0.146 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5172 | Test Loss 200.4796
Epoch 0535 | Time 0.147 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5114 | Test Loss 190.3209
Epoch 0536 | Time 0.177 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.5055 | Test Loss 148.5501
Epoch 0537 | Time 0.139 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4997 | Test Loss 136.9543
Epoch 0538 | Time 0.152 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4938 | Test Loss 160.2144
Epoch 0539 | Time 0.138 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4880 | Test Loss 213.2667
Epoch 0540 | Time 0.146 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4822 | Test Loss 157.1095
Epoch 0541 | Time 0.181 (0.148) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4763 | Test Loss 188.3846
Epoch 0542 | Time 0.234 (0.149) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4705 | Test Loss 216.1622
Epoch 0543 | Time 0.188 (0.150) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4647 | Test Loss 169.2286
Epoch 0544 | Time 0.200 (0.150) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4589 | Test Loss 156.9082
Epoch 0545 | Time 0.182 (0.150) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4531 | Test Loss 191.4149
Epoch 0546 | Time 0.182 (0.151) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4472 | Test Loss 186.0000
Epoch 0547 | Time 0.183 (0.151) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4414 | Test Loss 232.8529
Epoch 0548 | Time 0.159 (0.151) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4356 | Test Loss 199.7058
Epoch 0549 | Time 0.162 (0.151) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4298 | Test Loss 198.1270
Epoch 0550 | Time 0.207 (0.152) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4240 | Test Loss 212.0956
Epoch 0551 | Time 0.207 (0.152) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4182 | Test Loss 218.3063
Epoch 0552 | Time 0.218 (0.153) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4124 | Test Loss 209.4918
Epoch 0553 | Time 0.218 (0.154) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4067 | Test Loss 170.4302
Epoch 0554 | Time 0.216 (0.154) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.4009 | Test Loss 174.6248
Epoch 0555 | Time 0.229 (0.155) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 16.3951 | Test Loss 161.3206
Epoch 0556 | Time 0.218 (0.156) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3893 | Test Loss 189.1362
Epoch 0557 | Time 0.212 (0.156) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3835 | Test Loss 208.0521
Epoch 0558 | Time 0.222 (0.157) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3778 | Test Loss 173.5503
Epoch 0559 | Time 0.215 (0.157) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3720 | Test Loss 151.9322
Epoch 0560 | Time 0.172 (0.158) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3662 | Test Loss 233.8223
Epoch 0561 | Time 0.164 (0.158) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3605 | Test Loss 178.6321
Epoch 0562 | Time 0.173 (0.158) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3547 | Test Loss 172.0042
Epoch 0563 | Time 0.134 (0.158) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3489 | Test Loss 167.4314
Epoch 0564 | Time 0.141 (0.157) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3432 | Test Loss 152.9774
Epoch 0565 | Time 0.138 (0.157) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3374 | Test Loss 212.1932
Epoch 0566 | Time 0.140 (0.157) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3317 | Test Loss 204.0361
Epoch 0567 | Time 0.137 (0.157) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3259 | Test Loss 171.7738
Epoch 0568 | Time 0.139 (0.157) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3202 | Test Loss 194.6878
Epoch 0569 | Time 0.137 (0.156) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3145 | Test Loss 219.2226
Epoch 0570 | Time 0.128 (0.156) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3087 | Test Loss 176.9623
Epoch 0571 | Time 0.132 (0.156) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.3030 | Test Loss 178.7944
Epoch 0572 | Time 0.129 (0.156) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2973 | Test Loss 154.2999
Epoch 0573 | Time 0.128 (0.155) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2916 | Test Loss 245.8895
Epoch 0574 | Time 0.128 (0.155) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2858 | Test Loss 201.0654
Epoch 0575 | Time 0.124 (0.155) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2801 | Test Loss 165.6963
Epoch 0576 | Time 0.126 (0.155) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2744 | Test Loss 217.5176
Epoch 0577 | Time 0.151 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2687 | Test Loss 205.7086
Epoch 0578 | Time 0.131 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2630 | Test Loss 234.3434
Epoch 0579 | Time 0.135 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2573 | Test Loss 177.8679
Epoch 0580 | Time 0.126 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2516 | Test Loss 222.3354
Epoch 0581 | Time 0.133 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2459 | Test Loss 182.5290
Epoch 0582 | Time 0.133 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2402 | Test Loss 219.0183
Epoch 0583 | Time 0.134 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2345 | Test Loss 175.5090
Epoch 0584 | Time 0.143 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2288 | Test Loss 182.7318
Epoch 0585 | Time 0.135 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2231 | Test Loss 203.2743
Epoch 0586 | Time 0.140 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2174 | Test Loss 194.6297
Epoch 0587 | Time 0.138 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2118 | Test Loss 215.8751
Epoch 0588 | Time 0.139 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2061 | Test Loss 162.1993
Epoch 0589 | Time 0.130 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.2004 | Test Loss 188.1575
Epoch 0590 | Time 0.132 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1948 | Test Loss 234.9651
Epoch 0591 | Time 0.135 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1891 | Test Loss 213.3264
Epoch 0592 | Time 0.165 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1834 | Test Loss 186.2573
Epoch 0593 | Time 0.223 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1778 | Test Loss 226.7028
Epoch 0594 | Time 0.206 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1721 | Test Loss 219.3451
Epoch 0595 | Time 0.194 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1665 | Test Loss 174.6814
Epoch 0596 | Time 0.185 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1608 | Test Loss 185.4232
Epoch 0597 | Time 0.191 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1552 | Test Loss 208.1367
Epoch 0598 | Time 0.137 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1495 | Test Loss 200.2316
Epoch 0599 | Time 0.136 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1439 | Test Loss 170.2341
Epoch 0600 | Time 0.131 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1382 | Test Loss 203.9568
Epoch 0601 | Time 0.134 (0.154) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1326 | Test Loss 186.4143
Epoch 0602 | Time 0.138 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1270 | Test Loss 219.1048
Epoch 0603 | Time 0.134 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1214 | Test Loss 163.3755
Epoch 0604 | Time 0.142 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1157 | Test Loss 187.8764
Epoch 0605 | Time 0.136 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1101 | Test Loss 203.5818
Epoch 0606 | Time 0.134 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.1045 | Test Loss 199.9079
Epoch 0607 | Time 0.134 (0.153) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0989 | Test Loss 179.3923
Epoch 0608 | Time 0.136 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0933 | Test Loss 187.9915
Epoch 0609 | Time 0.136 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0877 | Test Loss 222.1392
Epoch 0610 | Time 0.132 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0821 | Test Loss 189.4557
Epoch 0611 | Time 0.140 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0765 | Test Loss 196.2436
Epoch 0612 | Time 0.130 (0.152) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0709 | Test Loss 179.9102
Epoch 0613 | Time 0.134 (0.151) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0653 | Test Loss 210.4547
Epoch 0614 | Time 0.134 (0.151) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0597 | Test Loss 181.3560
Epoch 0615 | Time 0.126 (0.151) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0541 | Test Loss 193.9235
Epoch 0616 | Time 0.128 (0.151) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0485 | Test Loss 233.6349
Epoch 0617 | Time 0.130 (0.151) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0429 | Test Loss 165.5932
Epoch 0618 | Time 0.133 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0373 | Test Loss 168.8081
Epoch 0619 | Time 0.135 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0318 | Test Loss 209.3372
Epoch 0620 | Time 0.127 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0262 | Test Loss 181.3830
Epoch 0621 | Time 0.130 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0206 | Test Loss 222.7181
Epoch 0622 | Time 0.127 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0151 | Test Loss 234.5184
Epoch 0623 | Time 0.126 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0095 | Test Loss 197.5725
Epoch 0624 | Time 0.128 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 16.0039 | Test Loss 214.2615
Epoch 0625 | Time 0.131 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9984 | Test Loss 202.3849
Epoch 0626 | Time 0.128 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9928 | Test Loss 174.6658
Epoch 0627 | Time 0.133 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9873 | Test Loss 246.9614
Epoch 0628 | Time 0.131 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9817 | Test Loss 162.1075
Epoch 0629 | Time 0.138 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9762 | Test Loss 198.0821
Epoch 0630 | Time 0.128 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9707 | Test Loss 207.7897
Epoch 0631 | Time 0.155 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9651 | Test Loss 184.9540
Epoch 0632 | Time 0.134 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9596 | Test Loss 244.6387
Epoch 0633 | Time 0.132 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9541 | Test Loss 200.4814
Epoch 0634 | Time 0.139 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9485 | Test Loss 212.8964
Epoch 0635 | Time 0.132 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9430 | Test Loss 196.0690
Epoch 0636 | Time 0.232 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9375 | Test Loss 179.7834
Epoch 0637 | Time 0.177 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9320 | Test Loss 211.0282
Epoch 0638 | Time 0.141 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9265 | Test Loss 137.7727
Epoch 0639 | Time 0.133 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9209 | Test Loss 233.3167
Epoch 0640 | Time 0.138 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9154 | Test Loss 160.9950
Epoch 0641 | Time 0.137 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9099 | Test Loss 215.2801
Epoch 0642 | Time 0.138 (0.148) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.9044 | Test Loss 248.5824
Epoch 0643 | Time 0.222 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8989 | Test Loss 207.1433
Epoch 0644 | Time 0.199 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8934 | Test Loss 230.1814
Epoch 0645 | Time 0.135 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8879 | Test Loss 246.0745
Epoch 0646 | Time 0.142 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8825 | Test Loss 215.5046
Epoch 0647 | Time 0.134 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8770 | Test Loss 210.3604
Epoch 0648 | Time 0.132 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8715 | Test Loss 270.1736
Epoch 0649 | Time 0.144 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8660 | Test Loss 210.5007
Epoch 0650 | Time 0.144 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8605 | Test Loss 218.4422
Epoch 0651 | Time 0.140 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8551 | Test Loss 220.6139
Epoch 0652 | Time 0.150 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8496 | Test Loss 203.0714
Epoch 0653 | Time 0.138 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8441 | Test Loss 229.2719
Epoch 0654 | Time 0.197 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8387 | Test Loss 147.4843
Epoch 0655 | Time 0.209 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8332 | Test Loss 220.8577
Epoch 0656 | Time 0.141 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8277 | Test Loss 220.1810
Epoch 0657 | Time 0.136 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8223 | Test Loss 243.7706
Epoch 0658 | Time 0.140 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8168 | Test Loss 226.6085
Epoch 0659 | Time 0.136 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8114 | Test Loss 226.2160
Epoch 0660 | Time 0.184 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8059 | Test Loss 194.9646
Epoch 0661 | Time 0.143 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.8005 | Test Loss 178.2619
Epoch 0662 | Time 0.138 (0.150) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.7950 | Test Loss 234.8005
Epoch 0663 | Time 0.132 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.7896 | Test Loss 247.2128
Epoch 0664 | Time 0.126 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.7842 | Test Loss 202.2936
Epoch 0665 | Time 0.129 (0.149) | NFE-F 59.9 | NFE-B 0.0 | Train Loss 15.7787 | Test Loss 207.5565
Epoch 0666 | Time 0.131 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7733 | Test Loss 202.8951
Epoch 0667 | Time 0.199 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7679 | Test Loss 211.7889
Epoch 0668 | Time 0.184 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7625 | Test Loss 166.5938
Epoch 0669 | Time 0.170 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7571 | Test Loss 210.1033
Epoch 0670 | Time 0.150 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7516 | Test Loss 232.0151
Epoch 0671 | Time 0.158 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7462 | Test Loss 224.5731
Epoch 0672 | Time 0.152 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7408 | Test Loss 213.6783
Epoch 0673 | Time 0.185 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7354 | Test Loss 198.4856
Epoch 0674 | Time 0.146 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7300 | Test Loss 207.3515
Epoch 0675 | Time 0.132 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7246 | Test Loss 243.9178
Epoch 0676 | Time 0.157 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7192 | Test Loss 171.1788
Epoch 0677 | Time 0.139 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7138 | Test Loss 165.1274
Epoch 0678 | Time 0.158 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7084 | Test Loss 186.9145
Epoch 0679 | Time 0.168 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.7030 | Test Loss 280.7147
Epoch 0680 | Time 0.169 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6977 | Test Loss 216.6690
Epoch 0681 | Time 0.181 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6923 | Test Loss 192.3319
Epoch 0682 | Time 0.179 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6869 | Test Loss 212.4729
Epoch 0683 | Time 0.183 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6815 | Test Loss 249.6927
Epoch 0684 | Time 0.140 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6761 | Test Loss 208.0598
Epoch 0685 | Time 0.138 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6708 | Test Loss 244.6673
Epoch 0686 | Time 0.134 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6654 | Test Loss 198.2083
Epoch 0687 | Time 0.136 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6600 | Test Loss 197.8999
Epoch 0688 | Time 0.139 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6547 | Test Loss 186.5487
Epoch 0689 | Time 0.141 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6493 | Test Loss 220.1789
Epoch 0690 | Time 0.168 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6440 | Test Loss 191.7478
Epoch 0691 | Time 0.145 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6386 | Test Loss 208.5387
Epoch 0692 | Time 0.151 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6333 | Test Loss 211.3654
Epoch 0693 | Time 0.147 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6279 | Test Loss 152.4785
Epoch 0694 | Time 0.131 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6226 | Test Loss 211.5719
Epoch 0695 | Time 0.142 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6172 | Test Loss 232.4455
Epoch 0696 | Time 0.131 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6119 | Test Loss 198.5070
Epoch 0697 | Time 0.153 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6066 | Test Loss 225.9613
Epoch 0698 | Time 0.188 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.6012 | Test Loss 231.4180
Epoch 0699 | Time 0.231 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5959 | Test Loss 252.2641
Epoch 0700 | Time 0.168 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5906 | Test Loss 233.5491
Epoch 0701 | Time 0.191 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5853 | Test Loss 155.0959
Epoch 0702 | Time 0.252 (0.153) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5800 | Test Loss 213.8109
Epoch 0703 | Time 0.290 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5746 | Test Loss 262.2497
Epoch 0704 | Time 0.168 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5693 | Test Loss 188.8102
Epoch 0705 | Time 0.146 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5640 | Test Loss 151.5757
Epoch 0706 | Time 0.176 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5587 | Test Loss 205.4994
Epoch 0707 | Time 0.147 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5534 | Test Loss 224.1695
Epoch 0708 | Time 0.140 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5481 | Test Loss 188.2890
Epoch 0709 | Time 0.137 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5428 | Test Loss 229.6776
Epoch 0710 | Time 0.186 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5375 | Test Loss 202.8466
Epoch 0711 | Time 0.236 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5322 | Test Loss 240.0451
Epoch 0712 | Time 0.200 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5269 | Test Loss 225.0995
Epoch 0713 | Time 0.211 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5216 | Test Loss 214.2594
Epoch 0714 | Time 0.187 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5164 | Test Loss 191.7975
Epoch 0715 | Time 0.245 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5111 | Test Loss 197.4250
Epoch 0716 | Time 0.219 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5058 | Test Loss 225.5139
Epoch 0717 | Time 0.233 (0.159) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.5005 | Test Loss 240.8296
Epoch 0718 | Time 0.314 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4952 | Test Loss 247.4850
Epoch 0719 | Time 0.252 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4900 | Test Loss 225.6324
Epoch 0720 | Time 0.198 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4847 | Test Loss 241.3519
Epoch 0721 | Time 0.199 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4794 | Test Loss 207.8870
Epoch 0722 | Time 0.266 (0.163) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4742 | Test Loss 237.7573
Epoch 0723 | Time 0.197 (0.163) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4689 | Test Loss 242.0647
Epoch 0724 | Time 0.268 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4637 | Test Loss 256.0261
Epoch 0725 | Time 0.237 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4584 | Test Loss 218.5879
Epoch 0726 | Time 0.209 (0.166) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4532 | Test Loss 150.8192
Epoch 0727 | Time 0.147 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4479 | Test Loss 245.2787
Epoch 0728 | Time 0.156 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4427 | Test Loss 257.2350
Epoch 0729 | Time 0.148 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4374 | Test Loss 246.2881
Epoch 0730 | Time 0.169 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4322 | Test Loss 180.8631
Epoch 0731 | Time 0.155 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4270 | Test Loss 231.8798
Epoch 0732 | Time 0.187 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4217 | Test Loss 221.8349
Epoch 0733 | Time 0.273 (0.166) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4165 | Test Loss 253.7269
Epoch 0734 | Time 0.175 (0.167) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4113 | Test Loss 211.6372
Epoch 0735 | Time 0.175 (0.167) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4060 | Test Loss 225.7738
Epoch 0736 | Time 0.133 (0.166) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.4008 | Test Loss 208.3576
Epoch 0737 | Time 0.215 (0.167) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3956 | Test Loss 231.3474
Epoch 0738 | Time 0.138 (0.166) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3904 | Test Loss 288.8828
Epoch 0739 | Time 0.141 (0.166) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3852 | Test Loss 300.2620
Epoch 0740 | Time 0.135 (0.166) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3800 | Test Loss 247.2405
Epoch 0741 | Time 0.136 (0.166) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3747 | Test Loss 188.8640
Epoch 0742 | Time 0.137 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3695 | Test Loss 181.4594
Epoch 0743 | Time 0.165 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3643 | Test Loss 272.2112
Epoch 0744 | Time 0.142 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3591 | Test Loss 189.0094
Epoch 0745 | Time 0.165 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3539 | Test Loss 235.0161
Epoch 0746 | Time 0.153 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3488 | Test Loss 278.2993
Epoch 0747 | Time 0.136 (0.165) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3436 | Test Loss 232.8560
Epoch 0748 | Time 0.132 (0.164) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3384 | Test Loss 196.6432
Epoch 0749 | Time 0.135 (0.164) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3332 | Test Loss 250.1145
Epoch 0750 | Time 0.135 (0.164) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3280 | Test Loss 192.1160
Epoch 0751 | Time 0.130 (0.163) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3228 | Test Loss 246.9624
Epoch 0752 | Time 0.136 (0.163) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3176 | Test Loss 232.4395
Epoch 0753 | Time 0.146 (0.163) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3125 | Test Loss 329.9490
Epoch 0754 | Time 0.138 (0.163) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3073 | Test Loss 221.1144
Epoch 0755 | Time 0.138 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.3021 | Test Loss 243.9036
Epoch 0756 | Time 0.138 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2970 | Test Loss 208.8357
Epoch 0757 | Time 0.142 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2918 | Test Loss 235.1973
Epoch 0758 | Time 0.139 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2866 | Test Loss 254.1396
Epoch 0759 | Time 0.133 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2815 | Test Loss 222.7238
Epoch 0760 | Time 0.148 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2763 | Test Loss 205.6709
Epoch 0761 | Time 0.241 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2712 | Test Loss 239.4372
Epoch 0762 | Time 0.161 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2660 | Test Loss 222.1977
Epoch 0763 | Time 0.143 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2609 | Test Loss 208.3878
Epoch 0764 | Time 0.141 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2557 | Test Loss 235.5556
Epoch 0765 | Time 0.139 (0.162) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2506 | Test Loss 233.2300
Epoch 0766 | Time 0.145 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2454 | Test Loss 208.6635
Epoch 0767 | Time 0.141 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2403 | Test Loss 274.6833
Epoch 0768 | Time 0.143 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2352 | Test Loss 187.4151
Epoch 0769 | Time 0.141 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2300 | Test Loss 199.2618
Epoch 0770 | Time 0.147 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2249 | Test Loss 223.7715
Epoch 0771 | Time 0.217 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2198 | Test Loss 219.6697
Epoch 0772 | Time 0.155 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2146 | Test Loss 309.3005
Epoch 0773 | Time 0.145 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2095 | Test Loss 224.0860
Epoch 0774 | Time 0.137 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.2044 | Test Loss 196.0721
Epoch 0775 | Time 0.137 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1993 | Test Loss 259.6271
Epoch 0776 | Time 0.141 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1942 | Test Loss 263.1327
Epoch 0777 | Time 0.141 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1891 | Test Loss 209.9189
Epoch 0778 | Time 0.138 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1840 | Test Loss 237.2254
Epoch 0779 | Time 0.208 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1788 | Test Loss 201.8620
Epoch 0780 | Time 0.187 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1737 | Test Loss 221.8123
Epoch 0781 | Time 0.163 (0.161) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1686 | Test Loss 296.1770
Epoch 0782 | Time 0.138 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1635 | Test Loss 278.4136
Epoch 0783 | Time 0.143 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1584 | Test Loss 201.0921
Epoch 0784 | Time 0.140 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1534 | Test Loss 229.4011
Epoch 0785 | Time 0.137 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1483 | Test Loss 243.8976
Epoch 0786 | Time 0.135 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1432 | Test Loss 194.8697
Epoch 0787 | Time 0.148 (0.159) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1381 | Test Loss 274.7164
Epoch 0788 | Time 0.185 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1330 | Test Loss 263.3458
Epoch 0789 | Time 0.196 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1279 | Test Loss 207.3270
Epoch 0790 | Time 0.134 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1228 | Test Loss 230.5239
Epoch 0791 | Time 0.149 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1178 | Test Loss 216.8486
Epoch 0792 | Time 0.141 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1127 | Test Loss 231.4079
Epoch 0793 | Time 0.155 (0.160) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1076 | Test Loss 272.9803
Epoch 0794 | Time 0.148 (0.159) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.1026 | Test Loss 274.6717
Epoch 0795 | Time 0.145 (0.159) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0975 | Test Loss 242.8023
Epoch 0796 | Time 0.136 (0.159) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0924 | Test Loss 290.3130
Epoch 0797 | Time 0.140 (0.159) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0874 | Test Loss 225.1538
Epoch 0798 | Time 0.138 (0.159) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0823 | Test Loss 196.0391
Epoch 0799 | Time 0.141 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0773 | Test Loss 177.0656
Epoch 0800 | Time 0.142 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0722 | Test Loss 276.3322
Epoch 0801 | Time 0.136 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0672 | Test Loss 267.0944
Epoch 0802 | Time 0.138 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0621 | Test Loss 246.0150
Epoch 0803 | Time 0.141 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0571 | Test Loss 219.1322
Epoch 0804 | Time 0.138 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0520 | Test Loss 250.5946
Epoch 0805 | Time 0.168 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0470 | Test Loss 300.2415
Epoch 0806 | Time 0.152 (0.158) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0419 | Test Loss 277.2934
Epoch 0807 | Time 0.144 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0369 | Test Loss 261.0051
Epoch 0808 | Time 0.143 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0319 | Test Loss 259.7975
Epoch 0809 | Time 0.146 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0268 | Test Loss 236.4400
Epoch 0810 | Time 0.140 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0218 | Test Loss 258.4342
Epoch 0811 | Time 0.140 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0168 | Test Loss 264.1613
Epoch 0812 | Time 0.143 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0118 | Test Loss 357.5313
Epoch 0813 | Time 0.131 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0067 | Test Loss 239.2179
Epoch 0814 | Time 0.180 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 15.0017 | Test Loss 277.0023
Epoch 0815 | Time 0.183 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9967 | Test Loss 327.2941
Epoch 0816 | Time 0.142 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9917 | Test Loss 268.3027
Epoch 0817 | Time 0.137 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9867 | Test Loss 282.3476
Epoch 0818 | Time 0.141 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9817 | Test Loss 289.8909
Epoch 0819 | Time 0.175 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9767 | Test Loss 236.8362
Epoch 0820 | Time 0.153 (0.157) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9717 | Test Loss 247.7054
Epoch 0821 | Time 0.138 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9667 | Test Loss 337.7065
Epoch 0822 | Time 0.139 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9617 | Test Loss 273.4162
Epoch 0823 | Time 0.140 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9567 | Test Loss 321.5367
Epoch 0824 | Time 0.140 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9517 | Test Loss 291.5616
Epoch 0825 | Time 0.139 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9467 | Test Loss 276.6623
Epoch 0826 | Time 0.136 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9417 | Test Loss 337.8416
Epoch 0827 | Time 0.170 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9367 | Test Loss 246.1561
Epoch 0828 | Time 0.146 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9317 | Test Loss 349.0540
Epoch 0829 | Time 0.142 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9267 | Test Loss 283.6796
Epoch 0830 | Time 0.140 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9218 | Test Loss 283.1519
Epoch 0831 | Time 0.188 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9168 | Test Loss 240.1901
Epoch 0832 | Time 0.184 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9118 | Test Loss 270.6982
Epoch 0833 | Time 0.147 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9068 | Test Loss 260.4682
Epoch 0834 | Time 0.141 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.9019 | Test Loss 217.8750
Epoch 0835 | Time 0.146 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8969 | Test Loss 312.6415
Epoch 0836 | Time 0.156 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8919 | Test Loss 310.0742
Epoch 0837 | Time 0.149 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8870 | Test Loss 253.1580
Epoch 0838 | Time 0.172 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8820 | Test Loss 284.0720
Epoch 0839 | Time 0.150 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8770 | Test Loss 288.5384
Epoch 0840 | Time 0.147 (0.156) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8721 | Test Loss 299.7413
Epoch 0841 | Time 0.153 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8671 | Test Loss 328.8777
Epoch 0842 | Time 0.139 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8622 | Test Loss 301.8056
Epoch 0843 | Time 0.141 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8572 | Test Loss 248.9521
Epoch 0844 | Time 0.138 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8523 | Test Loss 222.6479
Epoch 0845 | Time 0.137 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8473 | Test Loss 230.8573
Epoch 0846 | Time 0.139 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8424 | Test Loss 185.1282
Epoch 0847 | Time 0.192 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8375 | Test Loss 312.4076
Epoch 0848 | Time 0.170 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8325 | Test Loss 249.2478
Epoch 0849 | Time 0.142 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8276 | Test Loss 269.0050
Epoch 0850 | Time 0.137 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8226 | Test Loss 329.1999
Epoch 0851 | Time 0.147 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8177 | Test Loss 240.2403
Epoch 0852 | Time 0.141 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8128 | Test Loss 288.6224
Epoch 0853 | Time 0.141 (0.155) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8079 | Test Loss 330.0006
Epoch 0854 | Time 0.139 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.8029 | Test Loss 253.1808
Epoch 0855 | Time 0.141 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7980 | Test Loss 344.0670
Epoch 0856 | Time 0.138 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7931 | Test Loss 319.8339
Epoch 0857 | Time 0.143 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7882 | Test Loss 255.7839
Epoch 0858 | Time 0.140 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7833 | Test Loss 290.0082
Epoch 0859 | Time 0.135 (0.154) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7783 | Test Loss 306.7622
Epoch 0860 | Time 0.137 (0.153) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7734 | Test Loss 338.3706
Epoch 0861 | Time 0.144 (0.153) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7685 | Test Loss 295.5138
Epoch 0862 | Time 0.135 (0.153) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7636 | Test Loss 180.6344
Epoch 0863 | Time 0.142 (0.153) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7587 | Test Loss 279.4827
Epoch 0864 | Time 0.134 (0.153) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7538 | Test Loss 236.6417
Epoch 0865 | Time 0.136 (0.153) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7489 | Test Loss 268.2603
Epoch 0866 | Time 0.137 (0.153) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7440 | Test Loss 290.4970
Epoch 0867 | Time 0.143 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7391 | Test Loss 180.0665
Epoch 0868 | Time 0.143 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7342 | Test Loss 270.0486
Epoch 0869 | Time 0.145 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7293 | Test Loss 216.2762
Epoch 0870 | Time 0.142 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7244 | Test Loss 269.4580
Epoch 0871 | Time 0.139 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7196 | Test Loss 256.6120
Epoch 0872 | Time 0.136 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7147 | Test Loss 264.6937
Epoch 0873 | Time 0.136 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7098 | Test Loss 289.2123
Epoch 0874 | Time 0.196 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7049 | Test Loss 219.9232
Epoch 0875 | Time 0.158 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.7000 | Test Loss 204.6281
Epoch 0876 | Time 0.140 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6951 | Test Loss 283.6036
Epoch 0877 | Time 0.172 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6903 | Test Loss 284.2212
Epoch 0878 | Time 0.154 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6854 | Test Loss 222.6427
Epoch 0879 | Time 0.146 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6805 | Test Loss 268.8154
Epoch 0880 | Time 0.147 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6757 | Test Loss 290.6159
Epoch 0881 | Time 0.148 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6708 | Test Loss 273.6282
Epoch 0882 | Time 0.143 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6659 | Test Loss 310.9496
Epoch 0883 | Time 0.145 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6611 | Test Loss 265.0332
Epoch 0884 | Time 0.140 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6562 | Test Loss 288.0872
Epoch 0885 | Time 0.137 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6514 | Test Loss 268.3228
Epoch 0886 | Time 0.134 (0.152) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6465 | Test Loss 310.3373
Epoch 0887 | Time 0.138 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6417 | Test Loss 349.0927
Epoch 0888 | Time 0.144 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6368 | Test Loss 276.4943
Epoch 0889 | Time 0.139 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6320 | Test Loss 320.5491
Epoch 0890 | Time 0.144 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6271 | Test Loss 332.6225
Epoch 0891 | Time 0.137 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6223 | Test Loss 333.7048
Epoch 0892 | Time 0.140 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6174 | Test Loss 285.2182
Epoch 0893 | Time 0.136 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6126 | Test Loss 330.5583
Epoch 0894 | Time 0.141 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6078 | Test Loss 234.2831
Epoch 0895 | Time 0.135 (0.151) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.6029 | Test Loss 306.9220
Epoch 0896 | Time 0.139 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5981 | Test Loss 261.9519
Epoch 0897 | Time 0.138 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5933 | Test Loss 219.1288
Epoch 0898 | Time 0.138 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5884 | Test Loss 256.9284
Epoch 0899 | Time 0.134 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5836 | Test Loss 316.0829
Epoch 0900 | Time 0.135 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5788 | Test Loss 234.0377
Epoch 0901 | Time 0.134 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5739 | Test Loss 184.4313
Epoch 0902 | Time 0.131 (0.150) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5691 | Test Loss 370.5779
Epoch 0903 | Time 0.144 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5643 | Test Loss 371.2846
Epoch 0904 | Time 0.137 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5595 | Test Loss 330.6062
Epoch 0905 | Time 0.136 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5547 | Test Loss 321.4879
Epoch 0906 | Time 0.134 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5499 | Test Loss 292.5486
Epoch 0907 | Time 0.138 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5451 | Test Loss 292.5315
Epoch 0908 | Time 0.133 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5402 | Test Loss 292.5214
Epoch 0909 | Time 0.141 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5354 | Test Loss 271.2975
Epoch 0910 | Time 0.142 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5306 | Test Loss 262.7658
Epoch 0911 | Time 0.143 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5258 | Test Loss 280.6160
Epoch 0912 | Time 0.140 (0.149) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5210 | Test Loss 266.2528
Epoch 0913 | Time 0.136 (0.148) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5162 | Test Loss 289.0421
Epoch 0914 | Time 0.133 (0.148) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5114 | Test Loss 230.8151
Epoch 0915 | Time 0.136 (0.148) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5066 | Test Loss 299.8970
Epoch 0916 | Time 0.138 (0.148) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.5019 | Test Loss 309.9496
Epoch 0917 | Time 0.139 (0.148) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4971 | Test Loss 275.9811
Epoch 0918 | Time 0.133 (0.148) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4923 | Test Loss 243.1057
Epoch 0919 | Time 0.136 (0.148) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4875 | Test Loss 292.6895
Epoch 0920 | Time 0.139 (0.148) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4827 | Test Loss 307.1536
Epoch 0921 | Time 0.132 (0.147) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4779 | Test Loss 248.5640
Epoch 0922 | Time 0.139 (0.147) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4731 | Test Loss 321.8927
Epoch 0923 | Time 0.135 (0.147) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4684 | Test Loss 291.9599
Epoch 0924 | Time 0.137 (0.147) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4636 | Test Loss 357.0478
Epoch 0925 | Time 0.136 (0.147) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4588 | Test Loss 304.2701
Epoch 0926 | Time 0.135 (0.147) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4541 | Test Loss 269.7053
Epoch 0927 | Time 0.134 (0.147) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4493 | Test Loss 297.8638
Epoch 0928 | Time 0.132 (0.147) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4445 | Test Loss 207.2676
Epoch 0929 | Time 0.135 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4397 | Test Loss 332.7231
Epoch 0930 | Time 0.132 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4350 | Test Loss 223.2450
Epoch 0931 | Time 0.136 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4302 | Test Loss 370.2050
Epoch 0932 | Time 0.138 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4255 | Test Loss 311.9155
Epoch 0933 | Time 0.127 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4207 | Test Loss 364.4955
Epoch 0934 | Time 0.133 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4159 | Test Loss 348.5494
Epoch 0935 | Time 0.133 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4112 | Test Loss 410.2198
Epoch 0936 | Time 0.138 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4064 | Test Loss 327.3488
Epoch 0937 | Time 0.142 (0.146) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.4017 | Test Loss 340.1207
Epoch 0938 | Time 0.134 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3969 | Test Loss 298.4966
Epoch 0939 | Time 0.145 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3922 | Test Loss 331.8058
Epoch 0940 | Time 0.144 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3875 | Test Loss 224.6101
Epoch 0941 | Time 0.136 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3827 | Test Loss 344.5678
Epoch 0942 | Time 0.134 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3780 | Test Loss 324.4983
Epoch 0943 | Time 0.137 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3732 | Test Loss 258.2943
Epoch 0944 | Time 0.139 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3685 | Test Loss 295.6133
Epoch 0945 | Time 0.137 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3638 | Test Loss 238.9566
Epoch 0946 | Time 0.137 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3590 | Test Loss 310.4563
Epoch 0947 | Time 0.137 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3543 | Test Loss 359.5283
Epoch 0948 | Time 0.133 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3496 | Test Loss 288.7944
Epoch 0949 | Time 0.129 (0.145) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3448 | Test Loss 358.6060
Epoch 0950 | Time 0.136 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3401 | Test Loss 392.4374
Epoch 0951 | Time 0.134 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3354 | Test Loss 249.4349
Epoch 0952 | Time 0.137 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3307 | Test Loss 347.5431
Epoch 0953 | Time 0.132 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3260 | Test Loss 279.2797
Epoch 0954 | Time 0.138 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3212 | Test Loss 297.0203
Epoch 0955 | Time 0.137 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3165 | Test Loss 342.7089
Epoch 0956 | Time 0.139 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3118 | Test Loss 378.9081
Epoch 0957 | Time 0.136 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3071 | Test Loss 350.0969
Epoch 0958 | Time 0.134 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.3024 | Test Loss 336.1913
Epoch 0959 | Time 0.133 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2977 | Test Loss 314.1438
Epoch 0960 | Time 0.129 (0.144) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2930 | Test Loss 305.7427
Epoch 0961 | Time 0.134 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2883 | Test Loss 220.5555
Epoch 0962 | Time 0.139 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2836 | Test Loss 300.0706
Epoch 0963 | Time 0.134 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2789 | Test Loss 342.2793
Epoch 0964 | Time 0.137 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2742 | Test Loss 318.1105
Epoch 0965 | Time 0.136 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2695 | Test Loss 199.5449
Epoch 0966 | Time 0.133 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2648 | Test Loss 310.4574
Epoch 0967 | Time 0.134 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2601 | Test Loss 258.3119
Epoch 0968 | Time 0.141 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2554 | Test Loss 264.1788
Epoch 0969 | Time 0.139 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2507 | Test Loss 226.5960
Epoch 0970 | Time 0.139 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2460 | Test Loss 343.3542
Epoch 0971 | Time 0.137 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2413 | Test Loss 343.6367
Epoch 0972 | Time 0.136 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2367 | Test Loss 317.7665
Epoch 0973 | Time 0.131 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2320 | Test Loss 343.3992
Epoch 0974 | Time 0.137 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2273 | Test Loss 405.8643
Epoch 0975 | Time 0.142 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2226 | Test Loss 294.5629
Epoch 0976 | Time 0.145 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2179 | Test Loss 333.6316
Epoch 0977 | Time 0.137 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2133 | Test Loss 282.6203
Epoch 0978 | Time 0.136 (0.143) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2086 | Test Loss 309.7469
Epoch 0979 | Time 0.133 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.2039 | Test Loss 292.1346
Epoch 0980 | Time 0.135 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1993 | Test Loss 393.0746
Epoch 0981 | Time 0.137 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1946 | Test Loss 394.9339
Epoch 0982 | Time 0.133 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1899 | Test Loss 329.5659
Epoch 0983 | Time 0.136 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1853 | Test Loss 322.8413
Epoch 0984 | Time 0.135 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1806 | Test Loss 296.3456
Epoch 0985 | Time 0.129 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1759 | Test Loss 231.4777
Epoch 0986 | Time 0.136 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1713 | Test Loss 431.3707
Epoch 0987 | Time 0.131 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1666 | Test Loss 309.5171
Epoch 0988 | Time 0.135 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1620 | Test Loss 342.1281
Epoch 0989 | Time 0.134 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1573 | Test Loss 363.8866
Epoch 0990 | Time 0.138 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1527 | Test Loss 357.3131
Epoch 0991 | Time 0.140 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1480 | Test Loss 368.4356
Epoch 0992 | Time 0.136 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1434 | Test Loss 318.3708
Epoch 0993 | Time 0.137 (0.141) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1387 | Test Loss 334.1653
Epoch 0994 | Time 0.143 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1341 | Test Loss 385.1482
Epoch 0995 | Time 0.143 (0.142) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1295 | Test Loss 316.4290
Epoch 0996 | Time 0.137 (0.141) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1248 | Test Loss 287.1566
Epoch 0997 | Time 0.136 (0.141) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1202 | Test Loss 386.9961
Epoch 0998 | Time 0.144 (0.141) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1155 | Test Loss 248.4962
Epoch 0999 | Time 0.135 (0.141) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 14.1109 | Test Loss 463.8899
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))

        return nn.functional.tanh(out)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.058 (0.058) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 6540.6738
Epoch 0001 | Time 0.092 (0.058) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 5302.3306
Epoch 0002 | Time 0.093 (0.058) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 4222.1074
Epoch 0003 | Time 0.089 (0.059) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 3275.9482
Epoch 0004 | Time 0.092 (0.059) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 2461.6404
Epoch 0005 | Time 0.094 (0.059) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 1776.9880
Epoch 0006 | Time 0.088 (0.060) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 1218.1875
Epoch 0007 | Time 0.093 (0.060) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 779.4883
Epoch 0008 | Time 0.089 (0.060) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 453.0060
Epoch 0009 | Time 0.095 (0.061) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 228.6401
Epoch 0010 | Time 0.089 (0.061) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 94.1450
Epoch 0011 | Time 0.096 (0.061) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 35.3990
Epoch 0012 | Time 0.093 (0.062) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 36.8999
Epoch 0013 | Time 0.090 (0.062) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 82.4740
Epoch 0014 | Time 0.097 (0.062) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 156.1418
Epoch 0015 | Time 0.094 (0.063) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 243.0336
Epoch 0016 | Time 0.096 (0.063) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 330.2278
Epoch 0017 | Time 0.095 (0.063) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 407.3905
Epoch 0018 | Time 0.092 (0.063) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 467.1378
Epoch 0019 | Time 0.093 (0.064) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 505.0981
Epoch 0020 | Time 0.088 (0.064) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 519.7061
Epoch 0021 | Time 0.089 (0.064) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 511.7943
Epoch 0022 | Time 0.086 (0.065) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 484.0649
Epoch 0023 | Time 0.094 (0.065) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 440.5143
Epoch 0024 | Time 0.091 (0.065) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 385.8712
Epoch 0025 | Time 0.092 (0.065) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 325.0879
Epoch 0026 | Time 0.093 (0.066) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 262.9089
Epoch 0027 | Time 0.091 (0.066) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 203.5307
Epoch 0028 | Time 0.087 (0.066) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 150.3549
Epoch 0029 | Time 0.088 (0.066) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 105.8365
Epoch 0030 | Time 0.089 (0.067) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 71.4211
Epoch 0031 | Time 0.091 (0.067) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 47.5664
Epoch 0032 | Time 0.092 (0.067) | NFE-F 25.5 | NFE-B 0.0 | Train Loss 33.8363
Epoch 0033 | Time 0.097 (0.067) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 29.0550
Epoch 0034 | Time 0.091 (0.068) | NFE-F 25.8 | NFE-B 0.0 | Train Loss 31.5038
Epoch 0035 | Time 0.090 (0.068) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 39.1418
Epoch 0036 | Time 0.095 (0.068) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 49.8282
Epoch 0037 | Time 0.095 (0.068) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 61.5267
Epoch 0038 | Time 0.094 (0.069) | NFE-F 26.3 | NFE-B 0.0 | Train Loss 72.4750
Epoch 0039 | Time 0.092 (0.069) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 81.3063
Epoch 0040 | Time 0.093 (0.069) | NFE-F 26.6 | NFE-B 0.0 | Train Loss 87.1172
Epoch 0041 | Time 0.087 (0.069) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 89.4809
Epoch 0042 | Time 0.091 (0.069) | NFE-F 26.9 | NFE-B 0.0 | Train Loss 88.4131
Epoch 0043 | Time 0.091 (0.070) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 84.2997
Epoch 0044 | Time 0.095 (0.070) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 77.7978
Epoch 0045 | Time 0.090 (0.070) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 69.7262
Epoch 0046 | Time 0.087 (0.070) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 60.9549
Epoch 0047 | Time 0.094 (0.071) | NFE-F 27.5 | NFE-B 0.0 | Train Loss 52.3067
Epoch 0048 | Time 0.095 (0.071) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 44.4782
Epoch 0049 | Time 0.092 (0.071) | NFE-F 27.8 | NFE-B 0.0 | Train Loss 37.9846
Epoch 0050 | Time 0.094 (0.071) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 33.1329
Epoch 0051 | Time 0.088 (0.071) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 30.0193
Epoch 0052 | Time 0.088 (0.072) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 28.5499
Epoch 0053 | Time 0.092 (0.072) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 28.4792
Epoch 0054 | Time 0.090 (0.072) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 29.4590
Epoch 0055 | Time 0.090 (0.072) | NFE-F 28.5 | NFE-B 0.0 | Train Loss 31.0917
Epoch 0056 | Time 0.087 (0.072) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 32.9811
Epoch 0057 | Time 0.092 (0.072) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 34.7764
Epoch 0058 | Time 0.100 (0.073) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 36.2036
Epoch 0059 | Time 0.091 (0.073) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 37.0843
Epoch 0060 | Time 0.088 (0.073) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 37.3403
Epoch 0061 | Time 0.092 (0.073) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 36.9863
Epoch 0062 | Time 0.086 (0.073) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 36.1124
Epoch 0063 | Time 0.092 (0.074) | NFE-F 29.4 | NFE-B 0.0 | Train Loss 34.8613
Epoch 0064 | Time 0.095 (0.074) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 33.4016
Epoch 0065 | Time 0.093 (0.074) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 31.9034
Epoch 0066 | Time 0.090 (0.074) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 30.5169
Epoch 0067 | Time 0.093 (0.074) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 29.3566
Epoch 0068 | Time 0.093 (0.074) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 28.4930
Epoch 0069 | Time 0.092 (0.075) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 27.9503
Epoch 0070 | Time 0.088 (0.075) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 27.7109
Epoch 0071 | Time 0.089 (0.075) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 27.7241
Epoch 0072 | Time 0.087 (0.075) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 27.9175
Epoch 0073 | Time 0.088 (0.075) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 28.2099
Epoch 0074 | Time 0.090 (0.075) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 28.5229
Epoch 0075 | Time 0.088 (0.075) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 28.7904
Epoch 0076 | Time 0.090 (0.076) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 28.9651
Epoch 0077 | Time 0.094 (0.076) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 29.0215
Epoch 0078 | Time 0.095 (0.076) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 28.9557
Epoch 0079 | Time 0.091 (0.076) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 28.7817
Epoch 0080 | Time 0.094 (0.076) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 28.5271
Epoch 0081 | Time 0.091 (0.076) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 28.2265
Epoch 0082 | Time 0.088 (0.077) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 27.9159
Epoch 0083 | Time 0.088 (0.077) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 27.6270
Epoch 0084 | Time 0.089 (0.077) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 27.3840
Epoch 0085 | Time 0.090 (0.077) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 27.2010
Epoch 0086 | Time 0.090 (0.077) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 27.0819
Epoch 0087 | Time 0.088 (0.077) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 27.0212
Epoch 0088 | Time 0.090 (0.077) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 27.0069
Epoch 0089 | Time 0.087 (0.077) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 27.0226
Epoch 0090 | Time 0.088 (0.078) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 27.0508
Epoch 0091 | Time 0.094 (0.078) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 27.0758
Epoch 0092 | Time 0.089 (0.078) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 27.0853
Epoch 0093 | Time 0.092 (0.078) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 27.0716
Epoch 0094 | Time 0.088 (0.078) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 27.0323
Epoch 0095 | Time 0.094 (0.078) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 26.9692
Epoch 0096 | Time 0.091 (0.078) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 26.8877
Epoch 0097 | Time 0.091 (0.078) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 26.7951
Epoch 0098 | Time 0.091 (0.079) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 26.6994
Epoch 0099 | Time 0.090 (0.079) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 26.6077
Epoch 0100 | Time 0.091 (0.079) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 26.5254
Epoch 0101 | Time 0.089 (0.079) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 26.4559
Epoch 0102 | Time 0.093 (0.079) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 26.3998
Epoch 0103 | Time 0.087 (0.079) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 26.3559
Epoch 0104 | Time 0.089 (0.079) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 26.3212
Epoch 0105 | Time 0.089 (0.079) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 26.2921
Epoch 0106 | Time 0.089 (0.079) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 26.2647
Epoch 0107 | Time 0.092 (0.080) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 26.2356
Epoch 0108 | Time 0.089 (0.080) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 26.2024
Epoch 0109 | Time 0.091 (0.080) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 26.1638
Epoch 0110 | Time 0.090 (0.080) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 26.1196
Epoch 0111 | Time 0.089 (0.080) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 26.0706
Epoch 0112 | Time 0.087 (0.080) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 26.0184
Epoch 0113 | Time 0.084 (0.080) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 25.9647
Epoch 0114 | Time 0.090 (0.080) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 25.9114
Epoch 0115 | Time 0.087 (0.080) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 25.8597
Epoch 0116 | Time 0.086 (0.080) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 25.8107
Epoch 0117 | Time 0.088 (0.080) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 25.7647
Epoch 0118 | Time 0.090 (0.080) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 25.7215
Epoch 0119 | Time 0.086 (0.081) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 25.6805
Epoch 0120 | Time 0.091 (0.081) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 25.6409
Epoch 0121 | Time 0.091 (0.081) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 25.6018
Epoch 0122 | Time 0.088 (0.081) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 25.5623
Epoch 0123 | Time 0.088 (0.081) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 25.5220
Epoch 0124 | Time 0.090 (0.081) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 25.4804
Epoch 0125 | Time 0.100 (0.081) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 25.4375
Epoch 0126 | Time 0.091 (0.081) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 25.3936
Epoch 0127 | Time 0.091 (0.081) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 25.3489
Epoch 0128 | Time 0.093 (0.081) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 25.3041
Epoch 0129 | Time 0.091 (0.082) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 25.2594
Epoch 0130 | Time 0.089 (0.082) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 25.2151
Epoch 0131 | Time 0.090 (0.082) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 25.1715
Epoch 0132 | Time 0.089 (0.082) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 25.1287
Epoch 0133 | Time 0.090 (0.082) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 25.0865
Epoch 0134 | Time 0.091 (0.082) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 25.0447
Epoch 0135 | Time 0.088 (0.082) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 25.0032
Epoch 0136 | Time 0.099 (0.082) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 24.9618
Epoch 0137 | Time 0.092 (0.082) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 24.9203
Epoch 0138 | Time 0.089 (0.082) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 24.8785
Epoch 0139 | Time 0.094 (0.082) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 24.8365
Epoch 0140 | Time 0.091 (0.083) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 24.7942
Epoch 0141 | Time 0.089 (0.083) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 24.7517
Epoch 0142 | Time 0.090 (0.083) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 24.7092
Epoch 0143 | Time 0.088 (0.083) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 24.6667
Epoch 0144 | Time 0.091 (0.083) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 24.6243
Epoch 0145 | Time 0.091 (0.083) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 24.5821
Epoch 0146 | Time 0.090 (0.083) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 24.5402
Epoch 0147 | Time 0.093 (0.083) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 24.4984
Epoch 0148 | Time 0.092 (0.083) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 24.4568
Epoch 0149 | Time 0.092 (0.083) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 24.4153
Epoch 0150 | Time 0.094 (0.083) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 24.3739
Epoch 0151 | Time 0.090 (0.083) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 24.3325
Epoch 0152 | Time 0.092 (0.084) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 24.2911
Epoch 0153 | Time 0.091 (0.084) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 24.2497
Epoch 0154 | Time 0.087 (0.084) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 24.2082
Epoch 0155 | Time 0.089 (0.084) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 24.1668
Epoch 0156 | Time 0.089 (0.084) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 24.1254
Epoch 0157 | Time 0.094 (0.084) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 24.0840
Epoch 0158 | Time 0.098 (0.084) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 24.0428
Epoch 0159 | Time 0.092 (0.084) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 24.0016
Epoch 0160 | Time 0.089 (0.084) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 23.9605
Epoch 0161 | Time 0.088 (0.084) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 23.9196
Epoch 0162 | Time 0.092 (0.084) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 23.8787
Epoch 0163 | Time 0.091 (0.084) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 23.8379
Epoch 0164 | Time 0.095 (0.084) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 23.7972
Epoch 0165 | Time 0.090 (0.084) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 23.7566
Epoch 0166 | Time 0.088 (0.084) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 23.7160
Epoch 0167 | Time 0.087 (0.085) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 23.6754
Epoch 0168 | Time 0.089 (0.085) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 23.6349
Epoch 0169 | Time 0.093 (0.085) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 23.5945
Epoch 0170 | Time 0.095 (0.085) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 23.5542
Epoch 0171 | Time 0.096 (0.085) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 23.5139
Epoch 0172 | Time 0.092 (0.085) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 23.4737
Epoch 0173 | Time 0.093 (0.085) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 23.4336
Epoch 0174 | Time 0.095 (0.085) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 23.3936
Epoch 0175 | Time 0.095 (0.085) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 23.3536
Epoch 0176 | Time 0.095 (0.085) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 23.3138
Epoch 0177 | Time 0.088 (0.085) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 23.2740
Epoch 0178 | Time 0.096 (0.085) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 23.2344
Epoch 0179 | Time 0.094 (0.086) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 23.1948
Epoch 0180 | Time 0.090 (0.086) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 23.1553
Epoch 0181 | Time 0.087 (0.086) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 23.1158
Epoch 0182 | Time 0.088 (0.086) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 23.0765
Epoch 0183 | Time 0.090 (0.086) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 23.0372
Epoch 0184 | Time 0.090 (0.086) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 22.9980
Epoch 0185 | Time 0.093 (0.086) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 22.9589
Epoch 0186 | Time 0.090 (0.086) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 22.9200
Epoch 0187 | Time 0.093 (0.086) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 22.8811
Epoch 0188 | Time 0.091 (0.086) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 22.8422
Epoch 0189 | Time 0.089 (0.086) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 22.8035
Epoch 0190 | Time 0.091 (0.086) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 22.7649
Epoch 0191 | Time 0.088 (0.086) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 22.7264
Epoch 0192 | Time 0.089 (0.086) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 22.6880
Epoch 0193 | Time 0.091 (0.086) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 22.6496
Epoch 0194 | Time 0.091 (0.086) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 22.6114
Epoch 0195 | Time 0.091 (0.086) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 22.5732
Epoch 0196 | Time 0.091 (0.086) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 22.5352
Epoch 0197 | Time 0.094 (0.086) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 22.4972
Epoch 0198 | Time 0.092 (0.086) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 22.4594
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))

        return nn.functional.tanh(out)


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.055 (0.055) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 35.5333
Epoch 0001 | Time 0.088 (0.055) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 34.3534
Epoch 0002 | Time 0.086 (0.056) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 34.8787
Epoch 0003 | Time 0.087 (0.056) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 35.5551
Epoch 0004 | Time 0.093 (0.056) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 35.6223
Epoch 0005 | Time 0.093 (0.057) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 35.1656
Epoch 0006 | Time 0.091 (0.057) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 34.5091
Epoch 0007 | Time 0.094 (0.057) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 33.9334
Epoch 0008 | Time 0.095 (0.058) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 33.5943
Epoch 0009 | Time 0.102 (0.058) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 33.5070
Epoch 0010 | Time 0.093 (0.059) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 33.5715
Epoch 0011 | Time 0.091 (0.059) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 33.6396
Epoch 0012 | Time 0.093 (0.059) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 33.5983
Epoch 0013 | Time 0.086 (0.060) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 33.4177
Epoch 0014 | Time 0.091 (0.060) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 33.1405
Epoch 0015 | Time 0.087 (0.060) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 32.8404
Epoch 0016 | Time 0.090 (0.060) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 32.5829
Epoch 0017 | Time 0.089 (0.061) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 32.4014
Epoch 0018 | Time 0.097 (0.061) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 32.2913
Epoch 0019 | Time 0.088 (0.061) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 32.2203
Epoch 0020 | Time 0.089 (0.062) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 32.1474
Epoch 0021 | Time 0.088 (0.062) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 32.0415
Epoch 0022 | Time 0.088 (0.062) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 31.8916
Epoch 0023 | Time 0.090 (0.062) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 31.7075
Epoch 0024 | Time 0.088 (0.063) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 31.5116
Epoch 0025 | Time 0.090 (0.063) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 31.3271
Epoch 0026 | Time 0.089 (0.063) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 31.1689
Epoch 0027 | Time 0.087 (0.063) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 31.0385
Epoch 0028 | Time 0.088 (0.064) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 30.9259
Epoch 0029 | Time 0.094 (0.064) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 30.8147
Epoch 0030 | Time 0.092 (0.064) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 30.6918
Epoch 0031 | Time 0.093 (0.065) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 30.5510
Epoch 0032 | Time 0.095 (0.065) | NFE-F 25.5 | NFE-B 0.0 | Train Loss 30.3957
Epoch 0033 | Time 0.095 (0.065) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 30.2354
Epoch 0034 | Time 0.095 (0.065) | NFE-F 25.8 | NFE-B 0.0 | Train Loss 30.0803
Epoch 0035 | Time 0.095 (0.066) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 29.9370
Epoch 0036 | Time 0.089 (0.066) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 29.8056
Epoch 0037 | Time 0.089 (0.066) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 29.6813
Epoch 0038 | Time 0.089 (0.066) | NFE-F 26.3 | NFE-B 0.0 | Train Loss 29.5570
Epoch 0039 | Time 0.089 (0.067) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 29.4270
Epoch 0040 | Time 0.098 (0.067) | NFE-F 26.6 | NFE-B 0.0 | Train Loss 29.2898
Epoch 0041 | Time 0.090 (0.067) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 29.1476
Epoch 0042 | Time 0.087 (0.067) | NFE-F 26.9 | NFE-B 0.0 | Train Loss 29.0050
Epoch 0043 | Time 0.089 (0.068) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 28.8662
Epoch 0044 | Time 0.095 (0.068) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 28.7333
Epoch 0045 | Time 0.089 (0.068) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 28.6054
Epoch 0046 | Time 0.089 (0.068) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 28.4797
Epoch 0047 | Time 0.091 (0.069) | NFE-F 27.5 | NFE-B 0.0 | Train Loss 28.3530
Epoch 0048 | Time 0.090 (0.069) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 28.2235
Epoch 0049 | Time 0.096 (0.069) | NFE-F 27.8 | NFE-B 0.0 | Train Loss 28.0916
Epoch 0050 | Time 0.088 (0.069) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 27.9591
Epoch 0051 | Time 0.092 (0.069) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 27.8280
Epoch 0052 | Time 0.089 (0.070) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 27.6998
Epoch 0053 | Time 0.091 (0.070) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 27.5745
Epoch 0054 | Time 0.091 (0.070) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 27.4511
Epoch 0055 | Time 0.092 (0.070) | NFE-F 28.5 | NFE-B 0.0 | Train Loss 27.3279
Epoch 0056 | Time 0.089 (0.071) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 27.2040
Epoch 0057 | Time 0.088 (0.071) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 27.0793
Epoch 0058 | Time 0.090 (0.071) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 26.9545
Epoch 0059 | Time 0.092 (0.071) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 26.8308
Epoch 0060 | Time 0.095 (0.071) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 26.7088
Epoch 0061 | Time 0.093 (0.072) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 26.5885
Epoch 0062 | Time 0.101 (0.072) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 26.4696
Epoch 0063 | Time 0.095 (0.072) | NFE-F 29.4 | NFE-B 0.0 | Train Loss 26.3512
Epoch 0064 | Time 0.095 (0.072) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 26.2329
Epoch 0065 | Time 0.091 (0.073) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 26.1147
Epoch 0066 | Time 0.086 (0.073) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 25.9970
Epoch 0067 | Time 0.094 (0.073) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 25.8801
Epoch 0068 | Time 0.090 (0.073) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 25.7646
Epoch 0069 | Time 0.088 (0.073) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 25.6504
Epoch 0070 | Time 0.091 (0.073) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 25.5370
Epoch 0071 | Time 0.093 (0.074) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 25.4244
Epoch 0072 | Time 0.091 (0.074) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 25.3121
Epoch 0073 | Time 0.089 (0.074) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 25.2004
Epoch 0074 | Time 0.088 (0.074) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 25.0894
Epoch 0075 | Time 0.089 (0.074) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 24.9793
Epoch 0076 | Time 0.097 (0.074) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 24.8703
Epoch 0077 | Time 0.091 (0.075) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 24.7622
Epoch 0078 | Time 0.087 (0.075) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 24.6551
Epoch 0079 | Time 0.090 (0.075) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 24.5485
Epoch 0080 | Time 0.089 (0.075) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 24.4427
Epoch 0081 | Time 0.089 (0.075) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 24.3375
Epoch 0082 | Time 0.088 (0.075) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 24.2332
Epoch 0083 | Time 0.090 (0.075) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 24.1298
Epoch 0084 | Time 0.099 (0.076) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 24.0273
Epoch 0085 | Time 0.093 (0.076) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 23.9256
Epoch 0086 | Time 0.093 (0.076) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 23.8247
Epoch 0087 | Time 0.087 (0.076) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 23.7246
Epoch 0088 | Time 0.088 (0.076) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 23.6252
Epoch 0089 | Time 0.090 (0.076) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 23.5266
Epoch 0090 | Time 0.085 (0.076) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 23.4288
Epoch 0091 | Time 0.090 (0.077) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 23.3318
Epoch 0092 | Time 0.089 (0.077) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 23.2357
Epoch 0093 | Time 0.088 (0.077) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 23.1404
Epoch 0094 | Time 0.093 (0.077) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 23.0458
Epoch 0095 | Time 0.091 (0.077) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 22.9519
Epoch 0096 | Time 0.092 (0.077) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 22.8589
Epoch 0097 | Time 0.087 (0.077) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 22.7666
Epoch 0098 | Time 0.089 (0.077) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 22.6751
Epoch 0099 | Time 0.090 (0.078) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 22.5843
Epoch 0100 | Time 0.089 (0.078) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 22.4943
Epoch 0101 | Time 0.086 (0.078) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 22.4049
Epoch 0102 | Time 0.087 (0.078) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 22.3163
Epoch 0103 | Time 0.089 (0.078) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 22.2283
Epoch 0104 | Time 0.092 (0.078) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 22.1410
Epoch 0105 | Time 0.087 (0.078) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 22.0544
Epoch 0106 | Time 0.094 (0.078) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 21.9684
Epoch 0107 | Time 0.090 (0.078) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 21.8830
Epoch 0108 | Time 0.095 (0.079) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 21.7981
Epoch 0109 | Time 0.088 (0.079) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 21.7138
Epoch 0110 | Time 0.091 (0.079) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 21.6300
Epoch 0111 | Time 0.089 (0.079) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 21.5467
Epoch 0112 | Time 0.088 (0.079) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 21.4639
Epoch 0113 | Time 0.089 (0.079) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 21.3815
Epoch 0114 | Time 0.089 (0.079) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 21.2995
Epoch 0115 | Time 0.089 (0.079) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 21.2180
Epoch 0116 | Time 0.085 (0.079) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 21.1369
Epoch 0117 | Time 0.088 (0.079) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 21.0565
Epoch 0118 | Time 0.092 (0.080) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 20.9767
Epoch 0119 | Time 0.090 (0.080) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 20.8977
Epoch 0120 | Time 0.089 (0.080) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 20.8196
Epoch 0121 | Time 0.090 (0.080) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 20.7426
Epoch 0122 | Time 0.089 (0.080) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 20.6667
Epoch 0123 | Time 0.087 (0.080) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 20.5919
Epoch 0124 | Time 0.087 (0.080) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 20.5183
Epoch 0125 | Time 0.094 (0.080) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 20.4457
Epoch 0126 | Time 0.095 (0.080) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 20.3743
Epoch 0127 | Time 0.095 (0.081) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 20.3039
Epoch 0128 | Time 0.092 (0.081) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 20.2345
Epoch 0129 | Time 0.089 (0.081) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 20.1660
Epoch 0130 | Time 0.089 (0.081) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 20.0984
Epoch 0131 | Time 0.087 (0.081) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 20.0316
Epoch 0132 | Time 0.088 (0.081) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 19.9657
Epoch 0133 | Time 0.090 (0.081) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 19.9004
Epoch 0134 | Time 0.089 (0.081) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 19.8360
Epoch 0135 | Time 0.085 (0.081) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 19.7722
Epoch 0136 | Time 0.087 (0.081) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 19.7092
Epoch 0137 | Time 0.089 (0.081) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 19.6469
Epoch 0138 | Time 0.091 (0.081) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 19.5852
Epoch 0139 | Time 0.088 (0.081) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 19.5242
Epoch 0140 | Time 0.090 (0.082) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 19.4639
Epoch 0141 | Time 0.091 (0.082) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 19.4042
Epoch 0142 | Time 0.089 (0.082) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 19.3451
Epoch 0143 | Time 0.092 (0.082) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 19.2866
Epoch 0144 | Time 0.085 (0.082) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 19.2288
Epoch 0145 | Time 0.087 (0.082) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 19.1716
Epoch 0146 | Time 0.093 (0.082) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 19.1150
Epoch 0147 | Time 0.090 (0.082) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 19.0590
Epoch 0148 | Time 0.090 (0.082) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 19.0036
Epoch 0149 | Time 0.095 (0.082) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 18.9488
Epoch 0150 | Time 0.095 (0.082) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 18.8946
Epoch 0151 | Time 0.099 (0.083) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 18.8409
Epoch 0152 | Time 0.092 (0.083) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 18.7878
Epoch 0153 | Time 0.087 (0.083) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 18.7353
Epoch 0154 | Time 0.085 (0.083) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 18.6833
Epoch 0155 | Time 0.086 (0.083) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 18.6318
Epoch 0156 | Time 0.086 (0.083) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 18.5809
Epoch 0157 | Time 0.088 (0.083) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 18.5305
Epoch 0158 | Time 0.087 (0.083) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 18.4807
Epoch 0159 | Time 0.089 (0.083) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 18.4313
Epoch 0160 | Time 0.089 (0.083) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 18.3825
Epoch 0161 | Time 0.088 (0.083) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 18.3342
Epoch 0162 | Time 0.091 (0.083) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 18.2863
Epoch 0163 | Time 0.091 (0.083) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 18.2390
Epoch 0164 | Time 0.085 (0.083) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 18.1921
Epoch 0165 | Time 0.088 (0.083) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 18.1457
Epoch 0166 | Time 0.087 (0.083) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 18.0997
Epoch 0167 | Time 0.088 (0.083) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 18.0542
Epoch 0168 | Time 0.088 (0.083) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 18.0091
Epoch 0169 | Time 0.089 (0.084) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 17.9645
Epoch 0170 | Time 0.088 (0.084) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 17.9203
Epoch 0171 | Time 0.090 (0.084) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 17.8764
Epoch 0172 | Time 0.093 (0.084) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 17.8330
Epoch 0173 | Time 0.096 (0.084) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 17.7900
Epoch 0174 | Time 0.087 (0.084) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 17.7473
Epoch 0175 | Time 0.090 (0.084) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 17.7051
Epoch 0176 | Time 0.095 (0.084) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 17.6632
Epoch 0177 | Time 0.091 (0.084) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 17.6217
Epoch 0178 | Time 0.092 (0.084) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 17.5806
Epoch 0179 | Time 0.089 (0.084) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 17.5399
Epoch 0180 | Time 0.088 (0.084) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 17.4997
Epoch 0181 | Time 0.091 (0.084) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 17.4598
Epoch 0182 | Time 0.088 (0.084) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 17.4204
Epoch 0183 | Time 0.096 (0.085) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 17.3814
Epoch 0184 | Time 0.101 (0.085) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 17.3429
Epoch 0185 | Time 0.092 (0.085) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 17.3048
Epoch 0186 | Time 0.095 (0.085) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 17.2671
Epoch 0187 | Time 0.093 (0.085) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 17.2298
Epoch 0188 | Time 0.093 (0.085) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 17.1930
Epoch 0189 | Time 0.091 (0.085) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 17.1565
Epoch 0190 | Time 0.086 (0.085) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 17.1205
Epoch 0191 | Time 0.090 (0.085) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 17.0849
Epoch 0192 | Time 0.091 (0.085) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 17.0496
Epoch 0193 | Time 0.091 (0.085) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 17.0147
Epoch 0194 | Time 0.090 (0.085) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 16.9802
Epoch 0195 | Time 0.093 (0.085) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 16.9460
Epoch 0196 | Time 0.092 (0.085) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 16.9122
Epoch 0197 | Time 0.088 (0.085) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 16.8788
Epoch 0198 | Time 0.089 (0.085) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 16.8456
Epoch 0199 | Time 0.089 (0.086) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 16.8129
Epoch 0200 | Time 0.091 (0.086) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 16.7804
Epoch 0201 | Time 0.085 (0.086) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 16.7483
Epoch 0202 | Time 0.089 (0.086) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 16.7165
Epoch 0203 | Time 0.089 (0.086) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 16.6850
Epoch 0204 | Time 0.089 (0.086) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 16.6538
Epoch 0205 | Time 0.091 (0.086) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 16.6230
Epoch 0206 | Time 0.090 (0.086) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 16.5924
Epoch 0207 | Time 0.086 (0.086) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 16.5622
Epoch 0208 | Time 0.087 (0.086) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 16.5322
Epoch 0209 | Time 0.093 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 16.5025
Epoch 0210 | Time 0.094 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 16.4731
Epoch 0211 | Time 0.092 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 16.4440
Epoch 0212 | Time 0.093 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 16.4152
Epoch 0213 | Time 0.091 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 16.3867
Epoch 0214 | Time 0.093 (0.086) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 16.3584
Epoch 0215 | Time 0.087 (0.086) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 16.3304
Epoch 0216 | Time 0.092 (0.086) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 16.3027
Epoch 0217 | Time 0.089 (0.086) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 16.2752
Epoch 0218 | Time 0.092 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 16.2480
Epoch 0219 | Time 0.094 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 16.2211
Epoch 0220 | Time 0.091 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 16.1944
Epoch 0221 | Time 0.089 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 16.1679
Epoch 0222 | Time 0.089 (0.086) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 16.1417
Epoch 0223 | Time 0.094 (0.087) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 16.1157
Epoch 0224 | Time 0.090 (0.087) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 16.0900
Epoch 0225 | Time 0.087 (0.087) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 16.0645
Epoch 0226 | Time 0.092 (0.087) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 16.0392
Epoch 0227 | Time 0.090 (0.087) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 16.0142
Epoch 0228 | Time 0.097 (0.087) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 15.9894
Epoch 0229 | Time 0.087 (0.087) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 15.9648
Epoch 0230 | Time 0.089 (0.087) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 15.9405
Epoch 0231 | Time 0.095 (0.087) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 15.9163
Epoch 0232 | Time 0.088 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 15.8924
Epoch 0233 | Time 0.094 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 15.8687
Epoch 0234 | Time 0.092 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 15.8451
Epoch 0235 | Time 0.087 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 15.8218
Epoch 0236 | Time 0.091 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 15.7987
Epoch 0237 | Time 0.092 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 15.7758
Epoch 0238 | Time 0.090 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 15.7531
Epoch 0239 | Time 0.098 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 15.7306
Epoch 0240 | Time 0.090 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 15.7083
Epoch 0241 | Time 0.094 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 15.6862
Epoch 0242 | Time 0.089 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 15.6642
Epoch 0243 | Time 0.086 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 15.6425
Epoch 0244 | Time 0.087 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 15.6209
Epoch 0245 | Time 0.091 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 15.5995
Epoch 0246 | Time 0.090 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 15.5783
Epoch 0247 | Time 0.091 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 15.5572
Epoch 0248 | Time 0.090 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 15.5364
Epoch 0249 | Time 0.089 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 15.5157
Epoch 0250 | Time 0.086 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 15.4951
Epoch 0251 | Time 0.090 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 15.4748
Epoch 0252 | Time 0.093 (0.088) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 15.4545
Epoch 0253 | Time 0.091 (0.088) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 15.4345
Epoch 0254 | Time 0.093 (0.088) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 15.4146
Epoch 0255 | Time 0.091 (0.088) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 15.3949
Epoch 0256 | Time 0.087 (0.088) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 15.3753
Epoch 0257 | Time 0.089 (0.088) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 15.3559
Epoch 0258 | Time 0.092 (0.088) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 15.3366
Epoch 0259 | Time 0.089 (0.088) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 15.3175
Epoch 0260 | Time 0.088 (0.088) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 15.2985
Epoch 0261 | Time 0.089 (0.088) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 15.2797
Epoch 0262 | Time 0.094 (0.088) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 15.2610
Epoch 0263 | Time 0.082 (0.088) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 15.2424
Epoch 0264 | Time 0.088 (0.088) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 15.2240
Epoch 0265 | Time 0.088 (0.088) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 15.2057
Epoch 0266 | Time 0.090 (0.088) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 15.1876
Epoch 0267 | Time 0.087 (0.088) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 15.1696
Epoch 0268 | Time 0.085 (0.088) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 15.1517
Epoch 0269 | Time 0.087 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 15.1340
Epoch 0270 | Time 0.086 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 15.1163
Epoch 0271 | Time 0.086 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 15.0989
Epoch 0272 | Time 0.092 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 15.0815
Epoch 0273 | Time 0.089 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 15.0643
Epoch 0274 | Time 0.089 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 15.0471
Epoch 0275 | Time 0.087 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 15.0301
Epoch 0276 | Time 0.087 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 15.0132
Epoch 0277 | Time 0.085 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 14.9965
Epoch 0278 | Time 0.092 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 14.9798
Epoch 0279 | Time 0.085 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 14.9633
Epoch 0280 | Time 0.091 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 14.9468
Epoch 0281 | Time 0.091 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 14.9305
Epoch 0282 | Time 0.089 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 14.9143
Epoch 0283 | Time 0.094 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 14.8982
Epoch 0284 | Time 0.096 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 14.8822
Epoch 0285 | Time 0.092 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.8663
Epoch 0286 | Time 0.092 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.8506
Epoch 0287 | Time 0.085 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.8349
Epoch 0288 | Time 0.088 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.8193
Epoch 0289 | Time 0.104 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.8038
Epoch 0290 | Time 0.097 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.7884
Epoch 0291 | Time 0.091 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.7732
Epoch 0292 | Time 0.094 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.7580
Epoch 0293 | Time 0.116 (0.089) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 14.7429
Epoch 0294 | Time 0.122 (0.089) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.7279
Epoch 0295 | Time 0.119 (0.089) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.7130
Epoch 0296 | Time 0.114 (0.089) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.6982
Epoch 0297 | Time 0.103 (0.090) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.6835
Epoch 0298 | Time 0.116 (0.090) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.6688
Epoch 0299 | Time 0.101 (0.090) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.6543
Epoch 0300 | Time 0.120 (0.090) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.6398
Epoch 0301 | Time 0.114 (0.091) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.6255
Epoch 0302 | Time 0.095 (0.091) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.6112
Epoch 0303 | Time 0.102 (0.091) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 14.5970
Epoch 0304 | Time 0.133 (0.091) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.5828
Epoch 0305 | Time 0.101 (0.091) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.5688
Epoch 0306 | Time 0.095 (0.091) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.5549
Epoch 0307 | Time 0.133 (0.092) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.5410
Epoch 0308 | Time 0.108 (0.092) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.5272
Epoch 0309 | Time 0.093 (0.092) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.5135
Epoch 0310 | Time 0.115 (0.092) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.4998
Epoch 0311 | Time 0.136 (0.093) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.4863
Epoch 0312 | Time 0.102 (0.093) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.4728
Epoch 0313 | Time 0.096 (0.093) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.4594
Epoch 0314 | Time 0.108 (0.093) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 14.4461
Epoch 0315 | Time 0.120 (0.093) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.4328
Epoch 0316 | Time 0.093 (0.093) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.4196
Epoch 0317 | Time 0.099 (0.093) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.4065
Epoch 0318 | Time 0.133 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.3935
Epoch 0319 | Time 0.111 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.3805
Epoch 0320 | Time 0.096 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.3676
Epoch 0321 | Time 0.093 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.3547
Epoch 0322 | Time 0.099 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.3420
Epoch 0323 | Time 0.133 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.3293
Epoch 0324 | Time 0.103 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.3166
Epoch 0325 | Time 0.090 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.3041
Epoch 0326 | Time 0.108 (0.094) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 14.2916
Epoch 0327 | Time 0.118 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.2791
Epoch 0328 | Time 0.107 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.2667
Epoch 0329 | Time 0.089 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.2544
Epoch 0330 | Time 0.094 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.2422
Epoch 0331 | Time 0.090 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.2300
Epoch 0332 | Time 0.095 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.2179
Epoch 0333 | Time 0.092 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.2058
Epoch 0334 | Time 0.090 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.1938
Epoch 0335 | Time 0.089 (0.094) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.1818
Epoch 0336 | Time 0.092 (0.094) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.1700
Epoch 0337 | Time 0.108 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.1581
Epoch 0338 | Time 0.125 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.1464
Epoch 0339 | Time 0.090 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.1346
Epoch 0340 | Time 0.090 (0.095) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 14.1230
Epoch 0341 | Time 0.095 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.1114
Epoch 0342 | Time 0.094 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0998
Epoch 0343 | Time 0.096 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0883
Epoch 0344 | Time 0.098 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0769
Epoch 0345 | Time 0.092 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0655
Epoch 0346 | Time 0.087 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0542
Epoch 0347 | Time 0.089 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0429
Epoch 0348 | Time 0.107 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0317
Epoch 0349 | Time 0.097 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0205
Epoch 0350 | Time 0.112 (0.095) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 14.0094
Epoch 0351 | Time 0.147 (0.096) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 13.9983
Epoch 0352 | Time 0.110 (0.096) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 13.9873
Epoch 0353 | Time 0.103 (0.096) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 13.9764
Epoch 0354 | Time 0.122 (0.096) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 13.9654
Epoch 0355 | Time 0.143 (0.096) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 13.9546
Epoch 0356 | Time 0.093 (0.096) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 13.9438
Epoch 0357 | Time 0.102 (0.096) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 13.9330
Epoch 0358 | Time 0.090 (0.096) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.9223
Epoch 0359 | Time 0.090 (0.096) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.9116
Epoch 0360 | Time 0.095 (0.096) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.9010
Epoch 0361 | Time 0.104 (0.096) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8904
Epoch 0362 | Time 0.105 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8798
Epoch 0363 | Time 0.108 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8693
Epoch 0364 | Time 0.128 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8589
Epoch 0365 | Time 0.110 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8485
Epoch 0366 | Time 0.103 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8382
Epoch 0367 | Time 0.102 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8278
Epoch 0368 | Time 0.100 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8176
Epoch 0369 | Time 0.095 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.8074
Epoch 0370 | Time 0.092 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.7972
Epoch 0371 | Time 0.095 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.7871
Epoch 0372 | Time 0.090 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.7770
Epoch 0373 | Time 0.094 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.7669
Epoch 0374 | Time 0.096 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.7569
Epoch 0375 | Time 0.092 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.7469
Epoch 0376 | Time 0.091 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.7370
Epoch 0377 | Time 0.091 (0.097) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 13.7271
Epoch 0378 | Time 0.094 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.7173
Epoch 0379 | Time 0.093 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.7075
Epoch 0380 | Time 0.094 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6977
Epoch 0381 | Time 0.102 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6880
Epoch 0382 | Time 0.094 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6783
Epoch 0383 | Time 0.091 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6687
Epoch 0384 | Time 0.089 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6591
Epoch 0385 | Time 0.095 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6495
Epoch 0386 | Time 0.092 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6400
Epoch 0387 | Time 0.093 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6305
Epoch 0388 | Time 0.097 (0.097) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6211
Epoch 0389 | Time 0.092 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6117
Epoch 0390 | Time 0.090 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.6023
Epoch 0391 | Time 0.096 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5930
Epoch 0392 | Time 0.101 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5837
Epoch 0393 | Time 0.091 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5744
Epoch 0394 | Time 0.091 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5652
Epoch 0395 | Time 0.088 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5560
Epoch 0396 | Time 0.088 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5468
Epoch 0397 | Time 0.093 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5377
Epoch 0398 | Time 0.091 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5287
Epoch 0399 | Time 0.091 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5196
Epoch 0400 | Time 0.104 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5106
Epoch 0401 | Time 0.092 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.5016
Epoch 0402 | Time 0.091 (0.096) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 13.4927
Epoch 0403 | Time 0.093 (0.096) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4838
Epoch 0404 | Time 0.125 (0.096) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4749
Epoch 0405 | Time 0.178 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4661
Epoch 0406 | Time 0.135 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4572
Epoch 0407 | Time 0.102 (0.098) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4485
Epoch 0408 | Time 0.105 (0.098) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4397
Epoch 0409 | Time 0.093 (0.098) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4310
Epoch 0410 | Time 0.092 (0.098) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4223
Epoch 0411 | Time 0.092 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4137
Epoch 0412 | Time 0.103 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.4051
Epoch 0413 | Time 0.088 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3965
Epoch 0414 | Time 0.103 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3880
Epoch 0415 | Time 0.095 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3795
Epoch 0416 | Time 0.094 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3710
Epoch 0417 | Time 0.094 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3625
Epoch 0418 | Time 0.108 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3541
Epoch 0419 | Time 0.104 (0.098) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3457
Epoch 0420 | Time 0.093 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3373
Epoch 0421 | Time 0.089 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3290
Epoch 0422 | Time 0.098 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3207
Epoch 0423 | Time 0.093 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3124
Epoch 0424 | Time 0.090 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.3042
Epoch 0425 | Time 0.093 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2960
Epoch 0426 | Time 0.093 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2878
Epoch 0427 | Time 0.093 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2796
Epoch 0428 | Time 0.094 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2715
Epoch 0429 | Time 0.087 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2634
Epoch 0430 | Time 0.099 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2554
Epoch 0431 | Time 0.097 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2473
Epoch 0432 | Time 0.096 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2393
Epoch 0433 | Time 0.099 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2313
Epoch 0434 | Time 0.092 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2234
Epoch 0435 | Time 0.093 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2154
Epoch 0436 | Time 0.092 (0.097) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 13.2075
Epoch 0437 | Time 0.093 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1996
Epoch 0438 | Time 0.095 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1918
Epoch 0439 | Time 0.090 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1840
Epoch 0440 | Time 0.089 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1762
Epoch 0441 | Time 0.087 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1684
Epoch 0442 | Time 0.086 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1606
Epoch 0443 | Time 0.097 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1529
Epoch 0444 | Time 0.101 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1452
Epoch 0445 | Time 0.102 (0.097) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1375
Epoch 0446 | Time 0.185 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1299
Epoch 0447 | Time 0.145 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1223
Epoch 0448 | Time 0.131 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1147
Epoch 0449 | Time 0.109 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.1071
Epoch 0450 | Time 0.106 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0995
Epoch 0451 | Time 0.101 (0.099) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0920
Epoch 0452 | Time 0.101 (0.099) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0845
Epoch 0453 | Time 0.090 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0770
Epoch 0454 | Time 0.098 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0695
Epoch 0455 | Time 0.087 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0621
Epoch 0456 | Time 0.087 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0547
Epoch 0457 | Time 0.098 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0472
Epoch 0458 | Time 0.107 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0399
Epoch 0459 | Time 0.092 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0325
Epoch 0460 | Time 0.094 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0252
Epoch 0461 | Time 0.087 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0178
Epoch 0462 | Time 0.101 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0105
Epoch 0463 | Time 0.099 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 13.0033
Epoch 0464 | Time 0.087 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9960
Epoch 0465 | Time 0.150 (0.099) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9888
Epoch 0466 | Time 0.107 (0.099) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9816
Epoch 0467 | Time 0.091 (0.099) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9744
Epoch 0468 | Time 0.095 (0.099) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9672
Epoch 0469 | Time 0.090 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9601
Epoch 0470 | Time 0.096 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9530
Epoch 0471 | Time 0.092 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9459
Epoch 0472 | Time 0.090 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9388
Epoch 0473 | Time 0.090 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9318
Epoch 0474 | Time 0.103 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9248
Epoch 0475 | Time 0.109 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9178
Epoch 0476 | Time 0.113 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9108
Epoch 0477 | Time 0.085 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.9039
Epoch 0478 | Time 0.088 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8970
Epoch 0479 | Time 0.087 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8901
Epoch 0480 | Time 0.093 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8833
Epoch 0481 | Time 0.090 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8765
Epoch 0482 | Time 0.087 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8696
Epoch 0483 | Time 0.093 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8629
Epoch 0484 | Time 0.087 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8561
Epoch 0485 | Time 0.093 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8494
Epoch 0486 | Time 0.094 (0.098) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 12.8427
Epoch 0487 | Time 0.093 (0.098) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.8360
Epoch 0488 | Time 0.093 (0.098) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.8294
Epoch 0489 | Time 0.093 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.8227
Epoch 0490 | Time 0.091 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.8161
Epoch 0491 | Time 0.099 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.8095
Epoch 0492 | Time 0.098 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.8030
Epoch 0493 | Time 0.093 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7964
Epoch 0494 | Time 0.095 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7899
Epoch 0495 | Time 0.094 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7834
Epoch 0496 | Time 0.093 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7769
Epoch 0497 | Time 0.095 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7704
Epoch 0498 | Time 0.095 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7640
Epoch 0499 | Time 0.087 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7576
Epoch 0500 | Time 0.093 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7512
Epoch 0501 | Time 0.094 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7448
Epoch 0502 | Time 0.090 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7385
Epoch 0503 | Time 0.093 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7321
Epoch 0504 | Time 0.091 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7258
Epoch 0505 | Time 0.094 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7195
Epoch 0506 | Time 0.091 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7132
Epoch 0507 | Time 0.088 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7070
Epoch 0508 | Time 0.091 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.7007
Epoch 0509 | Time 0.090 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6945
Epoch 0510 | Time 0.093 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6883
Epoch 0511 | Time 0.093 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6821
Epoch 0512 | Time 0.086 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6759
Epoch 0513 | Time 0.088 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6698
Epoch 0514 | Time 0.090 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6636
Epoch 0515 | Time 0.087 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6575
Epoch 0516 | Time 0.104 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6514
Epoch 0517 | Time 0.096 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6453
Epoch 0518 | Time 0.092 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6392
Epoch 0519 | Time 0.090 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6332
Epoch 0520 | Time 0.105 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6271
Epoch 0521 | Time 0.099 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6211
Epoch 0522 | Time 0.099 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6151
Epoch 0523 | Time 0.096 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6091
Epoch 0524 | Time 0.114 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.6031
Epoch 0525 | Time 0.096 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.5972
Epoch 0526 | Time 0.095 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.5912
Epoch 0527 | Time 0.088 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.5853
Epoch 0528 | Time 0.093 (0.096) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.5793
Epoch 0529 | Time 0.155 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.5734
Epoch 0530 | Time 0.134 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.5675
Epoch 0531 | Time 0.100 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.5616
Epoch 0532 | Time 0.094 (0.097) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 12.5557
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.133 (0.133) | NFE-F 86.0 | NFE-B 0.0 | Train Loss 5132.6318
Epoch 0001 | Time 0.211 (0.134) | NFE-F 87.0 | NFE-B 0.0 | Train Loss 4981.5293
Epoch 0002 | Time 0.315 (0.136) | NFE-F 88.9 | NFE-B 0.0 | Train Loss 4865.7192
Epoch 0003 | Time 0.315 (0.137) | NFE-F 91.2 | NFE-B 0.0 | Train Loss 4751.7168
Epoch 0004 | Time 0.173 (0.138) | NFE-F 91.4 | NFE-B 0.0 | Train Loss 4641.0220
Epoch 0005 | Time 0.148 (0.138) | NFE-F 91.5 | NFE-B 0.0 | Train Loss 4533.9258
Epoch 0006 | Time 0.096 (0.137) | NFE-F 91.0 | NFE-B 0.0 | Train Loss 4430.5386
Epoch 0007 | Time 0.096 (0.137) | NFE-F 90.6 | NFE-B 0.0 | Train Loss 4328.5723
Epoch 0008 | Time 0.091 (0.137) | NFE-F 90.1 | NFE-B 0.0 | Train Loss 4228.0459
Epoch 0009 | Time 0.095 (0.136) | NFE-F 89.6 | NFE-B 0.0 | Train Loss 4128.9502
Epoch 0010 | Time 0.092 (0.136) | NFE-F 89.1 | NFE-B 0.0 | Train Loss 4031.3613
Epoch 0011 | Time 0.086 (0.135) | NFE-F 88.6 | NFE-B 0.0 | Train Loss 3935.2493
Epoch 0012 | Time 0.088 (0.135) | NFE-F 88.1 | NFE-B 0.0 | Train Loss 3840.5535
Epoch 0013 | Time 0.085 (0.134) | NFE-F 87.6 | NFE-B 0.0 | Train Loss 3747.7039
Epoch 0014 | Time 0.085 (0.134) | NFE-F 87.1 | NFE-B 0.0 | Train Loss 3656.1624
Epoch 0015 | Time 0.087 (0.133) | NFE-F 86.7 | NFE-B 0.0 | Train Loss 3565.6748
Epoch 0016 | Time 0.088 (0.133) | NFE-F 86.2 | NFE-B 0.0 | Train Loss 3477.1562
Epoch 0017 | Time 0.086 (0.132) | NFE-F 85.7 | NFE-B 0.0 | Train Loss 3390.0735
Epoch 0018 | Time 0.085 (0.132) | NFE-F 85.3 | NFE-B 0.0 | Train Loss 3305.0513
Epoch 0019 | Time 0.083 (0.131) | NFE-F 84.8 | NFE-B 0.0 | Train Loss 3221.1975
Epoch 0020 | Time 0.090 (0.131) | NFE-F 84.4 | NFE-B 0.0 | Train Loss 3138.7119
Epoch 0021 | Time 0.085 (0.130) | NFE-F 83.9 | NFE-B 0.0 | Train Loss 3057.3625
Epoch 0022 | Time 0.084 (0.130) | NFE-F 83.5 | NFE-B 0.0 | Train Loss 2978.0896
Epoch 0023 | Time 0.084 (0.130) | NFE-F 83.1 | NFE-B 0.0 | Train Loss 2900.5442
Epoch 0024 | Time 0.107 (0.129) | NFE-F 82.9 | NFE-B 0.0 | Train Loss 2824.2559
Epoch 0025 | Time 0.087 (0.129) | NFE-F 82.4 | NFE-B 0.0 | Train Loss 2749.5420
Epoch 0026 | Time 0.083 (0.128) | NFE-F 82.0 | NFE-B 0.0 | Train Loss 2676.3511
Epoch 0027 | Time 0.089 (0.128) | NFE-F 81.6 | NFE-B 0.0 | Train Loss 2604.6775
Epoch 0028 | Time 0.091 (0.128) | NFE-F 81.2 | NFE-B 0.0 | Train Loss 2534.5115
Epoch 0029 | Time 0.086 (0.127) | NFE-F 80.8 | NFE-B 0.0 | Train Loss 2465.8455
Epoch 0030 | Time 0.084 (0.127) | NFE-F 80.4 | NFE-B 0.0 | Train Loss 2398.6697
Epoch 0031 | Time 0.085 (0.126) | NFE-F 80.0 | NFE-B 0.0 | Train Loss 2332.9731
Epoch 0032 | Time 0.087 (0.126) | NFE-F 79.6 | NFE-B 0.0 | Train Loss 2268.7437
Epoch 0033 | Time 0.086 (0.126) | NFE-F 79.2 | NFE-B 0.0 | Train Loss 2205.9697
Epoch 0034 | Time 0.088 (0.125) | NFE-F 78.8 | NFE-B 0.0 | Train Loss 2144.6377
Epoch 0035 | Time 0.086 (0.125) | NFE-F 78.4 | NFE-B 0.0 | Train Loss 2084.7332
Epoch 0036 | Time 0.089 (0.124) | NFE-F 78.0 | NFE-B 0.0 | Train Loss 2026.2421
Epoch 0037 | Time 0.084 (0.124) | NFE-F 77.6 | NFE-B 0.0 | Train Loss 1969.1497
Epoch 0038 | Time 0.087 (0.124) | NFE-F 77.2 | NFE-B 0.0 | Train Loss 1913.4392
Epoch 0039 | Time 0.087 (0.123) | NFE-F 76.9 | NFE-B 0.0 | Train Loss 1859.0951
Epoch 0040 | Time 0.089 (0.123) | NFE-F 76.5 | NFE-B 0.0 | Train Loss 1806.0999
Epoch 0041 | Time 0.087 (0.123) | NFE-F 76.1 | NFE-B 0.0 | Train Loss 1754.4362
Epoch 0042 | Time 0.086 (0.122) | NFE-F 75.8 | NFE-B 0.0 | Train Loss 1704.0868
Epoch 0043 | Time 0.089 (0.122) | NFE-F 75.4 | NFE-B 0.0 | Train Loss 1655.0330
Epoch 0044 | Time 0.087 (0.122) | NFE-F 75.1 | NFE-B 0.0 | Train Loss 1607.2561
Epoch 0045 | Time 0.083 (0.121) | NFE-F 74.7 | NFE-B 0.0 | Train Loss 1560.7372
Epoch 0046 | Time 0.086 (0.121) | NFE-F 74.4 | NFE-B 0.0 | Train Loss 1515.4581
Epoch 0047 | Time 0.084 (0.120) | NFE-F 74.0 | NFE-B 0.0 | Train Loss 1471.3984
Epoch 0048 | Time 0.083 (0.120) | NFE-F 73.7 | NFE-B 0.0 | Train Loss 1428.5386
Epoch 0049 | Time 0.085 (0.120) | NFE-F 73.3 | NFE-B 0.0 | Train Loss 1386.8590
Epoch 0050 | Time 0.086 (0.119) | NFE-F 73.0 | NFE-B 0.0 | Train Loss 1346.3390
Epoch 0051 | Time 0.086 (0.119) | NFE-F 72.7 | NFE-B 0.0 | Train Loss 1306.9591
Epoch 0052 | Time 0.090 (0.119) | NFE-F 72.4 | NFE-B 0.0 | Train Loss 1268.6984
Epoch 0053 | Time 0.094 (0.119) | NFE-F 72.0 | NFE-B 0.0 | Train Loss 1231.5370
Epoch 0054 | Time 0.088 (0.118) | NFE-F 71.7 | NFE-B 0.0 | Train Loss 1195.4542
Epoch 0055 | Time 0.087 (0.118) | NFE-F 71.4 | NFE-B 0.0 | Train Loss 1160.4291
Epoch 0056 | Time 0.083 (0.118) | NFE-F 71.1 | NFE-B 0.0 | Train Loss 1126.4414
Epoch 0057 | Time 0.085 (0.117) | NFE-F 70.8 | NFE-B 0.0 | Train Loss 1093.4705
Epoch 0058 | Time 0.082 (0.117) | NFE-F 70.5 | NFE-B 0.0 | Train Loss 1061.4961
Epoch 0059 | Time 0.085 (0.117) | NFE-F 70.2 | NFE-B 0.0 | Train Loss 1030.4973
Epoch 0060 | Time 0.085 (0.116) | NFE-F 69.9 | NFE-B 0.0 | Train Loss 1000.4536
Epoch 0061 | Time 0.083 (0.116) | NFE-F 69.6 | NFE-B 0.0 | Train Loss 971.3448
Epoch 0062 | Time 0.087 (0.116) | NFE-F 69.3 | NFE-B 0.0 | Train Loss 943.1501
Epoch 0063 | Time 0.087 (0.115) | NFE-F 69.0 | NFE-B 0.0 | Train Loss 915.8499
Epoch 0064 | Time 0.085 (0.115) | NFE-F 68.7 | NFE-B 0.0 | Train Loss 889.4236
Epoch 0065 | Time 0.085 (0.115) | NFE-F 68.4 | NFE-B 0.0 | Train Loss 863.8513
Epoch 0066 | Time 0.095 (0.115) | NFE-F 68.1 | NFE-B 0.0 | Train Loss 839.1132
Epoch 0067 | Time 0.086 (0.114) | NFE-F 67.8 | NFE-B 0.0 | Train Loss 815.1893
Epoch 0068 | Time 0.086 (0.114) | NFE-F 67.5 | NFE-B 0.0 | Train Loss 792.0601
Epoch 0069 | Time 0.089 (0.114) | NFE-F 67.3 | NFE-B 0.0 | Train Loss 769.7064
Epoch 0070 | Time 0.086 (0.113) | NFE-F 67.0 | NFE-B 0.0 | Train Loss 748.1091
Epoch 0071 | Time 0.087 (0.113) | NFE-F 66.7 | NFE-B 0.0 | Train Loss 727.2489
Epoch 0072 | Time 0.087 (0.113) | NFE-F 66.5 | NFE-B 0.0 | Train Loss 707.1072
Epoch 0073 | Time 0.092 (0.113) | NFE-F 66.2 | NFE-B 0.0 | Train Loss 687.6653
Epoch 0074 | Time 0.088 (0.112) | NFE-F 65.9 | NFE-B 0.0 | Train Loss 668.9053
Epoch 0075 | Time 0.086 (0.112) | NFE-F 65.7 | NFE-B 0.0 | Train Loss 650.8082
Epoch 0076 | Time 0.086 (0.112) | NFE-F 65.4 | NFE-B 0.0 | Train Loss 633.3569
Epoch 0077 | Time 0.085 (0.112) | NFE-F 65.2 | NFE-B 0.0 | Train Loss 616.5334
Epoch 0078 | Time 0.092 (0.111) | NFE-F 64.9 | NFE-B 0.0 | Train Loss 600.3204
Epoch 0079 | Time 0.087 (0.111) | NFE-F 64.7 | NFE-B 0.0 | Train Loss 584.7004
Epoch 0080 | Time 0.087 (0.111) | NFE-F 64.4 | NFE-B 0.0 | Train Loss 569.6570
Epoch 0081 | Time 0.089 (0.111) | NFE-F 64.2 | NFE-B 0.0 | Train Loss 555.1735
Epoch 0082 | Time 0.088 (0.111) | NFE-F 63.9 | NFE-B 0.0 | Train Loss 541.2333
Epoch 0083 | Time 0.085 (0.110) | NFE-F 63.7 | NFE-B 0.0 | Train Loss 527.8205
Epoch 0084 | Time 0.085 (0.110) | NFE-F 63.5 | NFE-B 0.0 | Train Loss 514.9194
Epoch 0085 | Time 0.092 (0.110) | NFE-F 63.2 | NFE-B 0.0 | Train Loss 502.5143
Epoch 0086 | Time 0.083 (0.110) | NFE-F 63.0 | NFE-B 0.0 | Train Loss 490.5901
Epoch 0087 | Time 0.087 (0.109) | NFE-F 62.8 | NFE-B 0.0 | Train Loss 479.1318
Epoch 0088 | Time 0.085 (0.109) | NFE-F 62.5 | NFE-B 0.0 | Train Loss 468.1247
Epoch 0089 | Time 0.090 (0.109) | NFE-F 62.3 | NFE-B 0.0 | Train Loss 457.5545
Epoch 0090 | Time 0.089 (0.109) | NFE-F 62.1 | NFE-B 0.0 | Train Loss 447.4071
Epoch 0091 | Time 0.093 (0.109) | NFE-F 61.9 | NFE-B 0.0 | Train Loss 437.6687
Epoch 0092 | Time 0.091 (0.108) | NFE-F 61.6 | NFE-B 0.0 | Train Loss 428.3258
Epoch 0093 | Time 0.089 (0.108) | NFE-F 61.4 | NFE-B 0.0 | Train Loss 419.3652
Epoch 0094 | Time 0.087 (0.108) | NFE-F 61.2 | NFE-B 0.0 | Train Loss 410.7740
Epoch 0095 | Time 0.085 (0.108) | NFE-F 61.0 | NFE-B 0.0 | Train Loss 402.5399
Epoch 0096 | Time 0.089 (0.108) | NFE-F 60.8 | NFE-B 0.0 | Train Loss 394.6501
Epoch 0097 | Time 0.085 (0.107) | NFE-F 60.6 | NFE-B 0.0 | Train Loss 387.0928
Epoch 0098 | Time 0.084 (0.107) | NFE-F 60.4 | NFE-B 0.0 | Train Loss 379.8561
Epoch 0099 | Time 0.091 (0.107) | NFE-F 60.2 | NFE-B 0.0 | Train Loss 372.9287
Epoch 0100 | Time 0.089 (0.107) | NFE-F 60.0 | NFE-B 0.0 | Train Loss 366.2993
Epoch 0101 | Time 0.087 (0.107) | NFE-F 59.8 | NFE-B 0.0 | Train Loss 359.9572
Epoch 0102 | Time 0.090 (0.106) | NFE-F 59.6 | NFE-B 0.0 | Train Loss 353.8918
Epoch 0103 | Time 0.088 (0.106) | NFE-F 59.4 | NFE-B 0.0 | Train Loss 348.0927
Epoch 0104 | Time 0.089 (0.106) | NFE-F 59.2 | NFE-B 0.0 | Train Loss 342.5499
Epoch 0105 | Time 0.088 (0.106) | NFE-F 59.0 | NFE-B 0.0 | Train Loss 337.2535
Epoch 0106 | Time 0.085 (0.106) | NFE-F 58.8 | NFE-B 0.0 | Train Loss 332.1943
Epoch 0107 | Time 0.081 (0.105) | NFE-F 58.6 | NFE-B 0.0 | Train Loss 327.3629
Epoch 0108 | Time 0.088 (0.105) | NFE-F 58.4 | NFE-B 0.0 | Train Loss 322.7503
Epoch 0109 | Time 0.085 (0.105) | NFE-F 58.2 | NFE-B 0.0 | Train Loss 318.3481
Epoch 0110 | Time 0.085 (0.105) | NFE-F 58.1 | NFE-B 0.0 | Train Loss 314.1476
Epoch 0111 | Time 0.088 (0.105) | NFE-F 57.9 | NFE-B 0.0 | Train Loss 310.1407
Epoch 0112 | Time 0.091 (0.105) | NFE-F 57.7 | NFE-B 0.0 | Train Loss 306.3196
Epoch 0113 | Time 0.087 (0.104) | NFE-F 57.5 | NFE-B 0.0 | Train Loss 302.6764
Epoch 0114 | Time 0.088 (0.104) | NFE-F 57.4 | NFE-B 0.0 | Train Loss 299.2039
Epoch 0115 | Time 0.084 (0.104) | NFE-F 57.2 | NFE-B 0.0 | Train Loss 295.8949
Epoch 0116 | Time 0.081 (0.104) | NFE-F 57.0 | NFE-B 0.0 | Train Loss 292.7424
Epoch 0117 | Time 0.085 (0.104) | NFE-F 56.8 | NFE-B 0.0 | Train Loss 289.7396
Epoch 0118 | Time 0.087 (0.103) | NFE-F 56.7 | NFE-B 0.0 | Train Loss 286.8800
Epoch 0119 | Time 0.086 (0.103) | NFE-F 56.5 | NFE-B 0.0 | Train Loss 284.1576
Epoch 0120 | Time 0.087 (0.103) | NFE-F 56.3 | NFE-B 0.0 | Train Loss 281.5662
Epoch 0121 | Time 0.083 (0.103) | NFE-F 56.2 | NFE-B 0.0 | Train Loss 279.0998
Epoch 0122 | Time 0.085 (0.103) | NFE-F 56.0 | NFE-B 0.0 | Train Loss 276.7530
Epoch 0123 | Time 0.086 (0.103) | NFE-F 55.8 | NFE-B 0.0 | Train Loss 274.5203
Epoch 0124 | Time 0.091 (0.102) | NFE-F 55.7 | NFE-B 0.0 | Train Loss 272.3964
Epoch 0125 | Time 0.083 (0.102) | NFE-F 55.5 | NFE-B 0.0 | Train Loss 270.3764
Epoch 0126 | Time 0.085 (0.102) | NFE-F 55.4 | NFE-B 0.0 | Train Loss 268.4554
Epoch 0127 | Time 0.087 (0.102) | NFE-F 55.2 | NFE-B 0.0 | Train Loss 266.6286
Epoch 0128 | Time 0.083 (0.102) | NFE-F 55.1 | NFE-B 0.0 | Train Loss 264.8917
Epoch 0129 | Time 0.089 (0.102) | NFE-F 54.9 | NFE-B 0.0 | Train Loss 263.2404
Epoch 0130 | Time 0.086 (0.101) | NFE-F 54.8 | NFE-B 0.0 | Train Loss 261.6705
Epoch 0131 | Time 0.090 (0.101) | NFE-F 54.6 | NFE-B 0.0 | Train Loss 260.1779
Epoch 0132 | Time 0.085 (0.101) | NFE-F 54.5 | NFE-B 0.0 | Train Loss 258.7591
Epoch 0133 | Time 0.086 (0.101) | NFE-F 54.3 | NFE-B 0.0 | Train Loss 257.4102
Epoch 0134 | Time 0.084 (0.101) | NFE-F 54.2 | NFE-B 0.0 | Train Loss 256.1278
Epoch 0135 | Time 0.089 (0.101) | NFE-F 54.0 | NFE-B 0.0 | Train Loss 254.9086
Epoch 0136 | Time 0.092 (0.101) | NFE-F 53.9 | NFE-B 0.0 | Train Loss 253.7492
Epoch 0137 | Time 0.088 (0.101) | NFE-F 53.8 | NFE-B 0.0 | Train Loss 252.6466
Epoch 0138 | Time 0.088 (0.100) | NFE-F 53.6 | NFE-B 0.0 | Train Loss 251.5979
Epoch 0139 | Time 0.086 (0.100) | NFE-F 53.5 | NFE-B 0.0 | Train Loss 250.6004
Epoch 0140 | Time 0.089 (0.100) | NFE-F 53.4 | NFE-B 0.0 | Train Loss 249.6512
Epoch 0141 | Time 0.086 (0.100) | NFE-F 53.2 | NFE-B 0.0 | Train Loss 248.7477
Epoch 0142 | Time 0.084 (0.100) | NFE-F 53.1 | NFE-B 0.0 | Train Loss 247.8877
Epoch 0143 | Time 0.084 (0.100) | NFE-F 53.0 | NFE-B 0.0 | Train Loss 247.0687
Epoch 0144 | Time 0.087 (0.100) | NFE-F 52.8 | NFE-B 0.0 | Train Loss 246.2885
Epoch 0145 | Time 0.086 (0.099) | NFE-F 52.7 | NFE-B 0.0 | Train Loss 245.5450
Epoch 0146 | Time 0.083 (0.099) | NFE-F 52.6 | NFE-B 0.0 | Train Loss 244.8360
Epoch 0147 | Time 0.089 (0.099) | NFE-F 52.5 | NFE-B 0.0 | Train Loss 244.1597
Epoch 0148 | Time 0.085 (0.099) | NFE-F 52.3 | NFE-B 0.0 | Train Loss 243.5144
Epoch 0149 | Time 0.087 (0.099) | NFE-F 52.2 | NFE-B 0.0 | Train Loss 242.8982
Epoch 0150 | Time 0.088 (0.099) | NFE-F 52.1 | NFE-B 0.0 | Train Loss 242.3094
Epoch 0151 | Time 0.089 (0.099) | NFE-F 52.0 | NFE-B 0.0 | Train Loss 241.7465
Epoch 0152 | Time 0.085 (0.099) | NFE-F 51.8 | NFE-B 0.0 | Train Loss 241.2078
Epoch 0153 | Time 0.087 (0.098) | NFE-F 51.7 | NFE-B 0.0 | Train Loss 240.6922
Epoch 0154 | Time 0.088 (0.098) | NFE-F 51.6 | NFE-B 0.0 | Train Loss 240.1982
Epoch 0155 | Time 0.089 (0.098) | NFE-F 51.5 | NFE-B 0.0 | Train Loss 239.7244
Epoch 0156 | Time 0.087 (0.098) | NFE-F 51.4 | NFE-B 0.0 | Train Loss 239.2697
Epoch 0157 | Time 0.087 (0.098) | NFE-F 51.3 | NFE-B 0.0 | Train Loss 238.8330
Epoch 0158 | Time 0.089 (0.098) | NFE-F 51.1 | NFE-B 0.0 | Train Loss 238.4131
Epoch 0159 | Time 0.087 (0.098) | NFE-F 51.0 | NFE-B 0.0 | Train Loss 238.0089
Epoch 0160 | Time 0.092 (0.098) | NFE-F 50.9 | NFE-B 0.0 | Train Loss 237.6197
Epoch 0161 | Time 0.086 (0.098) | NFE-F 50.8 | NFE-B 0.0 | Train Loss 237.2443
Epoch 0162 | Time 0.090 (0.098) | NFE-F 50.7 | NFE-B 0.0 | Train Loss 236.8819
Epoch 0163 | Time 0.088 (0.097) | NFE-F 50.6 | NFE-B 0.0 | Train Loss 236.5318
Epoch 0164 | Time 0.085 (0.097) | NFE-F 50.5 | NFE-B 0.0 | Train Loss 236.1930
Epoch 0165 | Time 0.088 (0.097) | NFE-F 50.4 | NFE-B 0.0 | Train Loss 235.8649
Epoch 0166 | Time 0.083 (0.097) | NFE-F 50.3 | NFE-B 0.0 | Train Loss 235.5469
Epoch 0167 | Time 0.087 (0.097) | NFE-F 50.2 | NFE-B 0.0 | Train Loss 235.2381
Epoch 0168 | Time 0.087 (0.097) | NFE-F 50.1 | NFE-B 0.0 | Train Loss 234.9380
Epoch 0169 | Time 0.089 (0.097) | NFE-F 50.0 | NFE-B 0.0 | Train Loss 234.6462
Epoch 0170 | Time 0.089 (0.097) | NFE-F 49.9 | NFE-B 0.0 | Train Loss 234.3619
Epoch 0171 | Time 0.089 (0.097) | NFE-F 49.8 | NFE-B 0.0 | Train Loss 234.0847
Epoch 0172 | Time 0.084 (0.097) | NFE-F 49.7 | NFE-B 0.0 | Train Loss 233.8140
Epoch 0173 | Time 0.084 (0.096) | NFE-F 49.6 | NFE-B 0.0 | Train Loss 233.5495
Epoch 0174 | Time 0.086 (0.096) | NFE-F 49.5 | NFE-B 0.0 | Train Loss 233.2907
Epoch 0175 | Time 0.087 (0.096) | NFE-F 49.4 | NFE-B 0.0 | Train Loss 233.0372
Epoch 0176 | Time 0.085 (0.096) | NFE-F 49.3 | NFE-B 0.0 | Train Loss 232.7887
Epoch 0177 | Time 0.087 (0.096) | NFE-F 49.2 | NFE-B 0.0 | Train Loss 232.5448
Epoch 0178 | Time 0.085 (0.096) | NFE-F 49.1 | NFE-B 0.0 | Train Loss 232.3051
Epoch 0179 | Time 0.083 (0.096) | NFE-F 49.0 | NFE-B 0.0 | Train Loss 232.0693
Epoch 0180 | Time 0.083 (0.096) | NFE-F 48.9 | NFE-B 0.0 | Train Loss 231.8373
Epoch 0181 | Time 0.093 (0.096) | NFE-F 48.8 | NFE-B 0.0 | Train Loss 231.6086
Epoch 0182 | Time 0.085 (0.095) | NFE-F 48.8 | NFE-B 0.0 | Train Loss 231.3830
Epoch 0183 | Time 0.089 (0.095) | NFE-F 48.7 | NFE-B 0.0 | Train Loss 231.1603
Epoch 0184 | Time 0.090 (0.095) | NFE-F 48.6 | NFE-B 0.0 | Train Loss 230.9403
Epoch 0185 | Time 0.090 (0.095) | NFE-F 48.5 | NFE-B 0.0 | Train Loss 230.7229
Epoch 0186 | Time 0.086 (0.095) | NFE-F 48.4 | NFE-B 0.0 | Train Loss 230.5077
Epoch 0187 | Time 0.085 (0.095) | NFE-F 48.3 | NFE-B 0.0 | Train Loss 230.2946
Epoch 0188 | Time 0.085 (0.095) | NFE-F 48.2 | NFE-B 0.0 | Train Loss 230.0834
Epoch 0189 | Time 0.086 (0.095) | NFE-F 48.2 | NFE-B 0.0 | Train Loss 229.8740
Epoch 0190 | Time 0.091 (0.095) | NFE-F 48.1 | NFE-B 0.0 | Train Loss 229.6662
Epoch 0191 | Time 0.086 (0.095) | NFE-F 48.0 | NFE-B 0.0 | Train Loss 229.4601
Epoch 0192 | Time 0.089 (0.095) | NFE-F 47.9 | NFE-B 0.0 | Train Loss 229.2552
Epoch 0193 | Time 0.092 (0.095) | NFE-F 47.8 | NFE-B 0.0 | Train Loss 229.0517
Epoch 0194 | Time 0.083 (0.095) | NFE-F 47.8 | NFE-B 0.0 | Train Loss 228.8492
Epoch 0195 | Time 0.084 (0.094) | NFE-F 47.7 | NFE-B 0.0 | Train Loss 228.6479
Epoch 0196 | Time 0.084 (0.094) | NFE-F 47.6 | NFE-B 0.0 | Train Loss 228.4475
Epoch 0197 | Time 0.085 (0.094) | NFE-F 47.5 | NFE-B 0.0 | Train Loss 228.2480
Epoch 0198 | Time 0.088 (0.094) | NFE-F 47.5 | NFE-B 0.0 | Train Loss 228.0493
Epoch 0199 | Time 0.088 (0.094) | NFE-F 47.4 | NFE-B 0.0 | Train Loss 227.8514
Epoch 0200 | Time 0.089 (0.094) | NFE-F 47.3 | NFE-B 0.0 | Train Loss 227.6541
Epoch 0201 | Time 0.085 (0.094) | NFE-F 47.2 | NFE-B 0.0 | Train Loss 227.4575
Epoch 0202 | Time 0.085 (0.094) | NFE-F 47.2 | NFE-B 0.0 | Train Loss 227.2613
Epoch 0203 | Time 0.095 (0.094) | NFE-F 47.1 | NFE-B 0.0 | Train Loss 227.0657
Epoch 0204 | Time 0.092 (0.094) | NFE-F 47.0 | NFE-B 0.0 | Train Loss 226.8705
Epoch 0205 | Time 0.085 (0.094) | NFE-F 47.0 | NFE-B 0.0 | Train Loss 226.6758
Epoch 0206 | Time 0.087 (0.094) | NFE-F 46.9 | NFE-B 0.0 | Train Loss 226.4813
Epoch 0207 | Time 0.087 (0.094) | NFE-F 46.8 | NFE-B 0.0 | Train Loss 226.2872
Epoch 0208 | Time 0.088 (0.094) | NFE-F 46.7 | NFE-B 0.0 | Train Loss 226.0934
Epoch 0209 | Time 0.084 (0.094) | NFE-F 46.7 | NFE-B 0.0 | Train Loss 225.8999
Epoch 0210 | Time 0.091 (0.094) | NFE-F 46.6 | NFE-B 0.0 | Train Loss 225.7066
Epoch 0211 | Time 0.086 (0.093) | NFE-F 46.5 | NFE-B 0.0 | Train Loss 225.5135
Epoch 0212 | Time 0.088 (0.093) | NFE-F 46.5 | NFE-B 0.0 | Train Loss 225.3206
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.1)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.134 (0.134) | NFE-F 80.0 | NFE-B 0.0 | Train Loss 5383.4487
Epoch 0001 | Time 0.134 (0.134) | NFE-F 80.1 | NFE-B 0.0 | Train Loss 5275.2964
Epoch 0002 | Time 0.194 (0.134) | NFE-F 80.8 | NFE-B 0.0 | Train Loss 5162.4141
Epoch 0003 | Time 0.174 (0.135) | NFE-F 81.3 | NFE-B 0.0 | Train Loss 5048.3198
Epoch 0004 | Time 0.171 (0.135) | NFE-F 81.9 | NFE-B 0.0 | Train Loss 4935.2383
Epoch 0005 | Time 0.215 (0.136) | NFE-F 83.0 | NFE-B 0.0 | Train Loss 4823.2378
Epoch 0006 | Time 0.196 (0.136) | NFE-F 83.8 | NFE-B 0.0 | Train Loss 4712.6660
Epoch 0007 | Time 0.135 (0.136) | NFE-F 83.8 | NFE-B 0.0 | Train Loss 4600.6992
Epoch 0008 | Time 0.184 (0.137) | NFE-F 84.5 | NFE-B 0.0 | Train Loss 4495.8970
Epoch 0009 | Time 0.208 (0.138) | NFE-F 85.3 | NFE-B 0.0 | Train Loss 4393.8853
Epoch 0010 | Time 0.127 (0.137) | NFE-F 85.2 | NFE-B 0.0 | Train Loss 4292.4268
Epoch 0011 | Time 0.182 (0.138) | NFE-F 86.0 | NFE-B 0.0 | Train Loss 4192.5991
Epoch 0012 | Time 0.213 (0.139) | NFE-F 87.1 | NFE-B 0.0 | Train Loss 4093.7937
Epoch 0013 | Time 0.176 (0.139) | NFE-F 87.6 | NFE-B 0.0 | Train Loss 3996.4878
Epoch 0014 | Time 0.187 (0.140) | NFE-F 88.3 | NFE-B 0.0 | Train Loss 3900.0066
Epoch 0015 | Time 0.167 (0.140) | NFE-F 88.8 | NFE-B 0.0 | Train Loss 3811.9790
Epoch 0016 | Time 0.222 (0.141) | NFE-F 89.8 | NFE-B 0.0 | Train Loss 3728.1375
Epoch 0017 | Time 0.300 (0.142) | NFE-F 91.8 | NFE-B 0.0 | Train Loss 3636.3091
Epoch 0018 | Time 0.289 (0.144) | NFE-F 93.6 | NFE-B 0.0 | Train Loss 3544.9075
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.constant_(m.weight, 0)
        nn.init.normal_(m.bias, 0, 0.1)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.062 (0.062) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 9996.7217
Epoch 0001 | Time 0.088 (0.063) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 9837.1494
Epoch 0002 | Time 0.094 (0.063) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 9680.9658
Epoch 0003 | Time 0.091 (0.063) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 9526.1426
Epoch 0004 | Time 0.087 (0.063) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 9372.6982
Epoch 0005 | Time 0.085 (0.064) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 9220.6504
Epoch 0006 | Time 0.085 (0.064) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 9070.0215
Epoch 0007 | Time 0.087 (0.064) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 8920.8291
Epoch 0008 | Time 0.095 (0.064) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 8773.0908
Epoch 0009 | Time 0.096 (0.065) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 8626.8213
Epoch 0010 | Time 0.095 (0.065) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 8482.0391
Epoch 0011 | Time 0.086 (0.065) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 8338.7568
Epoch 0012 | Time 0.087 (0.066) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 8196.9893
Epoch 0013 | Time 0.090 (0.066) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 8056.7490
Epoch 0014 | Time 0.087 (0.066) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 7918.0488
Epoch 0015 | Time 0.086 (0.066) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 7780.8994
Epoch 0016 | Time 0.089 (0.066) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 7645.3086
Epoch 0017 | Time 0.085 (0.067) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 7511.2876
Epoch 0018 | Time 0.084 (0.067) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 7378.8428
Epoch 0019 | Time 0.090 (0.067) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 7247.9810
Epoch 0020 | Time 0.093 (0.067) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 7118.7095
Epoch 0021 | Time 0.092 (0.067) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 6991.0293
Epoch 0022 | Time 0.090 (0.068) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 6864.9478
Epoch 0023 | Time 0.085 (0.068) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 6740.4653
Epoch 0024 | Time 0.087 (0.068) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 6617.5845
Epoch 0025 | Time 0.093 (0.068) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 6496.3057
Epoch 0026 | Time 0.095 (0.069) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 6376.6289
Epoch 0027 | Time 0.086 (0.069) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 6258.5537
Epoch 0028 | Time 0.085 (0.069) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 6142.0786
Epoch 0029 | Time 0.084 (0.069) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 6027.1997
Epoch 0030 | Time 0.088 (0.069) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 5913.9146
Epoch 0031 | Time 0.090 (0.069) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 5802.2173
Epoch 0032 | Time 0.090 (0.070) | NFE-F 25.5 | NFE-B 0.0 | Train Loss 5692.1050
Epoch 0033 | Time 0.093 (0.070) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 5583.5713
Epoch 0034 | Time 0.087 (0.070) | NFE-F 25.8 | NFE-B 0.0 | Train Loss 5476.6099
Epoch 0035 | Time 0.089 (0.070) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 5371.2134
Epoch 0036 | Time 0.087 (0.070) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 5267.3755
Epoch 0037 | Time 0.090 (0.071) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 5165.0874
Epoch 0038 | Time 0.089 (0.071) | NFE-F 26.3 | NFE-B 0.0 | Train Loss 5064.3403
Epoch 0039 | Time 0.088 (0.071) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 4965.1255
Epoch 0040 | Time 0.084 (0.071) | NFE-F 26.6 | NFE-B 0.0 | Train Loss 4867.4336
Epoch 0041 | Time 0.092 (0.071) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 4771.2534
Epoch 0042 | Time 0.096 (0.072) | NFE-F 26.9 | NFE-B 0.0 | Train Loss 4676.5757
Epoch 0043 | Time 0.086 (0.072) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 4583.3892
Epoch 0044 | Time 0.087 (0.072) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 4491.6836
Epoch 0045 | Time 0.088 (0.072) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 4401.4463
Epoch 0046 | Time 0.088 (0.072) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 4312.6665
Epoch 0047 | Time 0.091 (0.072) | NFE-F 27.5 | NFE-B 0.0 | Train Loss 4225.3315
Epoch 0048 | Time 0.091 (0.073) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 4139.4282
Epoch 0049 | Time 0.092 (0.073) | NFE-F 27.8 | NFE-B 0.0 | Train Loss 4054.9453
Epoch 0050 | Time 0.088 (0.073) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 3971.8694
Epoch 0051 | Time 0.086 (0.073) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 3890.1877
Epoch 0052 | Time 0.089 (0.073) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 3809.8865
Epoch 0053 | Time 0.087 (0.073) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 3730.9524
Epoch 0054 | Time 0.088 (0.073) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 3653.3721
Epoch 0055 | Time 0.089 (0.074) | NFE-F 28.5 | NFE-B 0.0 | Train Loss 3577.1311
Epoch 0056 | Time 0.091 (0.074) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 3502.2166
Epoch 0057 | Time 0.087 (0.074) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 3428.6143
Epoch 0058 | Time 0.091 (0.074) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 3356.3098
Epoch 0059 | Time 0.086 (0.074) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 3285.2900
Epoch 0060 | Time 0.089 (0.074) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 3215.5391
Epoch 0061 | Time 0.085 (0.074) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 3147.0439
Epoch 0062 | Time 0.085 (0.075) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 3079.7896
Epoch 0063 | Time 0.090 (0.075) | NFE-F 29.4 | NFE-B 0.0 | Train Loss 3013.7617
Epoch 0064 | Time 0.087 (0.075) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 2948.9456
Epoch 0065 | Time 0.088 (0.075) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 2885.3271
Epoch 0066 | Time 0.090 (0.075) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 2822.8921
Epoch 0067 | Time 0.090 (0.075) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 2761.6260
Epoch 0068 | Time 0.093 (0.075) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 2701.5129
Epoch 0069 | Time 0.086 (0.076) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 2642.5403
Epoch 0070 | Time 0.085 (0.076) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 2584.6926
Epoch 0071 | Time 0.089 (0.076) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 2527.9551
Epoch 0072 | Time 0.085 (0.076) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 2472.3137
Epoch 0073 | Time 0.092 (0.076) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 2417.7537
Epoch 0074 | Time 0.092 (0.076) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 2364.2607
Epoch 0075 | Time 0.088 (0.076) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 2311.8206
Epoch 0076 | Time 0.089 (0.076) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 2260.4192
Epoch 0077 | Time 0.085 (0.077) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 2210.0415
Epoch 0078 | Time 0.086 (0.077) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 2160.6741
Epoch 0079 | Time 0.087 (0.077) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 2112.3022
Epoch 0080 | Time 0.088 (0.077) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 2064.9116
Epoch 0081 | Time 0.086 (0.077) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 2018.4883
Epoch 0082 | Time 0.091 (0.077) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 1973.0183
Epoch 0083 | Time 0.091 (0.077) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 1928.4879
Epoch 0084 | Time 0.094 (0.077) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 1884.8828
Epoch 0085 | Time 0.091 (0.078) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 1842.1890
Epoch 0086 | Time 0.093 (0.078) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 1800.3933
Epoch 0087 | Time 0.097 (0.078) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 1759.4819
Epoch 0088 | Time 0.090 (0.078) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 1719.4413
Epoch 0089 | Time 0.087 (0.078) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 1680.2576
Epoch 0090 | Time 0.090 (0.078) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 1641.9174
Epoch 0091 | Time 0.091 (0.078) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 1604.4080
Epoch 0092 | Time 0.094 (0.079) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 1567.7157
Epoch 0093 | Time 0.096 (0.079) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 1531.8270
Epoch 0094 | Time 0.087 (0.079) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 1496.7294
Epoch 0095 | Time 0.085 (0.079) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 1462.4097
Epoch 0096 | Time 0.089 (0.079) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 1428.8550
Epoch 0097 | Time 0.091 (0.079) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 1396.0529
Epoch 0098 | Time 0.104 (0.079) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 1363.9902
Epoch 0099 | Time 0.090 (0.079) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 1332.6550
Epoch 0100 | Time 0.088 (0.079) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 1302.0344
Epoch 0101 | Time 0.087 (0.080) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 1272.1160
Epoch 0102 | Time 0.090 (0.080) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 1242.8879
Epoch 0103 | Time 0.089 (0.080) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 1214.3380
Epoch 0104 | Time 0.090 (0.080) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 1186.4540
Epoch 0105 | Time 0.118 (0.080) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 1159.2244
Epoch 0106 | Time 0.128 (0.081) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 1132.6370
Epoch 0107 | Time 0.095 (0.081) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 1106.6803
Epoch 0108 | Time 0.093 (0.081) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 1081.3429
Epoch 0109 | Time 0.087 (0.081) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 1056.6130
Epoch 0110 | Time 0.089 (0.081) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 1032.4797
Epoch 0111 | Time 0.094 (0.081) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 1008.9317
Epoch 0112 | Time 0.093 (0.081) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 985.9579
Epoch 0113 | Time 0.089 (0.081) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 963.5474
Epoch 0114 | Time 0.093 (0.082) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 941.6893
Epoch 0115 | Time 0.092 (0.082) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 920.3730
Epoch 0116 | Time 0.088 (0.082) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 899.5880
Epoch 0117 | Time 0.087 (0.082) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 879.3236
Epoch 0118 | Time 0.092 (0.082) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 859.5695
Epoch 0119 | Time 0.094 (0.082) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 840.3154
Epoch 0120 | Time 0.090 (0.082) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 821.5518
Epoch 0121 | Time 0.094 (0.082) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 803.2682
Epoch 0122 | Time 0.123 (0.083) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 785.4553
Epoch 0123 | Time 0.121 (0.083) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 768.1028
Epoch 0124 | Time 0.094 (0.083) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 751.2015
Epoch 0125 | Time 0.092 (0.083) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 734.7419
Epoch 0126 | Time 0.091 (0.083) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 718.7145
Epoch 0127 | Time 0.094 (0.083) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 703.1104
Epoch 0128 | Time 0.105 (0.084) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 687.9205
Epoch 0129 | Time 0.095 (0.084) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 673.1356
Epoch 0130 | Time 0.087 (0.084) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 658.7471
Epoch 0131 | Time 0.092 (0.084) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 644.7464
Epoch 0132 | Time 0.089 (0.084) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 631.1249
Epoch 0133 | Time 0.100 (0.084) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 617.8741
Epoch 0134 | Time 0.091 (0.084) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 604.9858
Epoch 0135 | Time 0.088 (0.084) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 592.4518
Epoch 0136 | Time 0.086 (0.084) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 580.2641
Epoch 0137 | Time 0.096 (0.084) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 568.4148
Epoch 0138 | Time 0.095 (0.084) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 556.8960
Epoch 0139 | Time 0.091 (0.084) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 545.7001
Epoch 0140 | Time 0.091 (0.085) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 534.8196
Epoch 0141 | Time 0.087 (0.085) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 524.2473
Epoch 0142 | Time 0.089 (0.085) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 513.9755
Epoch 0143 | Time 0.092 (0.085) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 503.9974
Epoch 0144 | Time 0.097 (0.085) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 494.3058
Epoch 0145 | Time 0.111 (0.085) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 484.8936
Epoch 0146 | Time 0.101 (0.085) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 475.7543
Epoch 0147 | Time 0.105 (0.085) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 466.8811
Epoch 0148 | Time 0.093 (0.086) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 458.2675
Epoch 0149 | Time 0.089 (0.086) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 449.9070
Epoch 0150 | Time 0.103 (0.086) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 441.7933
Epoch 0151 | Time 0.110 (0.086) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 433.9202
Epoch 0152 | Time 0.090 (0.086) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 426.2816
Epoch 0153 | Time 0.092 (0.086) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 418.8717
Epoch 0154 | Time 0.091 (0.086) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 411.6843
Epoch 0155 | Time 0.091 (0.086) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 404.7140
Epoch 0156 | Time 0.089 (0.086) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 397.9550
Epoch 0157 | Time 0.093 (0.086) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 391.4019
Epoch 0158 | Time 0.089 (0.086) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 385.0491
Epoch 0159 | Time 0.091 (0.086) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 378.8913
Epoch 0160 | Time 0.087 (0.086) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 372.9236
Epoch 0161 | Time 0.087 (0.086) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 367.1405
Epoch 0162 | Time 0.088 (0.086) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 361.5374
Epoch 0163 | Time 0.092 (0.086) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 356.1091
Epoch 0164 | Time 0.088 (0.086) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 350.8512
Epoch 0165 | Time 0.090 (0.086) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 345.7588
Epoch 0166 | Time 0.091 (0.086) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 340.8273
Epoch 0167 | Time 0.088 (0.087) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 336.0522
Epoch 0168 | Time 0.093 (0.087) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 331.4292
Epoch 0169 | Time 0.094 (0.087) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 326.9540
Epoch 0170 | Time 0.095 (0.087) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 322.6225
Epoch 0171 | Time 0.148 (0.087) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 318.4305
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.constant_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.056 (0.056) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 583.9598
Epoch 0001 | Time 0.085 (0.056) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 545.5901
Epoch 0002 | Time 0.085 (0.057) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 500.3229
Epoch 0003 | Time 0.090 (0.057) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 458.0338
Epoch 0004 | Time 0.092 (0.057) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 428.6454
Epoch 0005 | Time 0.083 (0.058) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 400.6925
Epoch 0006 | Time 0.090 (0.058) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 374.1298
Epoch 0007 | Time 0.087 (0.058) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 348.9561
Epoch 0008 | Time 0.091 (0.058) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 325.1686
Epoch 0009 | Time 0.088 (0.059) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 302.7610
Epoch 0010 | Time 0.091 (0.059) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 281.7241
Epoch 0011 | Time 0.085 (0.059) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 262.0439
Epoch 0012 | Time 0.089 (0.060) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 243.7030
Epoch 0013 | Time 0.082 (0.060) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 226.6790
Epoch 0014 | Time 0.086 (0.060) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 210.9451
Epoch 0015 | Time 0.087 (0.060) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 196.4702
Epoch 0016 | Time 0.088 (0.061) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 183.2185
Epoch 0017 | Time 0.090 (0.061) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 171.1498
Epoch 0018 | Time 0.091 (0.061) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 160.2197
Epoch 0019 | Time 0.092 (0.062) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 150.3799
Epoch 0020 | Time 0.089 (0.062) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 141.5784
Epoch 0021 | Time 0.086 (0.062) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 133.7600
Epoch 0022 | Time 0.083 (0.062) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 126.8663
Epoch 0023 | Time 0.087 (0.063) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 120.8372
Epoch 0024 | Time 0.091 (0.063) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 115.6104
Epoch 0025 | Time 0.084 (0.063) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 111.1221
Epoch 0026 | Time 0.088 (0.063) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 107.3088
Epoch 0027 | Time 0.087 (0.064) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 104.1062
Epoch 0028 | Time 0.090 (0.064) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 101.4511
Epoch 0029 | Time 0.085 (0.064) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 99.2817
Epoch 0030 | Time 0.089 (0.064) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 97.5380
Epoch 0031 | Time 0.088 (0.064) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 96.1623
Epoch 0032 | Time 0.088 (0.065) | NFE-F 25.5 | NFE-B 0.0 | Train Loss 95.1003
Epoch 0033 | Time 0.083 (0.065) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 94.3006
Epoch 0034 | Time 0.083 (0.065) | NFE-F 25.8 | NFE-B 0.0 | Train Loss 93.7160
Epoch 0035 | Time 0.089 (0.065) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 93.3030
Epoch 0036 | Time 0.083 (0.065) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 93.0224
Epoch 0037 | Time 0.081 (0.066) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 92.8394
Epoch 0038 | Time 0.088 (0.066) | NFE-F 26.3 | NFE-B 0.0 | Train Loss 92.7233
Epoch 0039 | Time 0.083 (0.066) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 92.6478
Epoch 0040 | Time 0.092 (0.066) | NFE-F 26.6 | NFE-B 0.0 | Train Loss 92.5908
Epoch 0041 | Time 0.087 (0.067) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 92.5340
Epoch 0042 | Time 0.083 (0.067) | NFE-F 26.9 | NFE-B 0.0 | Train Loss 92.4630
Epoch 0043 | Time 0.087 (0.067) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 92.3667
Epoch 0044 | Time 0.090 (0.067) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 92.2370
Epoch 0045 | Time 0.085 (0.067) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 92.0690
Epoch 0046 | Time 0.087 (0.067) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 91.8598
Epoch 0047 | Time 0.088 (0.068) | NFE-F 27.5 | NFE-B 0.0 | Train Loss 91.6087
Epoch 0048 | Time 0.084 (0.068) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 91.3168
Epoch 0049 | Time 0.092 (0.068) | NFE-F 27.8 | NFE-B 0.0 | Train Loss 90.9865
Epoch 0050 | Time 0.087 (0.068) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 90.6214
Epoch 0051 | Time 0.084 (0.068) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 90.2257
Epoch 0052 | Time 0.089 (0.069) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 89.8042
Epoch 0053 | Time 0.085 (0.069) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 89.3619
Epoch 0054 | Time 0.084 (0.069) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 88.9039
Epoch 0055 | Time 0.084 (0.069) | NFE-F 28.5 | NFE-B 0.0 | Train Loss 88.4352
Epoch 0056 | Time 0.082 (0.069) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 87.9605
Epoch 0057 | Time 0.084 (0.069) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 87.4842
Epoch 0058 | Time 0.084 (0.070) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 87.0100
Epoch 0059 | Time 0.083 (0.070) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 86.5413
Epoch 0060 | Time 0.088 (0.070) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 86.0810
Epoch 0061 | Time 0.086 (0.070) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 85.6312
Epoch 0062 | Time 0.089 (0.070) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 85.1935
Epoch 0063 | Time 0.087 (0.070) | NFE-F 29.4 | NFE-B 0.0 | Train Loss 84.7693
Epoch 0064 | Time 0.083 (0.070) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 84.3591
Epoch 0065 | Time 0.082 (0.071) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 83.9632
Epoch 0066 | Time 0.082 (0.071) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 83.5815
Epoch 0067 | Time 0.084 (0.071) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 83.2134
Epoch 0068 | Time 0.085 (0.071) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 82.8582
Epoch 0069 | Time 0.082 (0.071) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 82.5150
Epoch 0070 | Time 0.085 (0.071) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 82.1827
Epoch 0071 | Time 0.086 (0.071) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 81.8601
Epoch 0072 | Time 0.118 (0.072) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 81.5460
Epoch 0073 | Time 0.108 (0.072) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 81.2391
Epoch 0074 | Time 0.095 (0.072) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 80.9384
Epoch 0075 | Time 0.127 (0.073) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 80.6427
Epoch 0076 | Time 0.098 (0.073) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 80.3508
Epoch 0077 | Time 0.100 (0.073) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 80.0621
Epoch 0078 | Time 0.121 (0.074) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 79.7755
Epoch 0079 | Time 0.086 (0.074) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 79.4906
Epoch 0080 | Time 0.103 (0.074) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 79.2066
Epoch 0081 | Time 0.130 (0.075) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 78.9232
Epoch 0082 | Time 0.089 (0.075) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 78.6401
Epoch 0083 | Time 0.123 (0.076) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 78.3572
Epoch 0084 | Time 0.108 (0.076) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 78.0741
Epoch 0085 | Time 0.088 (0.076) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 77.7910
Epoch 0086 | Time 0.090 (0.076) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 77.5080
Epoch 0087 | Time 0.092 (0.076) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 77.2249
Epoch 0088 | Time 0.089 (0.076) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 76.9421
Epoch 0089 | Time 0.085 (0.076) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 76.6596
Epoch 0090 | Time 0.093 (0.077) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 76.3778
Epoch 0091 | Time 0.085 (0.077) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 76.0965
Epoch 0092 | Time 0.081 (0.077) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 75.8162
Epoch 0093 | Time 0.089 (0.077) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 75.5370
Epoch 0094 | Time 0.090 (0.077) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 75.2589
Epoch 0095 | Time 0.091 (0.077) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 74.9822
Epoch 0096 | Time 0.086 (0.077) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 74.7069
Epoch 0097 | Time 0.083 (0.077) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 74.4332
Epoch 0098 | Time 0.088 (0.077) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 74.1611
Epoch 0099 | Time 0.084 (0.077) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 73.8906
Epoch 0100 | Time 0.084 (0.078) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 73.6219
Epoch 0101 | Time 0.083 (0.078) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 73.3548
Epoch 0102 | Time 0.085 (0.078) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 73.0893
Epoch 0103 | Time 0.083 (0.078) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 72.8256
Epoch 0104 | Time 0.083 (0.078) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 72.5634
Epoch 0105 | Time 0.082 (0.078) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 72.3028
Epoch 0106 | Time 0.084 (0.078) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 72.0438
Epoch 0107 | Time 0.089 (0.078) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 71.7862
Epoch 0108 | Time 0.084 (0.078) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 71.5301
Epoch 0109 | Time 0.093 (0.078) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 71.2754
Epoch 0110 | Time 0.119 (0.079) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 71.0220
Epoch 0111 | Time 0.086 (0.079) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 70.7701
Epoch 0112 | Time 0.085 (0.079) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 70.5193
Epoch 0113 | Time 0.082 (0.079) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 70.2699
Epoch 0114 | Time 0.082 (0.079) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 70.0217
Epoch 0115 | Time 0.084 (0.079) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 69.7747
Epoch 0116 | Time 0.081 (0.079) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 69.5289
Epoch 0117 | Time 0.084 (0.079) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 69.2843
Epoch 0118 | Time 0.087 (0.079) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 69.0410
Epoch 0119 | Time 0.086 (0.079) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 68.7988
Epoch 0120 | Time 0.084 (0.079) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 68.5578
Epoch 0121 | Time 0.086 (0.079) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 68.3181
Epoch 0122 | Time 0.085 (0.079) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 68.0795
Epoch 0123 | Time 0.086 (0.079) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 67.8422
Epoch 0124 | Time 0.088 (0.079) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 67.6060
Epoch 0125 | Time 0.082 (0.079) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 67.3711
Epoch 0126 | Time 0.087 (0.080) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 67.1374
Epoch 0127 | Time 0.086 (0.080) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 66.9050
Epoch 0128 | Time 0.088 (0.080) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 66.6737
Epoch 0129 | Time 0.086 (0.080) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 66.4437
Epoch 0130 | Time 0.090 (0.080) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 66.2149
Epoch 0131 | Time 0.084 (0.080) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 65.9874
Epoch 0132 | Time 0.084 (0.080) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 65.7610
Epoch 0133 | Time 0.089 (0.080) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 65.5359
Epoch 0134 | Time 0.082 (0.080) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 65.3120
Epoch 0135 | Time 0.083 (0.080) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 65.0892
Epoch 0136 | Time 0.081 (0.080) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 64.8677
Epoch 0137 | Time 0.085 (0.080) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 64.6473
Epoch 0138 | Time 0.083 (0.080) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 64.4281
Epoch 0139 | Time 0.082 (0.080) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 64.2101
Epoch 0140 | Time 0.085 (0.080) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 63.9933
Epoch 0141 | Time 0.084 (0.080) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 63.7776
Epoch 0142 | Time 0.092 (0.080) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 63.5631
Epoch 0143 | Time 0.085 (0.080) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 63.3497
Epoch 0144 | Time 0.090 (0.081) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 63.1374
Epoch 0145 | Time 0.086 (0.081) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 62.9263
Epoch 0146 | Time 0.088 (0.081) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 62.7162
Epoch 0147 | Time 0.084 (0.081) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 62.5073
Epoch 0148 | Time 0.086 (0.081) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 62.2995
Epoch 0149 | Time 0.086 (0.081) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 62.0928
Epoch 0150 | Time 0.081 (0.081) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 61.8872
Epoch 0151 | Time 0.085 (0.081) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 61.6827
Epoch 0152 | Time 0.084 (0.081) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 61.4793
Epoch 0153 | Time 0.091 (0.081) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 61.2770
Epoch 0154 | Time 0.082 (0.081) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 61.0757
Epoch 0155 | Time 0.082 (0.081) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 60.8755
Epoch 0156 | Time 0.084 (0.081) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 60.6764
Epoch 0157 | Time 0.084 (0.081) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 60.4783
Epoch 0158 | Time 0.085 (0.081) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 60.2813
Epoch 0159 | Time 0.084 (0.081) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 60.0853
Epoch 0160 | Time 0.090 (0.081) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 59.8904
Epoch 0161 | Time 0.082 (0.081) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 59.6964
Epoch 0162 | Time 0.085 (0.081) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 59.5035
Epoch 0163 | Time 0.087 (0.081) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 59.3117
Epoch 0164 | Time 0.086 (0.081) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 59.1208
Epoch 0165 | Time 0.090 (0.081) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 58.9309
Epoch 0166 | Time 0.083 (0.081) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 58.7421
Epoch 0167 | Time 0.083 (0.081) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 58.5542
Epoch 0168 | Time 0.084 (0.082) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 58.3673
Epoch 0169 | Time 0.083 (0.082) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 58.1814
Epoch 0170 | Time 0.089 (0.082) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 57.9965
Epoch 0171 | Time 0.082 (0.082) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 57.8125
Epoch 0172 | Time 0.082 (0.082) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 57.6295
Epoch 0173 | Time 0.085 (0.082) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 57.4474
Epoch 0174 | Time 0.081 (0.082) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 57.2663
Epoch 0175 | Time 0.082 (0.082) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 57.0861
Epoch 0176 | Time 0.080 (0.082) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 56.9068
Epoch 0177 | Time 0.091 (0.082) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 56.7285
Epoch 0178 | Time 0.085 (0.082) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 56.5511
Epoch 0179 | Time 0.085 (0.082) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 56.3746
Epoch 0180 | Time 0.087 (0.082) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 56.1990
Epoch 0181 | Time 0.086 (0.082) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 56.0243
Epoch 0182 | Time 0.083 (0.082) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 55.8505
Epoch 0183 | Time 0.085 (0.082) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 55.6776
Epoch 0184 | Time 0.085 (0.082) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 55.5055
Epoch 0185 | Time 0.085 (0.082) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 55.3343
Epoch 0186 | Time 0.084 (0.082) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 55.1640
Epoch 0187 | Time 0.084 (0.082) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 54.9946
Epoch 0188 | Time 0.085 (0.082) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 54.8260
Epoch 0189 | Time 0.084 (0.082) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 54.6583
Epoch 0190 | Time 0.086 (0.082) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 54.4913
Epoch 0191 | Time 0.085 (0.082) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 54.3253
Epoch 0192 | Time 0.084 (0.082) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 54.1600
Epoch 0193 | Time 0.086 (0.082) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 53.9956
Epoch 0194 | Time 0.084 (0.082) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 53.8320
Epoch 0195 | Time 0.087 (0.082) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 53.6692
Epoch 0196 | Time 0.086 (0.082) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 53.5072
Epoch 0197 | Time 0.089 (0.082) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 53.3459
Epoch 0198 | Time 0.088 (0.082) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 53.1855
Epoch 0199 | Time 0.086 (0.082) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 53.0259
Epoch 0200 | Time 0.088 (0.083) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 52.8670
Epoch 0201 | Time 0.085 (0.083) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 52.7089
Epoch 0202 | Time 0.089 (0.083) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 52.5516
Epoch 0203 | Time 0.088 (0.083) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 52.3950
Epoch 0204 | Time 0.085 (0.083) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 52.2392
Epoch 0205 | Time 0.084 (0.083) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 52.0841
Epoch 0206 | Time 0.082 (0.083) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 51.9298
Epoch 0207 | Time 0.087 (0.083) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 51.7762
Epoch 0208 | Time 0.088 (0.083) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 51.6233
Epoch 0209 | Time 0.084 (0.083) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 51.4712
Epoch 0210 | Time 0.085 (0.083) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 51.3198
Epoch 0211 | Time 0.083 (0.083) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 51.1691
Epoch 0212 | Time 0.090 (0.083) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 51.0191
Epoch 0213 | Time 0.086 (0.083) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 50.8697
Epoch 0214 | Time 0.081 (0.083) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 50.7211
Epoch 0215 | Time 0.084 (0.083) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 50.5732
Epoch 0216 | Time 0.082 (0.083) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 50.4260
Epoch 0217 | Time 0.088 (0.083) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 50.2794
Epoch 0218 | Time 0.082 (0.083) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 50.1335
Epoch 0219 | Time 0.084 (0.083) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 49.9883
Epoch 0220 | Time 0.085 (0.083) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 49.8437
Epoch 0221 | Time 0.082 (0.083) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 49.6999
Epoch 0222 | Time 0.082 (0.083) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 49.5566
Epoch 0223 | Time 0.085 (0.083) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 49.4140
Epoch 0224 | Time 0.084 (0.083) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 49.2721
Epoch 0225 | Time 0.081 (0.083) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 49.1307
Epoch 0226 | Time 0.083 (0.083) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 48.9900
Epoch 0227 | Time 0.081 (0.083) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 48.8500
Epoch 0228 | Time 0.085 (0.083) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 48.7106
Epoch 0229 | Time 0.087 (0.083) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 48.5717
Epoch 0230 | Time 0.089 (0.083) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 48.4335
Epoch 0231 | Time 0.087 (0.083) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 48.2959
Epoch 0232 | Time 0.082 (0.083) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 48.1589
Epoch 0233 | Time 0.093 (0.083) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 48.0225
Epoch 0234 | Time 0.082 (0.083) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 47.8867
Epoch 0235 | Time 0.085 (0.083) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 47.7515
Epoch 0236 | Time 0.096 (0.083) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 47.6169
Epoch 0237 | Time 0.085 (0.083) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 47.4828
Epoch 0238 | Time 0.082 (0.083) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 47.3493
Epoch 0239 | Time 0.083 (0.083) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 47.2164
Epoch 0240 | Time 0.081 (0.083) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 47.0841
Epoch 0241 | Time 0.083 (0.083) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 46.9523
Epoch 0242 | Time 0.085 (0.083) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 46.8211
Epoch 0243 | Time 0.088 (0.083) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 46.6904
Epoch 0244 | Time 0.083 (0.083) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 46.5603
Epoch 0245 | Time 0.080 (0.083) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 46.4308
Epoch 0246 | Time 0.086 (0.083) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 46.3017
Epoch 0247 | Time 0.092 (0.083) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 46.1732
Epoch 0248 | Time 0.082 (0.083) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 46.0453
Epoch 0249 | Time 0.089 (0.083) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 45.9179
Epoch 0250 | Time 0.085 (0.083) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 45.7910
Epoch 0251 | Time 0.087 (0.084) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 45.6646
Epoch 0252 | Time 0.088 (0.084) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 45.5387
Epoch 0253 | Time 0.089 (0.084) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 45.4134
Epoch 0254 | Time 0.086 (0.084) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 45.2886
Epoch 0255 | Time 0.092 (0.084) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 45.1643
Epoch 0256 | Time 0.086 (0.084) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 45.0404
Epoch 0257 | Time 0.083 (0.084) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 44.9171
Epoch 0258 | Time 0.089 (0.084) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 44.7943
Epoch 0259 | Time 0.091 (0.084) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 44.6720
Epoch 0260 | Time 0.083 (0.084) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 44.5502
Epoch 0261 | Time 0.084 (0.084) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 44.4288
Epoch 0262 | Time 0.089 (0.084) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 44.3080
Epoch 0263 | Time 0.087 (0.084) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 44.1876
Epoch 0264 | Time 0.081 (0.084) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 44.0677
Epoch 0265 | Time 0.083 (0.084) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 43.9482
Epoch 0266 | Time 0.084 (0.084) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 43.8293
Epoch 0267 | Time 0.083 (0.084) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 43.7108
Epoch 0268 | Time 0.082 (0.084) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 43.5928
Epoch 0269 | Time 0.081 (0.084) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 43.4752
Epoch 0270 | Time 0.088 (0.084) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 43.3581
Epoch 0271 | Time 0.089 (0.084) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 43.2415
Epoch 0272 | Time 0.084 (0.084) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 43.1253
Epoch 0273 | Time 0.088 (0.084) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 43.0096
Epoch 0274 | Time 0.082 (0.084) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 42.8943
Epoch 0275 | Time 0.080 (0.084) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 42.7794
Epoch 0276 | Time 0.090 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 42.6650
Epoch 0277 | Time 0.086 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 42.5511
Epoch 0278 | Time 0.088 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 42.4375
Epoch 0279 | Time 0.086 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 42.3244
Epoch 0280 | Time 0.086 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 42.2118
Epoch 0281 | Time 0.088 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 42.0996
Epoch 0282 | Time 0.092 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 41.9878
Epoch 0283 | Time 0.083 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 41.8764
Epoch 0284 | Time 0.081 (0.084) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 41.7654
Epoch 0285 | Time 0.084 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 41.6549
Epoch 0286 | Time 0.081 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 41.5448
Epoch 0287 | Time 0.087 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 41.4351
Epoch 0288 | Time 0.082 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 41.3258
Epoch 0289 | Time 0.082 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 41.2169
Epoch 0290 | Time 0.084 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 41.1085
Epoch 0291 | Time 0.092 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 41.0004
Epoch 0292 | Time 0.090 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 40.8928
Epoch 0293 | Time 0.089 (0.084) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 40.7855
Epoch 0294 | Time 0.088 (0.084) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 40.6786
Epoch 0295 | Time 0.082 (0.084) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 40.5722
Epoch 0296 | Time 0.085 (0.084) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 40.4661
Epoch 0297 | Time 0.083 (0.084) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 40.3605
Epoch 0298 | Time 0.085 (0.084) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 40.2552
Epoch 0299 | Time 0.084 (0.084) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 40.1503
Epoch 0300 | Time 0.088 (0.084) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 40.0458
Epoch 0301 | Time 0.100 (0.085) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 39.9417
Epoch 0302 | Time 0.092 (0.085) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 39.8380
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        # out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.056 (0.056) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 1162.8940
Epoch 0001 | Time 0.085 (0.057) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 1099.0060
Epoch 0002 | Time 0.093 (0.057) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 1038.4946
Epoch 0003 | Time 0.090 (0.057) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 988.0681
Epoch 0004 | Time 0.080 (0.058) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 938.5386
Epoch 0005 | Time 0.085 (0.058) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 890.9938
Epoch 0006 | Time 0.092 (0.058) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 847.9694
Epoch 0007 | Time 0.084 (0.059) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 806.6031
Epoch 0008 | Time 0.087 (0.059) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 766.6641
Epoch 0009 | Time 0.087 (0.059) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 728.1566
Epoch 0010 | Time 0.082 (0.059) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 691.0829
Epoch 0011 | Time 0.083 (0.060) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 655.4436
Epoch 0012 | Time 0.085 (0.060) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 621.2367
Epoch 0013 | Time 0.086 (0.060) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 588.4567
Epoch 0014 | Time 0.088 (0.060) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 557.0964
Epoch 0015 | Time 0.084 (0.061) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 527.1456
Epoch 0016 | Time 0.091 (0.061) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 498.5916
Epoch 0017 | Time 0.089 (0.061) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 471.4184
Epoch 0018 | Time 0.087 (0.061) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 445.6078
Epoch 0019 | Time 0.087 (0.062) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 421.1387
Epoch 0020 | Time 0.089 (0.062) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 397.9879
Epoch 0021 | Time 0.088 (0.062) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 376.1288
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.047 (0.047) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 316.1271
Epoch 0001 | Time 0.077 (0.047) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 292.0857
Epoch 0002 | Time 0.077 (0.048) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 269.3337
Epoch 0003 | Time 0.075 (0.048) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 247.8647
Epoch 0004 | Time 0.079 (0.048) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 227.6795
Epoch 0005 | Time 0.075 (0.048) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 208.7838
Epoch 0006 | Time 0.073 (0.049) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 191.1821
Epoch 0007 | Time 0.074 (0.049) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 174.8855
Epoch 0008 | Time 0.074 (0.049) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 159.9023
Epoch 0009 | Time 0.076 (0.049) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 146.2413
Epoch 0010 | Time 0.074 (0.050) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 133.9215
Epoch 0011 | Time 0.080 (0.050) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 122.9515
Epoch 0012 | Time 0.080 (0.050) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 113.3477
Epoch 0013 | Time 0.080 (0.051) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 105.1172
Epoch 0014 | Time 0.076 (0.051) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 98.2620
Epoch 0015 | Time 0.089 (0.051) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 92.7602
Epoch 0016 | Time 0.085 (0.052) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 88.5601
Epoch 0017 | Time 0.089 (0.052) | NFE-F 16.6 | NFE-B 0.0 | Train Loss 85.5678
Epoch 0018 | Time 0.090 (0.052) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 83.6451
Epoch 0019 | Time 0.087 (0.053) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 82.6177
Epoch 0020 | Time 0.087 (0.053) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 82.2938
Epoch 0021 | Time 0.092 (0.053) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 82.4852
Epoch 0022 | Time 0.093 (0.054) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 83.0220
Epoch 0023 | Time 0.090 (0.054) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 83.7604
Epoch 0024 | Time 0.090 (0.054) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 84.5842
Epoch 0025 | Time 0.091 (0.055) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 85.4025
Epoch 0026 | Time 0.096 (0.055) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 86.1471
Epoch 0027 | Time 0.087 (0.056) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 86.7691
Epoch 0028 | Time 0.092 (0.056) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 87.2361
Epoch 0029 | Time 0.091 (0.056) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 87.5305
Epoch 0030 | Time 0.090 (0.057) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 87.6462
Epoch 0031 | Time 0.086 (0.057) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 87.5871
Epoch 0032 | Time 0.091 (0.057) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 87.3652
Epoch 0033 | Time 0.088 (0.058) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 86.9984
Epoch 0034 | Time 0.094 (0.058) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 86.5087
Epoch 0035 | Time 0.094 (0.058) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 85.9206
Epoch 0036 | Time 0.090 (0.059) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 85.2599
Epoch 0037 | Time 0.092 (0.059) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 84.5522
Epoch 0038 | Time 0.090 (0.059) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 83.8214
Epoch 0039 | Time 0.086 (0.060) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 83.0900
Epoch 0040 | Time 0.089 (0.060) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 82.3771
Epoch 0041 | Time 0.088 (0.060) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 81.6991
Epoch 0042 | Time 0.088 (0.060) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 81.0687
Epoch 0043 | Time 0.085 (0.061) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 80.4950
Epoch 0044 | Time 0.089 (0.061) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 79.9837
Epoch 0045 | Time 0.095 (0.061) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 79.5373
Epoch 0046 | Time 0.090 (0.062) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 79.1552
Epoch 0047 | Time 0.089 (0.062) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 78.8343
Epoch 0048 | Time 0.090 (0.062) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 78.5692
Epoch 0049 | Time 0.093 (0.062) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 78.3531
Epoch 0050 | Time 0.092 (0.063) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 78.1780
Epoch 0051 | Time 0.089 (0.063) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 78.0353
Epoch 0052 | Time 0.090 (0.063) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 77.9164
Epoch 0053 | Time 0.088 (0.063) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 77.8128
Epoch 0054 | Time 0.092 (0.064) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 77.7169
Epoch 0055 | Time 0.095 (0.064) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 77.6221
Epoch 0056 | Time 0.105 (0.064) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 77.5230
Epoch 0057 | Time 0.097 (0.065) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 77.4153
Epoch 0058 | Time 0.088 (0.065) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 77.2964
Epoch 0059 | Time 0.090 (0.065) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 77.1646
Epoch 0060 | Time 0.091 (0.066) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 77.0197
Epoch 0061 | Time 0.094 (0.066) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 76.8621
Epoch 0062 | Time 0.087 (0.066) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 76.6933
Epoch 0063 | Time 0.094 (0.066) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 76.5152
Epoch 0064 | Time 0.092 (0.067) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 76.3299
Epoch 0065 | Time 0.089 (0.067) | NFE-F 25.5 | NFE-B 0.0 | Train Loss 76.1399
Epoch 0066 | Time 0.088 (0.067) | NFE-F 25.7 | NFE-B 0.0 | Train Loss 75.9473
Epoch 0067 | Time 0.094 (0.067) | NFE-F 25.8 | NFE-B 0.0 | Train Loss 75.7544
Epoch 0068 | Time 0.089 (0.068) | NFE-F 26.0 | NFE-B 0.0 | Train Loss 75.5629
Epoch 0069 | Time 0.088 (0.068) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 75.3741
Epoch 0070 | Time 0.090 (0.068) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 75.1893
Epoch 0071 | Time 0.090 (0.068) | NFE-F 26.4 | NFE-B 0.0 | Train Loss 75.0091
Epoch 0072 | Time 0.088 (0.068) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 74.8337
Epoch 0073 | Time 0.086 (0.069) | NFE-F 26.6 | NFE-B 0.0 | Train Loss 74.6631
Epoch 0074 | Time 0.089 (0.069) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 74.4970
Epoch 0075 | Time 0.089 (0.069) | NFE-F 26.9 | NFE-B 0.0 | Train Loss 74.3350
Epoch 0076 | Time 0.092 (0.069) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 74.1764
Epoch 0077 | Time 0.091 (0.069) | NFE-F 27.2 | NFE-B 0.0 | Train Loss 74.0204
Epoch 0078 | Time 0.090 (0.070) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 73.8664
Epoch 0079 | Time 0.087 (0.070) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 73.7136
Epoch 0080 | Time 0.086 (0.070) | NFE-F 27.6 | NFE-B 0.0 | Train Loss 73.5614
Epoch 0081 | Time 0.091 (0.070) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 73.4093
Epoch 0082 | Time 0.086 (0.070) | NFE-F 27.8 | NFE-B 0.0 | Train Loss 73.2568
Epoch 0083 | Time 0.092 (0.071) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 73.1035
Epoch 0084 | Time 0.091 (0.071) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 72.9494
Epoch 0085 | Time 0.095 (0.071) | NFE-F 28.2 | NFE-B 0.0 | Train Loss 72.7943
Epoch 0086 | Time 0.088 (0.071) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 72.6382
Epoch 0087 | Time 0.091 (0.071) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 72.4812
Epoch 0088 | Time 0.091 (0.072) | NFE-F 28.5 | NFE-B 0.0 | Train Loss 72.3235
Epoch 0089 | Time 0.096 (0.072) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 72.1653
Epoch 0090 | Time 0.093 (0.072) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 72.0066
Epoch 0091 | Time 0.088 (0.072) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 71.8479
Epoch 0092 | Time 0.088 (0.072) | NFE-F 29.0 | NFE-B 0.0 | Train Loss 71.6891
Epoch 0093 | Time 0.087 (0.072) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 71.5307
Epoch 0094 | Time 0.088 (0.073) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 71.3725
Epoch 0095 | Time 0.091 (0.073) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 71.2148
Epoch 0096 | Time 0.094 (0.073) | NFE-F 29.4 | NFE-B 0.0 | Train Loss 71.0577
Epoch 0097 | Time 0.089 (0.073) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 70.9010
Epoch 0098 | Time 0.089 (0.073) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 70.7450
Epoch 0099 | Time 0.089 (0.074) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 70.5895
Epoch 0100 | Time 0.091 (0.074) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 70.4345
Epoch 0101 | Time 0.086 (0.074) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 70.2799
Epoch 0102 | Time 0.091 (0.074) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 70.1257
Epoch 0103 | Time 0.088 (0.074) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 69.9718
Epoch 0104 | Time 0.089 (0.074) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 69.8181
Epoch 0105 | Time 0.090 (0.074) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 69.6646
Epoch 0106 | Time 0.088 (0.075) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 69.5114
Epoch 0107 | Time 0.088 (0.075) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 69.3582
Epoch 0108 | Time 0.090 (0.075) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 69.2051
Epoch 0109 | Time 0.091 (0.075) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 69.0521
Epoch 0110 | Time 0.089 (0.075) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 68.8992
Epoch 0111 | Time 0.099 (0.075) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 68.7464
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.1, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.046 (0.046) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 4844.2178
Epoch 0001 | Time 0.076 (0.046) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 221.2587
Epoch 0002 | Time 0.088 (0.047) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 3877.5693
Epoch 0003 | Time 0.087 (0.047) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 7648.1929
Epoch 0004 | Time 0.087 (0.047) | NFE-F 14.9 | NFE-B 0.0 | Train Loss 7652.2432
Epoch 0005 | Time 0.088 (0.048) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 5010.6279
Epoch 0006 | Time 0.089 (0.048) | NFE-F 15.4 | NFE-B 0.0 | Train Loss 2012.4635
Epoch 0007 | Time 0.089 (0.049) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 315.6819
Epoch 0008 | Time 0.097 (0.049) | NFE-F 15.9 | NFE-B 0.0 | Train Loss 432.0567
Epoch 0009 | Time 0.092 (0.049) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 1733.4109
Epoch 0010 | Time 0.089 (0.050) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 3019.1145
Epoch 0011 | Time 0.090 (0.050) | NFE-F 16.6 | NFE-B 0.0 | Train Loss 3394.6099
Epoch 0012 | Time 0.086 (0.051) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 2744.6016
Epoch 0013 | Time 0.089 (0.051) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 1567.4263
Epoch 0014 | Time 0.088 (0.051) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 537.7178
Epoch 0015 | Time 0.086 (0.052) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 123.1213
Epoch 0016 | Time 0.091 (0.052) | NFE-F 17.8 | NFE-B 0.0 | Train Loss 380.3769
Epoch 0017 | Time 0.090 (0.052) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 984.0091
Epoch 0018 | Time 0.090 (0.053) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 1468.5396
Epoch 0019 | Time 0.094 (0.053) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 1526.5009
Epoch 0020 | Time 0.094 (0.054) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 1158.9382
Epoch 0021 | Time 0.089 (0.054) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 613.2892
Epoch 0022 | Time 0.084 (0.054) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 197.5547
Epoch 0023 | Time 0.086 (0.055) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 97.3884
Epoch 0024 | Time 0.088 (0.055) | NFE-F 19.5 | NFE-B 0.0 | Train Loss 287.9778
Epoch 0025 | Time 0.088 (0.055) | NFE-F 19.7 | NFE-B 0.0 | Train Loss 577.7367
Epoch 0026 | Time 0.091 (0.056) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 748.1001
Epoch 0027 | Time 0.085 (0.056) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 691.8577
Epoch 0028 | Time 0.088 (0.056) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 460.6310
Epoch 0029 | Time 0.091 (0.057) | NFE-F 20.5 | NFE-B 0.0 | Train Loss 206.6494
Epoch 0030 | Time 0.097 (0.057) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 73.1783
Epoch 0031 | Time 0.092 (0.057) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 107.0451
Epoch 0032 | Time 0.093 (0.058) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 242.8912
Epoch 0033 | Time 0.092 (0.058) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 361.2183
Epoch 0034 | Time 0.093 (0.058) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 374.3663
Epoch 0035 | Time 0.090 (0.059) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 279.2275
Epoch 0036 | Time 0.089 (0.059) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 145.8871
Epoch 0037 | Time 0.088 (0.059) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 59.5342
Epoch 0038 | Time 0.090 (0.060) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 60.9815
Epoch 0039 | Time 0.086 (0.060) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 125.4846
Epoch 0040 | Time 0.084 (0.060) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 189.4531
Epoch 0041 | Time 0.093 (0.060) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 200.6219
Epoch 0042 | Time 0.091 (0.061) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 153.0610
Epoch 0043 | Time 0.087 (0.061) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 84.0917
Epoch 0044 | Time 0.092 (0.061) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 40.8377
Epoch 0045 | Time 0.090 (0.062) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 44.7152
Epoch 0046 | Time 0.090 (0.062) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 79.7268
Epoch 0047 | Time 0.092 (0.062) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 109.9368
Epoch 0048 | Time 0.095 (0.063) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 109.2678
Epoch 0049 | Time 0.090 (0.063) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 79.5006
Epoch 0050 | Time 0.090 (0.063) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 44.5014
Epoch 0051 | Time 0.092 (0.063) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 28.5214
Epoch 0052 | Time 0.094 (0.064) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 37.4669
Epoch 0053 | Time 0.092 (0.064) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 57.1671
Epoch 0054 | Time 0.091 (0.064) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 67.6115
Epoch 0055 | Time 0.091 (0.065) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 59.6663
Epoch 0056 | Time 0.085 (0.065) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 40.5916
Epoch 0057 | Time 0.089 (0.065) | NFE-F 25.3 | NFE-B 0.0 | Train Loss 25.5749
Epoch 0058 | Time 0.091 (0.065) | NFE-F 25.4 | NFE-B 0.0 | Train Loss 24.1682
Epoch 0059 | Time 0.092 (0.066) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 33.3903
Epoch 0060 | Time 0.088 (0.066) | NFE-F 25.7 | NFE-B 0.0 | Train Loss 42.1706
Epoch 0061 | Time 0.090 (0.066) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 41.8067
Epoch 0062 | Time 0.087 (0.066) | NFE-F 26.0 | NFE-B 0.0 | Train Loss 32.8313
Epoch 0063 | Time 0.092 (0.066) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 23.0538
Epoch 0064 | Time 0.088 (0.067) | NFE-F 26.3 | NFE-B 0.0 | Train Loss 19.7570
Epoch 0065 | Time 0.090 (0.067) | NFE-F 26.4 | NFE-B 0.0 | Train Loss 23.5645
Epoch 0066 | Time 0.090 (0.067) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 28.9803
Epoch 0067 | Time 0.088 (0.067) | NFE-F 26.7 | NFE-B 0.0 | Train Loss 30.1347
Epoch 0068 | Time 0.093 (0.068) | NFE-F 26.8 | NFE-B 0.0 | Train Loss 25.9517
Epoch 0069 | Time 0.088 (0.068) | NFE-F 26.9 | NFE-B 0.0 | Train Loss 20.2975
Epoch 0070 | Time 0.090 (0.068) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 17.7467
Epoch 0071 | Time 0.087 (0.068) | NFE-F 27.2 | NFE-B 0.0 | Train Loss 19.3939
Epoch 0072 | Time 0.091 (0.068) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 22.4433
Epoch 0073 | Time 0.089 (0.069) | NFE-F 27.5 | NFE-B 0.0 | Train Loss 23.3657
Epoch 0074 | Time 0.090 (0.069) | NFE-F 27.6 | NFE-B 0.0 | Train Loss 21.2145
Epoch 0075 | Time 0.085 (0.069) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 18.0561
Epoch 0076 | Time 0.091 (0.069) | NFE-F 27.8 | NFE-B 0.0 | Train Loss 16.5666
Epoch 0077 | Time 0.089 (0.069) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 17.4556
Epoch 0078 | Time 0.091 (0.070) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 19.1313
Epoch 0079 | Time 0.089 (0.070) | NFE-F 28.2 | NFE-B 0.0 | Train Loss 19.5711
Epoch 0080 | Time 0.098 (0.070) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 18.2936
Epoch 0081 | Time 0.087 (0.070) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 16.5522
Epoch 0082 | Time 0.088 (0.070) | NFE-F 28.5 | NFE-B 0.0 | Train Loss 15.8485
Epoch 0083 | Time 0.090 (0.071) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 16.4508
Epoch 0084 | Time 0.088 (0.071) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 17.3418
Epoch 0085 | Time 0.091 (0.071) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 17.4191
Epoch 0086 | Time 0.091 (0.071) | NFE-F 29.0 | NFE-B 0.0 | Train Loss 16.5806
Epoch 0087 | Time 0.090 (0.071) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 15.6538
Epoch 0088 | Time 0.088 (0.072) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 15.4218
Epoch 0089 | Time 0.088 (0.072) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 15.8550
Epoch 0090 | Time 0.085 (0.072) | NFE-F 29.4 | NFE-B 0.0 | Train Loss 16.2774
Epoch 0091 | Time 0.086 (0.072) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 16.1518
Epoch 0092 | Time 0.089 (0.072) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 15.5951
Epoch 0093 | Time 0.085 (0.072) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 15.1475
Epoch 0094 | Time 0.089 (0.072) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 15.1508
Epoch 0095 | Time 0.088 (0.073) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 15.4326
Epoch 0096 | Time 0.090 (0.073) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 15.5708
Epoch 0097 | Time 0.092 (0.073) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 15.3709
Epoch 0098 | Time 0.092 (0.073) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 15.0278
Epoch 0099 | Time 0.087 (0.073) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 14.8557
Epoch 0100 | Time 0.092 (0.074) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 14.9382
Epoch 0101 | Time 0.086 (0.074) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 15.0799
Epoch 0102 | Time 0.089 (0.074) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 15.0607
Epoch 0103 | Time 0.092 (0.074) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 14.8729
Epoch 0104 | Time 0.089 (0.074) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 14.6927
Epoch 0105 | Time 0.090 (0.074) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 14.6594
Epoch 0106 | Time 0.088 (0.074) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 14.7317
Epoch 0107 | Time 0.088 (0.075) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 14.7635
Epoch 0108 | Time 0.092 (0.075) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 14.6804
Epoch 0109 | Time 0.089 (0.075) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 14.5482
Epoch 0110 | Time 0.088 (0.075) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 14.4763
Epoch 0111 | Time 0.089 (0.075) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 14.4899
Epoch 0112 | Time 0.088 (0.075) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 14.5164
Epoch 0113 | Time 0.088 (0.075) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 14.4831
Epoch 0114 | Time 0.089 (0.076) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 14.3981
Epoch 0115 | Time 0.095 (0.076) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 14.3268
Epoch 0116 | Time 0.090 (0.076) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 14.3087
Epoch 0117 | Time 0.087 (0.076) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 14.3171
Epoch 0118 | Time 0.088 (0.076) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 14.3010
Epoch 0119 | Time 0.094 (0.076) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 14.2471
Epoch 0120 | Time 0.096 (0.076) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 14.1885
Epoch 0121 | Time 0.093 (0.077) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 14.1585
Epoch 0122 | Time 0.091 (0.077) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 14.1522
Epoch 0123 | Time 0.088 (0.077) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 14.1389
Epoch 0124 | Time 0.102 (0.077) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 14.1023
Epoch 0125 | Time 0.088 (0.077) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 14.0565
Epoch 0126 | Time 0.086 (0.077) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 14.0248
Epoch 0127 | Time 0.088 (0.077) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 14.0100
Epoch 0128 | Time 0.086 (0.078) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 13.9954
Epoch 0129 | Time 0.088 (0.078) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 13.9673
Epoch 0130 | Time 0.096 (0.078) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 13.9313
Epoch 0131 | Time 0.094 (0.078) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 13.9017
Epoch 0132 | Time 0.088 (0.078) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 13.8832
Epoch 0133 | Time 0.086 (0.078) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 13.8668
Epoch 0134 | Time 0.088 (0.078) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 13.8431
Epoch 0135 | Time 0.087 (0.078) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 13.8136
Epoch 0136 | Time 0.086 (0.078) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 13.7871
Epoch 0137 | Time 0.088 (0.079) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 13.7675
Epoch 0138 | Time 0.086 (0.079) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 13.7501
Epoch 0139 | Time 0.085 (0.079) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 13.7287
Epoch 0140 | Time 0.089 (0.079) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 13.7035
Epoch 0141 | Time 0.095 (0.079) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 13.6799
Epoch 0142 | Time 0.089 (0.079) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 13.6605
Epoch 0143 | Time 0.090 (0.079) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 13.6428
Epoch 0144 | Time 0.088 (0.079) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 13.6229
Epoch 0145 | Time 0.093 (0.079) | NFE-F 33.9 | NFE-B 0.0 | Train Loss 13.6007
Epoch 0146 | Time 0.089 (0.079) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 13.5794
Epoch 0147 | Time 0.088 (0.080) | NFE-F 34.0 | NFE-B 0.0 | Train Loss 13.5608
Epoch 0148 | Time 0.086 (0.080) | NFE-F 34.1 | NFE-B 0.0 | Train Loss 13.5433
Epoch 0149 | Time 0.090 (0.080) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 13.5245
Epoch 0150 | Time 0.090 (0.080) | NFE-F 34.2 | NFE-B 0.0 | Train Loss 13.5044
Epoch 0151 | Time 0.087 (0.080) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 13.4850
Epoch 0152 | Time 0.086 (0.080) | NFE-F 34.3 | NFE-B 0.0 | Train Loss 13.4673
Epoch 0153 | Time 0.091 (0.080) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 13.4503
Epoch 0154 | Time 0.090 (0.080) | NFE-F 34.4 | NFE-B 0.0 | Train Loss 13.4325
Epoch 0155 | Time 0.089 (0.080) | NFE-F 34.5 | NFE-B 0.0 | Train Loss 13.4140
Epoch 0156 | Time 0.093 (0.080) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 13.3961
Epoch 0157 | Time 0.087 (0.080) | NFE-F 34.6 | NFE-B 0.0 | Train Loss 13.3793
Epoch 0158 | Time 0.095 (0.081) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 13.3628
Epoch 0159 | Time 0.092 (0.081) | NFE-F 34.7 | NFE-B 0.0 | Train Loss 13.3460
Epoch 0160 | Time 0.089 (0.081) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 13.3288
Epoch 0161 | Time 0.087 (0.081) | NFE-F 34.8 | NFE-B 0.0 | Train Loss 13.3121
Epoch 0162 | Time 0.086 (0.081) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 13.2961
Epoch 0163 | Time 0.091 (0.081) | NFE-F 34.9 | NFE-B 0.0 | Train Loss 13.2803
Epoch 0164 | Time 0.092 (0.081) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 13.2643
Epoch 0165 | Time 0.090 (0.081) | NFE-F 35.0 | NFE-B 0.0 | Train Loss 13.2482
Epoch 0166 | Time 0.092 (0.081) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 13.2325
Epoch 0167 | Time 0.088 (0.081) | NFE-F 35.1 | NFE-B 0.0 | Train Loss 13.2172
Epoch 0168 | Time 0.088 (0.081) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 13.2021
Epoch 0169 | Time 0.093 (0.082) | NFE-F 35.2 | NFE-B 0.0 | Train Loss 13.1868
Epoch 0170 | Time 0.093 (0.082) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 13.1716
Epoch 0171 | Time 0.096 (0.082) | NFE-F 35.3 | NFE-B 0.0 | Train Loss 13.1568
Epoch 0172 | Time 0.094 (0.082) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 13.1422
Epoch 0173 | Time 0.088 (0.082) | NFE-F 35.4 | NFE-B 0.0 | Train Loss 13.1277
Epoch 0174 | Time 0.087 (0.082) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 13.1132
Epoch 0175 | Time 0.090 (0.082) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 13.0988
Epoch 0176 | Time 0.092 (0.082) | NFE-F 35.5 | NFE-B 0.0 | Train Loss 13.0847
Epoch 0177 | Time 0.090 (0.082) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 13.0707
Epoch 0178 | Time 0.087 (0.082) | NFE-F 35.6 | NFE-B 0.0 | Train Loss 13.0568
Epoch 0179 | Time 0.088 (0.082) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 13.0430
Epoch 0180 | Time 0.089 (0.082) | NFE-F 35.7 | NFE-B 0.0 | Train Loss 13.0293
Epoch 0181 | Time 0.086 (0.083) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 13.0157
Epoch 0182 | Time 0.085 (0.083) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 13.0024
Epoch 0183 | Time 0.088 (0.083) | NFE-F 35.8 | NFE-B 0.0 | Train Loss 12.9891
Epoch 0184 | Time 0.087 (0.083) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 12.9758
Epoch 0185 | Time 0.087 (0.083) | NFE-F 35.9 | NFE-B 0.0 | Train Loss 12.9627
Epoch 0186 | Time 0.092 (0.083) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 12.9498
Epoch 0187 | Time 0.087 (0.083) | NFE-F 36.0 | NFE-B 0.0 | Train Loss 12.9369
Epoch 0188 | Time 0.096 (0.083) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 12.9241
Epoch 0189 | Time 0.087 (0.083) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 12.9115
Epoch 0190 | Time 0.086 (0.083) | NFE-F 36.1 | NFE-B 0.0 | Train Loss 12.8989
Epoch 0191 | Time 0.088 (0.083) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 12.8865
Epoch 0192 | Time 0.087 (0.083) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 12.8741
Epoch 0193 | Time 0.085 (0.083) | NFE-F 36.2 | NFE-B 0.0 | Train Loss 12.8619
Epoch 0194 | Time 0.088 (0.083) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 12.8497
Epoch 0195 | Time 0.092 (0.083) | NFE-F 36.3 | NFE-B 0.0 | Train Loss 12.8376
Epoch 0196 | Time 0.088 (0.083) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 12.8257
Epoch 0197 | Time 0.093 (0.083) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 12.8138
Epoch 0198 | Time 0.092 (0.084) | NFE-F 36.4 | NFE-B 0.0 | Train Loss 12.8019
Epoch 0199 | Time 0.093 (0.084) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 12.7902
Epoch 0200 | Time 0.094 (0.084) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 12.7786
Epoch 0201 | Time 0.089 (0.084) | NFE-F 36.5 | NFE-B 0.0 | Train Loss 12.7671
Epoch 0202 | Time 0.088 (0.084) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 12.7556
Epoch 0203 | Time 0.090 (0.084) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 12.7442
Epoch 0204 | Time 0.091 (0.084) | NFE-F 36.6 | NFE-B 0.0 | Train Loss 12.7329
Epoch 0205 | Time 0.093 (0.084) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 12.7217
Epoch 0206 | Time 0.096 (0.084) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 12.7106
Epoch 0207 | Time 0.090 (0.084) | NFE-F 36.7 | NFE-B 0.0 | Train Loss 12.6995
Epoch 0208 | Time 0.094 (0.084) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 12.6886
Epoch 0209 | Time 0.088 (0.084) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 12.6776
Epoch 0210 | Time 0.085 (0.084) | NFE-F 36.8 | NFE-B 0.0 | Train Loss 12.6668
Epoch 0211 | Time 0.088 (0.084) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 12.6561
Epoch 0212 | Time 0.086 (0.084) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 12.6454
Epoch 0213 | Time 0.091 (0.084) | NFE-F 36.9 | NFE-B 0.0 | Train Loss 12.6347
Epoch 0214 | Time 0.091 (0.085) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 12.6242
Epoch 0215 | Time 0.094 (0.085) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 12.6137
Epoch 0216 | Time 0.097 (0.085) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 12.6033
Epoch 0217 | Time 0.090 (0.085) | NFE-F 37.0 | NFE-B 0.0 | Train Loss 12.5929
Epoch 0218 | Time 0.092 (0.085) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 12.5827
Epoch 0219 | Time 0.095 (0.085) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 12.5724
Epoch 0220 | Time 0.091 (0.085) | NFE-F 37.1 | NFE-B 0.0 | Train Loss 12.5623
Epoch 0221 | Time 0.087 (0.085) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 12.5522
Epoch 0222 | Time 0.090 (0.085) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 12.5422
Epoch 0223 | Time 0.090 (0.085) | NFE-F 37.2 | NFE-B 0.0 | Train Loss 12.5322
Epoch 0224 | Time 0.092 (0.085) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 12.5223
Epoch 0225 | Time 0.098 (0.085) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 12.5124
Epoch 0226 | Time 0.091 (0.085) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 12.5027
Epoch 0227 | Time 0.085 (0.085) | NFE-F 37.3 | NFE-B 0.0 | Train Loss 12.4929
Epoch 0228 | Time 0.090 (0.085) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 12.4833
Epoch 0229 | Time 0.088 (0.085) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 12.4736
Epoch 0230 | Time 0.089 (0.086) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 12.4641
Epoch 0231 | Time 0.092 (0.086) | NFE-F 37.4 | NFE-B 0.0 | Train Loss 12.4546
Epoch 0232 | Time 0.087 (0.086) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 12.4451
Epoch 0233 | Time 0.087 (0.086) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 12.4357
Epoch 0234 | Time 0.093 (0.086) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 12.4264
Epoch 0235 | Time 0.089 (0.086) | NFE-F 37.5 | NFE-B 0.0 | Train Loss 12.4171
Epoch 0236 | Time 0.088 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 12.4078
Epoch 0237 | Time 0.087 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 12.3986
Epoch 0238 | Time 0.090 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 12.3895
Epoch 0239 | Time 0.090 (0.086) | NFE-F 37.6 | NFE-B 0.0 | Train Loss 12.3804
Epoch 0240 | Time 0.094 (0.086) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 12.3713
Epoch 0241 | Time 0.086 (0.086) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 12.3623
Epoch 0242 | Time 0.094 (0.086) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 12.3534
Epoch 0243 | Time 0.089 (0.086) | NFE-F 37.7 | NFE-B 0.0 | Train Loss 12.3445
Epoch 0244 | Time 0.091 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 12.3356
Epoch 0245 | Time 0.091 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 12.3268
Epoch 0246 | Time 0.089 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 12.3181
Epoch 0247 | Time 0.088 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 12.3093
Epoch 0248 | Time 0.086 (0.086) | NFE-F 37.8 | NFE-B 0.0 | Train Loss 12.3007
Epoch 0249 | Time 0.091 (0.086) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 12.2920
Epoch 0250 | Time 0.089 (0.086) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 12.2834
Epoch 0251 | Time 0.089 (0.086) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 12.2749
Epoch 0252 | Time 0.087 (0.086) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 12.2664
Epoch 0253 | Time 0.093 (0.086) | NFE-F 37.9 | NFE-B 0.0 | Train Loss 12.2579
Epoch 0254 | Time 0.092 (0.086) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 12.2495
Epoch 0255 | Time 0.087 (0.086) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 12.2411
Epoch 0256 | Time 0.088 (0.086) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 12.2327
Epoch 0257 | Time 0.092 (0.086) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 12.2244
Epoch 0258 | Time 0.090 (0.087) | NFE-F 38.0 | NFE-B 0.0 | Train Loss 12.2162
Epoch 0259 | Time 0.091 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 12.2079
Epoch 0260 | Time 0.088 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 12.1997
Epoch 0261 | Time 0.091 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 12.1916
Epoch 0262 | Time 0.097 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 12.1835
Epoch 0263 | Time 0.092 (0.087) | NFE-F 38.1 | NFE-B 0.0 | Train Loss 12.1754
Epoch 0264 | Time 0.099 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 12.1673
Epoch 0265 | Time 0.089 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 12.1593
Epoch 0266 | Time 0.089 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 12.1513
Epoch 0267 | Time 0.088 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 12.1434
Epoch 0268 | Time 0.089 (0.087) | NFE-F 38.2 | NFE-B 0.0 | Train Loss 12.1355
Epoch 0269 | Time 0.088 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 12.1276
Epoch 0270 | Time 0.086 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 12.1198
Epoch 0271 | Time 0.087 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 12.1120
Epoch 0272 | Time 0.089 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 12.1042
Epoch 0273 | Time 0.089 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 12.0964
Epoch 0274 | Time 0.087 (0.087) | NFE-F 38.3 | NFE-B 0.0 | Train Loss 12.0887
Epoch 0275 | Time 0.092 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 12.0810
Epoch 0276 | Time 0.089 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 12.0734
Epoch 0277 | Time 0.092 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 12.0658
Epoch 0278 | Time 0.089 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 12.0582
Epoch 0279 | Time 0.086 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 12.0506
Epoch 0280 | Time 0.092 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 12.0431
Epoch 0281 | Time 0.087 (0.087) | NFE-F 38.4 | NFE-B 0.0 | Train Loss 12.0356
Epoch 0282 | Time 0.082 (0.087) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 12.0281
Epoch 0283 | Time 0.089 (0.087) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 12.0206
Epoch 0284 | Time 0.089 (0.087) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 12.0132
Epoch 0285 | Time 0.090 (0.087) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 12.0058
Epoch 0286 | Time 0.092 (0.087) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 11.9985
Epoch 0287 | Time 0.090 (0.087) | NFE-F 38.5 | NFE-B 0.0 | Train Loss 11.9911
Epoch 0288 | Time 0.091 (0.087) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 11.9838
Epoch 0289 | Time 0.093 (0.087) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 11.9765
Epoch 0290 | Time 0.091 (0.087) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 11.9693
Epoch 0291 | Time 0.091 (0.087) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 11.9620
Epoch 0292 | Time 0.090 (0.087) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 11.9548
Epoch 0293 | Time 0.087 (0.087) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 11.9476
Epoch 0294 | Time 0.091 (0.088) | NFE-F 38.6 | NFE-B 0.0 | Train Loss 11.9405
Epoch 0295 | Time 0.090 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 11.9333
Epoch 0296 | Time 0.089 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 11.9262
Epoch 0297 | Time 0.101 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 11.9191
Epoch 0298 | Time 0.093 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 11.9120
Epoch 0299 | Time 0.089 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 11.9050
Epoch 0300 | Time 0.085 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 11.8980
Epoch 0301 | Time 0.090 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 11.8909
Epoch 0302 | Time 0.090 (0.088) | NFE-F 38.7 | NFE-B 0.0 | Train Loss 11.8840
Epoch 0303 | Time 0.092 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 11.8770
Epoch 0304 | Time 0.089 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 11.8701
Epoch 0305 | Time 0.094 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 11.8631
Epoch 0306 | Time 0.092 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 11.8562
Epoch 0307 | Time 0.086 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 11.8494
Epoch 0308 | Time 0.095 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 11.8425
Epoch 0309 | Time 0.094 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 11.8357
Epoch 0310 | Time 0.091 (0.088) | NFE-F 38.8 | NFE-B 0.0 | Train Loss 11.8289
Epoch 0311 | Time 0.092 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.8221
Epoch 0312 | Time 0.088 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.8153
Epoch 0313 | Time 0.088 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.8085
Epoch 0314 | Time 0.088 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.8018
Epoch 0315 | Time 0.087 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.7950
Epoch 0316 | Time 0.086 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.7883
Epoch 0317 | Time 0.088 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.7816
Epoch 0318 | Time 0.085 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.7750
Epoch 0319 | Time 0.091 (0.088) | NFE-F 38.9 | NFE-B 0.0 | Train Loss 11.7683
Epoch 0320 | Time 0.092 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7617
Epoch 0321 | Time 0.087 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7551
Epoch 0322 | Time 0.088 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7484
Epoch 0323 | Time 0.088 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7419
Epoch 0324 | Time 0.088 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7353
Epoch 0325 | Time 0.089 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7287
Epoch 0326 | Time 0.091 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7222
Epoch 0327 | Time 0.090 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7157
Epoch 0328 | Time 0.091 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7092
Epoch 0329 | Time 0.091 (0.088) | NFE-F 39.0 | NFE-B 0.0 | Train Loss 11.7027
Epoch 0330 | Time 0.096 (0.088) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6962
Epoch 0331 | Time 0.109 (0.088) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6897
Epoch 0332 | Time 0.092 (0.089) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6833
Epoch 0333 | Time 0.090 (0.089) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6768
Epoch 0334 | Time 0.090 (0.089) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6704
Epoch 0335 | Time 0.090 (0.089) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6640
Epoch 0336 | Time 0.121 (0.089) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6576
Epoch 0337 | Time 0.108 (0.089) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6512
Epoch 0338 | Time 0.103 (0.089) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6448
Epoch 0339 | Time 0.122 (0.090) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6385
Epoch 0340 | Time 0.111 (0.090) | NFE-F 39.1 | NFE-B 0.0 | Train Loss 11.6322
Epoch 0341 | Time 0.093 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.6258
Epoch 0342 | Time 0.086 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.6195
Epoch 0343 | Time 0.087 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.6132
Epoch 0344 | Time 0.093 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.6069
Epoch 0345 | Time 0.091 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.6006
Epoch 0346 | Time 0.095 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.5944
Epoch 0347 | Time 0.111 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.5881
Epoch 0348 | Time 0.128 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.5819
Epoch 0349 | Time 0.092 (0.090) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.5756
Epoch 0350 | Time 0.121 (0.091) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.5694
Epoch 0351 | Time 0.106 (0.091) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.5632
Epoch 0352 | Time 0.094 (0.091) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.5570
Epoch 0353 | Time 0.091 (0.091) | NFE-F 39.2 | NFE-B 0.0 | Train Loss 11.5508
Epoch 0354 | Time 0.091 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.5446
Epoch 0355 | Time 0.086 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.5385
Epoch 0356 | Time 0.086 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.5323
Epoch 0357 | Time 0.087 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.5262
Epoch 0358 | Time 0.087 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.5200
Epoch 0359 | Time 0.088 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.5139
Epoch 0360 | Time 0.092 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.5078
Epoch 0361 | Time 0.091 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.5017
Epoch 0362 | Time 0.087 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.4956
Epoch 0363 | Time 0.088 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.4895
Epoch 0364 | Time 0.090 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.4834
Epoch 0365 | Time 0.090 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.4773
Epoch 0366 | Time 0.090 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.4713
Epoch 0367 | Time 0.120 (0.091) | NFE-F 39.3 | NFE-B 0.0 | Train Loss 11.4652
Epoch 0368 | Time 0.107 (0.091) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4592
Epoch 0369 | Time 0.089 (0.091) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4532
Epoch 0370 | Time 0.089 (0.091) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4471
Epoch 0371 | Time 0.084 (0.091) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4411
Epoch 0372 | Time 0.092 (0.091) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4351
Epoch 0373 | Time 0.125 (0.091) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4291
Epoch 0374 | Time 0.115 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4231
Epoch 0375 | Time 0.088 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4172
Epoch 0376 | Time 0.090 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4112
Epoch 0377 | Time 0.091 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.4052
Epoch 0378 | Time 0.088 (0.091) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.3993
Epoch 0379 | Time 0.095 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.3933
Epoch 0380 | Time 0.130 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.3874
Epoch 0381 | Time 0.121 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.3814
Epoch 0382 | Time 0.095 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.3755
Epoch 0383 | Time 0.089 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.3696
Epoch 0384 | Time 0.088 (0.092) | NFE-F 39.4 | NFE-B 0.0 | Train Loss 11.3637
Epoch 0385 | Time 0.089 (0.092) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3578
Epoch 0386 | Time 0.086 (0.092) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3519
Epoch 0387 | Time 0.090 (0.092) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3460
Epoch 0388 | Time 0.091 (0.092) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3401
Epoch 0389 | Time 0.121 (0.092) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3343
Epoch 0390 | Time 0.109 (0.092) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3284
Epoch 0391 | Time 0.098 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3225
Epoch 0392 | Time 0.094 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3167
Epoch 0393 | Time 0.093 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3108
Epoch 0394 | Time 0.089 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.3050
Epoch 0395 | Time 0.096 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2992
Epoch 0396 | Time 0.090 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2933
Epoch 0397 | Time 0.092 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2875
Epoch 0398 | Time 0.087 (0.092) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2817
Epoch 0399 | Time 0.101 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2759
Epoch 0400 | Time 0.132 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2701
Epoch 0401 | Time 0.104 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2643
Epoch 0402 | Time 0.094 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2585
Epoch 0403 | Time 0.087 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2527
Epoch 0404 | Time 0.088 (0.093) | NFE-F 39.5 | NFE-B 0.0 | Train Loss 11.2470
Epoch 0405 | Time 0.087 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.2412
Epoch 0406 | Time 0.089 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.2354
Epoch 0407 | Time 0.087 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.2297
Epoch 0408 | Time 0.095 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.2239
Epoch 0409 | Time 0.092 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.2182
Epoch 0410 | Time 0.088 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.2124
Epoch 0411 | Time 0.085 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.2067
Epoch 0412 | Time 0.090 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.2010
Epoch 0413 | Time 0.087 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1952
Epoch 0414 | Time 0.092 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1895
Epoch 0415 | Time 0.086 (0.093) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1838
Epoch 0416 | Time 0.089 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1781
Epoch 0417 | Time 0.087 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1724
Epoch 0418 | Time 0.087 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1667
Epoch 0419 | Time 0.089 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1610
Epoch 0420 | Time 0.084 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1553
Epoch 0421 | Time 0.088 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1496
Epoch 0422 | Time 0.086 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1440
Epoch 0423 | Time 0.090 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1383
Epoch 0424 | Time 0.093 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1326
Epoch 0425 | Time 0.093 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1270
Epoch 0426 | Time 0.086 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1213
Epoch 0427 | Time 0.086 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1157
Epoch 0428 | Time 0.086 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1100
Epoch 0429 | Time 0.090 (0.092) | NFE-F 39.6 | NFE-B 0.0 | Train Loss 11.1044
Epoch 0430 | Time 0.090 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0987
Epoch 0431 | Time 0.089 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0931
Epoch 0432 | Time 0.091 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0875
Epoch 0433 | Time 0.088 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0819
Epoch 0434 | Time 0.088 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0763
Epoch 0435 | Time 0.089 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0706
Epoch 0436 | Time 0.091 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0650
Epoch 0437 | Time 0.087 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0594
Epoch 0438 | Time 0.088 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0538
Epoch 0439 | Time 0.089 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0482
Epoch 0440 | Time 0.086 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0427
Epoch 0441 | Time 0.091 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0371
Epoch 0442 | Time 0.086 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0315
Epoch 0443 | Time 0.088 (0.092) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0259
Epoch 0444 | Time 0.087 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0204
Epoch 0445 | Time 0.089 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0148
Epoch 0446 | Time 0.089 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0092
Epoch 0447 | Time 0.092 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 11.0037
Epoch 0448 | Time 0.089 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9981
Epoch 0449 | Time 0.094 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9926
Epoch 0450 | Time 0.088 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9870
Epoch 0451 | Time 0.087 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9815
Epoch 0452 | Time 0.086 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9760
Epoch 0453 | Time 0.085 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9704
Epoch 0454 | Time 0.086 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9649
Epoch 0455 | Time 0.087 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9594
Epoch 0456 | Time 0.087 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9539
Epoch 0457 | Time 0.088 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9484
Epoch 0458 | Time 0.085 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9429
Epoch 0459 | Time 0.096 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9373
Epoch 0460 | Time 0.088 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9318
Epoch 0461 | Time 0.089 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9264
Epoch 0462 | Time 0.088 (0.091) | NFE-F 39.7 | NFE-B 0.0 | Train Loss 10.9209
Epoch 0463 | Time 0.086 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.9154
Epoch 0464 | Time 0.091 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.9099
Epoch 0465 | Time 0.090 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.9044
Epoch 0466 | Time 0.095 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8989
Epoch 0467 | Time 0.089 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8935
Epoch 0468 | Time 0.092 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8880
Epoch 0469 | Time 0.088 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8825
Epoch 0470 | Time 0.090 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8771
Epoch 0471 | Time 0.089 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8716
Epoch 0472 | Time 0.085 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8662
Epoch 0473 | Time 0.087 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8607
Epoch 0474 | Time 0.089 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8553
Epoch 0475 | Time 0.091 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8498
Epoch 0476 | Time 0.088 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8444
Epoch 0477 | Time 0.093 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8390
Epoch 0478 | Time 0.090 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8336
Epoch 0479 | Time 0.088 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8281
Epoch 0480 | Time 0.090 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8227
Epoch 0481 | Time 0.088 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8173
Epoch 0482 | Time 0.089 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8119
Epoch 0483 | Time 0.091 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8065
Epoch 0484 | Time 0.086 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.8011
Epoch 0485 | Time 0.085 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7957
Epoch 0486 | Time 0.088 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7903
Epoch 0487 | Time 0.088 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7849
Epoch 0488 | Time 0.088 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7795
Epoch 0489 | Time 0.090 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7741
Epoch 0490 | Time 0.085 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7687
Epoch 0491 | Time 0.089 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7634
Epoch 0492 | Time 0.097 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7580
Epoch 0493 | Time 0.092 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7526
Epoch 0494 | Time 0.087 (0.091) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7473
Epoch 0495 | Time 0.090 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7419
Epoch 0496 | Time 0.088 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7365
Epoch 0497 | Time 0.092 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7312
Epoch 0498 | Time 0.092 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7258
Epoch 0499 | Time 0.089 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7205
Epoch 0500 | Time 0.087 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7152
Epoch 0501 | Time 0.087 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7098
Epoch 0502 | Time 0.087 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.7045
Epoch 0503 | Time 0.083 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6992
Epoch 0504 | Time 0.089 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6938
Epoch 0505 | Time 0.090 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6885
Epoch 0506 | Time 0.087 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6832
Epoch 0507 | Time 0.084 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6779
Epoch 0508 | Time 0.086 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6726
Epoch 0509 | Time 0.085 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6673
Epoch 0510 | Time 0.086 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6619
Epoch 0511 | Time 0.088 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6566
Epoch 0512 | Time 0.086 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6513
Epoch 0513 | Time 0.087 (0.090) | NFE-F 39.8 | NFE-B 0.0 | Train Loss 10.6461
Epoch 0514 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.6408
Epoch 0515 | Time 0.096 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.6355
Epoch 0516 | Time 0.088 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.6302
Epoch 0517 | Time 0.091 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.6249
Epoch 0518 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.6196
Epoch 0519 | Time 0.089 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.6144
Epoch 0520 | Time 0.088 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.6091
Epoch 0521 | Time 0.088 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.6038
Epoch 0522 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5986
Epoch 0523 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5933
Epoch 0524 | Time 0.084 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5881
Epoch 0525 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5828
Epoch 0526 | Time 0.094 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5776
Epoch 0527 | Time 0.091 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5723
Epoch 0528 | Time 0.088 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5671
Epoch 0529 | Time 0.088 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5619
Epoch 0530 | Time 0.086 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5566
Epoch 0531 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5514
Epoch 0532 | Time 0.086 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5462
Epoch 0533 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5410
Epoch 0534 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5358
Epoch 0535 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5306
Epoch 0536 | Time 0.086 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5253
Epoch 0537 | Time 0.088 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5201
Epoch 0538 | Time 0.091 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5149
Epoch 0539 | Time 0.086 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5097
Epoch 0540 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.5045
Epoch 0541 | Time 0.086 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4994
Epoch 0542 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4942
Epoch 0543 | Time 0.091 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4890
Epoch 0544 | Time 0.088 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4838
Epoch 0545 | Time 0.092 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4786
Epoch 0546 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4735
Epoch 0547 | Time 0.086 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4683
Epoch 0548 | Time 0.092 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4631
Epoch 0549 | Time 0.095 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4580
Epoch 0550 | Time 0.089 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4528
Epoch 0551 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4477
Epoch 0552 | Time 0.096 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4425
Epoch 0553 | Time 0.092 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4374
Epoch 0554 | Time 0.089 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4322
Epoch 0555 | Time 0.092 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4271
Epoch 0556 | Time 0.094 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4219
Epoch 0557 | Time 0.091 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4168
Epoch 0558 | Time 0.091 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4117
Epoch 0559 | Time 0.089 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4066
Epoch 0560 | Time 0.113 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.4014
Epoch 0561 | Time 0.097 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3963
Epoch 0562 | Time 0.095 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3912
Epoch 0563 | Time 0.088 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3861
Epoch 0564 | Time 0.089 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3810
Epoch 0565 | Time 0.084 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3759
Epoch 0566 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3708
Epoch 0567 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3657
Epoch 0568 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3606
Epoch 0569 | Time 0.093 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3555
Epoch 0570 | Time 0.092 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3504
Epoch 0571 | Time 0.095 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3454
Epoch 0572 | Time 0.095 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3403
Epoch 0573 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3352
Epoch 0574 | Time 0.093 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3301
Epoch 0575 | Time 0.092 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3251
Epoch 0576 | Time 0.089 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3200
Epoch 0577 | Time 0.089 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3149
Epoch 0578 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3099
Epoch 0579 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.3048
Epoch 0580 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.2998
Epoch 0581 | Time 0.092 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.2947
Epoch 0582 | Time 0.092 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.2897
Epoch 0583 | Time 0.087 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.2847
Epoch 0584 | Time 0.090 (0.090) | NFE-F 39.9 | NFE-B 0.0 | Train Loss 10.2796
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(data[0])
        data[0] = scaler.transform(data[0])

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.1, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(data[0])
        data[0] = scaler.transform(data[0])

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.1, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, self.y = datasets.load_boston(return_X_y=True)
        print(self.y.shape)

        scaler = preprocessing.StandardScaler().fit(X)
        data[0] = scaler.transform(data[0])

        self.x = data[0][:self.n_examples, :]
        self.y = data[1][:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        data = datasets.load_boston(return_X_y=True)

        self.x = data[0][self.n_examples:, :]
        self.y = data[1][self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.1, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, self.y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, self.y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.1, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.1, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.047 (0.047) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 431.4239
Epoch 0001 | Time 0.089 (0.047) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 3090.5476
Epoch 0002 | Time 0.084 (0.048) | NFE-F 14.5 | NFE-B 0.0 | Train Loss 3596.3225
Epoch 0003 | Time 0.083 (0.048) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 1649.2513
Epoch 0004 | Time 0.087 (0.048) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 349.7791
Epoch 0005 | Time 0.086 (0.049) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 780.7834
Epoch 0006 | Time 0.086 (0.049) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 1751.8783
Epoch 0007 | Time 0.087 (0.049) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 1829.0812
Epoch 0008 | Time 0.091 (0.050) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 1050.7603
Epoch 0009 | Time 0.092 (0.050) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 309.0150
Epoch 0010 | Time 0.085 (0.051) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 236.9787
Epoch 0011 | Time 0.086 (0.051) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 689.1999
Epoch 0012 | Time 0.086 (0.051) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 1027.7334
Epoch 0013 | Time 0.090 (0.052) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 875.8964
Epoch 0014 | Time 0.087 (0.052) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 429.5528
Epoch 0015 | Time 0.088 (0.052) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 127.6650
Epoch 0016 | Time 0.088 (0.053) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 193.7731
Epoch 0017 | Time 0.088 (0.053) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 459.0784
Epoch 0018 | Time 0.085 (0.053) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 589.4785
Epoch 0019 | Time 0.086 (0.054) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 450.0569
Epoch 0020 | Time 0.089 (0.054) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 200.1974
Epoch 0021 | Time 0.090 (0.055) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 81.4621
Epoch 0022 | Time 0.088 (0.055) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 167.2093
Epoch 0023 | Time 0.086 (0.055) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 313.9353
Epoch 0024 | Time 0.090 (0.056) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 340.6650
Epoch 0025 | Time 0.088 (0.056) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 222.4507
Epoch 0026 | Time 0.093 (0.056) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 89.3753
Epoch 0027 | Time 0.096 (0.057) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 66.5696
Epoch 0028 | Time 0.089 (0.057) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 144.3323
Epoch 0029 | Time 0.091 (0.057) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 208.1235
Epoch 0030 | Time 0.091 (0.058) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 178.0983
Epoch 0031 | Time 0.092 (0.058) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 90.4247
Epoch 0032 | Time 0.086 (0.058) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 39.2146
Epoch 0033 | Time 0.087 (0.059) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 64.1660
Epoch 0034 | Time 0.087 (0.059) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 114.4672
Epoch 0035 | Time 0.087 (0.059) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 119.7133
Epoch 0036 | Time 0.088 (0.059) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 73.2621
Epoch 0037 | Time 0.098 (0.060) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 29.7935
Epoch 0038 | Time 0.086 (0.060) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 32.4786
Epoch 0039 | Time 0.084 (0.060) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 64.2273
Epoch 0040 | Time 0.090 (0.061) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 76.9912
Epoch 0041 | Time 0.087 (0.061) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 53.9252
Epoch 0042 | Time 0.091 (0.061) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 24.7199
Epoch 0043 | Time 0.090 (0.061) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 22.1976
Epoch 0044 | Time 0.091 (0.062) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 41.5521
Epoch 0045 | Time 0.087 (0.062) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 52.0509
Epoch 0046 | Time 0.085 (0.062) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 39.4393
Epoch 0047 | Time 0.088 (0.062) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 21.2422
Epoch 0048 | Time 0.090 (0.063) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 19.4348
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.048 (0.048) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 2545.4380
Epoch 0001 | Time 0.074 (0.049) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 1803.5424
Epoch 0002 | Time 0.075 (0.049) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 1197.1371
Epoch 0003 | Time 0.095 (0.049) | NFE-F 14.5 | NFE-B 0.0 | Train Loss 751.6193
Epoch 0004 | Time 0.094 (0.050) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 452.1495
Epoch 0005 | Time 0.095 (0.050) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 270.3991
Epoch 0006 | Time 0.095 (0.051) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 188.2127
Epoch 0007 | Time 0.093 (0.051) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 184.8852
Epoch 0008 | Time 0.088 (0.051) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 236.0090
Epoch 0009 | Time 0.092 (0.052) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 315.7709
Epoch 0010 | Time 0.096 (0.052) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 400.3388
Epoch 0011 | Time 0.093 (0.053) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 470.9984
Epoch 0012 | Time 0.094 (0.053) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 515.9412
Epoch 0013 | Time 0.094 (0.053) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 530.3951
Epoch 0014 | Time 0.091 (0.054) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 515.4871
Epoch 0015 | Time 0.094 (0.054) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 476.4977
Epoch 0016 | Time 0.091 (0.055) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 421.0557
Epoch 0017 | Time 0.091 (0.055) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 357.5752
Epoch 0018 | Time 0.095 (0.055) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 294.0389
Epoch 0019 | Time 0.090 (0.056) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 237.1234
Epoch 0020 | Time 0.097 (0.056) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 191.6321
Epoch 0021 | Time 0.094 (0.057) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 160.2099
Epoch 0022 | Time 0.091 (0.057) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 143.3208
Epoch 0023 | Time 0.090 (0.057) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 139.4762
Epoch 0024 | Time 0.087 (0.058) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 145.6825
Epoch 0025 | Time 0.091 (0.058) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 158.0517
Epoch 0026 | Time 0.095 (0.058) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 172.4798
Epoch 0027 | Time 0.093 (0.059) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 185.2855
Epoch 0028 | Time 0.091 (0.059) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 193.7046
Epoch 0029 | Time 0.090 (0.059) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 196.1731
Epoch 0030 | Time 0.087 (0.059) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 192.3762
Epoch 0031 | Time 0.097 (0.060) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 183.0932
Epoch 0032 | Time 0.091 (0.060) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 169.8954
Epoch 0033 | Time 0.095 (0.061) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 154.7697
Epoch 0034 | Time 0.092 (0.061) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 139.7380
Epoch 0035 | Time 0.090 (0.061) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 126.5296
Epoch 0036 | Time 0.096 (0.061) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 116.3460
Epoch 0037 | Time 0.087 (0.062) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 109.7424
Epoch 0038 | Time 0.093 (0.062) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 106.6285
Epoch 0039 | Time 0.092 (0.062) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 106.3738
Epoch 0040 | Time 0.090 (0.063) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 107.9913
Epoch 0041 | Time 0.100 (0.063) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 110.3565
Epoch 0042 | Time 0.093 (0.063) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 112.4223
Epoch 0043 | Time 0.092 (0.064) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 113.3902
Epoch 0044 | Time 0.093 (0.064) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 112.8134
Epoch 0045 | Time 0.094 (0.064) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 110.6209
Epoch 0046 | Time 0.091 (0.064) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 107.0711
Epoch 0047 | Time 0.096 (0.065) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 102.6512
Epoch 0048 | Time 0.093 (0.065) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 97.9502
Epoch 0049 | Time 0.094 (0.065) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 93.5328
Epoch 0050 | Time 0.089 (0.066) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 89.8356
Epoch 0051 | Time 0.086 (0.066) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 87.1031
Epoch 0052 | Time 0.092 (0.066) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 85.3699
Epoch 0053 | Time 0.084 (0.066) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 84.4855
Epoch 0054 | Time 0.090 (0.066) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 84.1711
Epoch 0055 | Time 0.090 (0.067) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 84.0944
Epoch 0056 | Time 0.088 (0.067) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 83.9426
Epoch 0057 | Time 0.089 (0.067) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 83.4821
Epoch 0058 | Time 0.089 (0.067) | NFE-F 25.3 | NFE-B 0.0 | Train Loss 82.5924
Epoch 0059 | Time 0.088 (0.068) | NFE-F 25.5 | NFE-B 0.0 | Train Loss 81.2717
Epoch 0060 | Time 0.088 (0.068) | NFE-F 25.6 | NFE-B 0.0 | Train Loss 79.6184
Epoch 0061 | Time 0.094 (0.068) | NFE-F 25.8 | NFE-B 0.0 | Train Loss 77.7933
Epoch 0062 | Time 0.086 (0.068) | NFE-F 25.9 | NFE-B 0.0 | Train Loss 75.9757
Epoch 0063 | Time 0.086 (0.068) | NFE-F 26.1 | NFE-B 0.0 | Train Loss 74.3221
Epoch 0064 | Time 0.093 (0.069) | NFE-F 26.2 | NFE-B 0.0 | Train Loss 72.9359
Epoch 0065 | Time 0.088 (0.069) | NFE-F 26.3 | NFE-B 0.0 | Train Loss 71.8532
Epoch 0066 | Time 0.086 (0.069) | NFE-F 26.5 | NFE-B 0.0 | Train Loss 71.0459
Epoch 0067 | Time 0.092 (0.069) | NFE-F 26.6 | NFE-B 0.0 | Train Loss 70.4372
Epoch 0068 | Time 0.095 (0.069) | NFE-F 26.7 | NFE-B 0.0 | Train Loss 69.9263
Epoch 0069 | Time 0.096 (0.070) | NFE-F 26.9 | NFE-B 0.0 | Train Loss 69.4140
Epoch 0070 | Time 0.093 (0.070) | NFE-F 27.0 | NFE-B 0.0 | Train Loss 68.8244
Epoch 0071 | Time 0.089 (0.070) | NFE-F 27.1 | NFE-B 0.0 | Train Loss 68.1176
Epoch 0072 | Time 0.090 (0.070) | NFE-F 27.3 | NFE-B 0.0 | Train Loss 67.2924
Epoch 0073 | Time 0.087 (0.071) | NFE-F 27.4 | NFE-B 0.0 | Train Loss 66.3803
Epoch 0074 | Time 0.087 (0.071) | NFE-F 27.5 | NFE-B 0.0 | Train Loss 65.4322
Epoch 0075 | Time 0.089 (0.071) | NFE-F 27.7 | NFE-B 0.0 | Train Loss 64.5034
Epoch 0076 | Time 0.088 (0.071) | NFE-F 27.8 | NFE-B 0.0 | Train Loss 63.6397
Epoch 0077 | Time 0.088 (0.071) | NFE-F 27.9 | NFE-B 0.0 | Train Loss 62.8680
Epoch 0078 | Time 0.092 (0.071) | NFE-F 28.0 | NFE-B 0.0 | Train Loss 62.1930
Epoch 0079 | Time 0.090 (0.072) | NFE-F 28.1 | NFE-B 0.0 | Train Loss 61.5993
Epoch 0080 | Time 0.097 (0.072) | NFE-F 28.3 | NFE-B 0.0 | Train Loss 61.0586
Epoch 0081 | Time 0.091 (0.072) | NFE-F 28.4 | NFE-B 0.0 | Train Loss 60.5379
Epoch 0082 | Time 0.089 (0.072) | NFE-F 28.5 | NFE-B 0.0 | Train Loss 60.0087
Epoch 0083 | Time 0.089 (0.072) | NFE-F 28.6 | NFE-B 0.0 | Train Loss 59.4525
Epoch 0084 | Time 0.091 (0.073) | NFE-F 28.7 | NFE-B 0.0 | Train Loss 58.8637
Epoch 0085 | Time 0.092 (0.073) | NFE-F 28.8 | NFE-B 0.0 | Train Loss 58.2487
Epoch 0086 | Time 0.097 (0.073) | NFE-F 28.9 | NFE-B 0.0 | Train Loss 57.6221
Epoch 0087 | Time 0.090 (0.073) | NFE-F 29.1 | NFE-B 0.0 | Train Loss 57.0016
Epoch 0088 | Time 0.091 (0.073) | NFE-F 29.2 | NFE-B 0.0 | Train Loss 56.4027
Epoch 0089 | Time 0.090 (0.074) | NFE-F 29.3 | NFE-B 0.0 | Train Loss 55.8354
Epoch 0090 | Time 0.085 (0.074) | NFE-F 29.4 | NFE-B 0.0 | Train Loss 55.3024
Epoch 0091 | Time 0.090 (0.074) | NFE-F 29.5 | NFE-B 0.0 | Train Loss 54.7992
Epoch 0092 | Time 0.085 (0.074) | NFE-F 29.6 | NFE-B 0.0 | Train Loss 54.3168
Epoch 0093 | Time 0.089 (0.074) | NFE-F 29.7 | NFE-B 0.0 | Train Loss 53.8446
Epoch 0094 | Time 0.088 (0.074) | NFE-F 29.8 | NFE-B 0.0 | Train Loss 53.3731
Epoch 0095 | Time 0.088 (0.074) | NFE-F 29.9 | NFE-B 0.0 | Train Loss 52.8964
Epoch 0096 | Time 0.086 (0.074) | NFE-F 30.0 | NFE-B 0.0 | Train Loss 52.4129
Epoch 0097 | Time 0.089 (0.075) | NFE-F 30.1 | NFE-B 0.0 | Train Loss 51.9251
Epoch 0098 | Time 0.089 (0.075) | NFE-F 30.2 | NFE-B 0.0 | Train Loss 51.4378
Epoch 0099 | Time 0.091 (0.075) | NFE-F 30.3 | NFE-B 0.0 | Train Loss 50.9568
Epoch 0100 | Time 0.088 (0.075) | NFE-F 30.4 | NFE-B 0.0 | Train Loss 50.4869
Epoch 0101 | Time 0.087 (0.075) | NFE-F 30.5 | NFE-B 0.0 | Train Loss 50.0307
Epoch 0102 | Time 0.090 (0.075) | NFE-F 30.6 | NFE-B 0.0 | Train Loss 49.5885
Epoch 0103 | Time 0.088 (0.075) | NFE-F 30.7 | NFE-B 0.0 | Train Loss 49.1582
Epoch 0104 | Time 0.087 (0.076) | NFE-F 30.8 | NFE-B 0.0 | Train Loss 48.7363
Epoch 0105 | Time 0.088 (0.076) | NFE-F 30.9 | NFE-B 0.0 | Train Loss 48.3195
Epoch 0106 | Time 0.093 (0.076) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 47.9050
Epoch 0107 | Time 0.089 (0.076) | NFE-F 31.0 | NFE-B 0.0 | Train Loss 47.4914
Epoch 0108 | Time 0.095 (0.076) | NFE-F 31.1 | NFE-B 0.0 | Train Loss 47.0788
Epoch 0109 | Time 0.088 (0.076) | NFE-F 31.2 | NFE-B 0.0 | Train Loss 46.6686
Epoch 0110 | Time 0.089 (0.076) | NFE-F 31.3 | NFE-B 0.0 | Train Loss 46.2626
Epoch 0111 | Time 0.084 (0.077) | NFE-F 31.4 | NFE-B 0.0 | Train Loss 45.8625
Epoch 0112 | Time 0.087 (0.077) | NFE-F 31.5 | NFE-B 0.0 | Train Loss 45.4696
Epoch 0113 | Time 0.092 (0.077) | NFE-F 31.6 | NFE-B 0.0 | Train Loss 45.0842
Epoch 0114 | Time 0.091 (0.077) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 44.7059
Epoch 0115 | Time 0.087 (0.077) | NFE-F 31.7 | NFE-B 0.0 | Train Loss 44.3335
Epoch 0116 | Time 0.090 (0.077) | NFE-F 31.8 | NFE-B 0.0 | Train Loss 43.9660
Epoch 0117 | Time 0.090 (0.077) | NFE-F 31.9 | NFE-B 0.0 | Train Loss 43.6021
Epoch 0118 | Time 0.101 (0.078) | NFE-F 32.0 | NFE-B 0.0 | Train Loss 43.2414
Epoch 0119 | Time 0.094 (0.078) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 42.8837
Epoch 0120 | Time 0.088 (0.078) | NFE-F 32.1 | NFE-B 0.0 | Train Loss 42.5295
Epoch 0121 | Time 0.087 (0.078) | NFE-F 32.2 | NFE-B 0.0 | Train Loss 42.1792
Epoch 0122 | Time 0.085 (0.078) | NFE-F 32.3 | NFE-B 0.0 | Train Loss 41.8336
Epoch 0123 | Time 0.088 (0.078) | NFE-F 32.4 | NFE-B 0.0 | Train Loss 41.4931
Epoch 0124 | Time 0.086 (0.078) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 41.1576
Epoch 0125 | Time 0.087 (0.078) | NFE-F 32.5 | NFE-B 0.0 | Train Loss 40.8272
Epoch 0126 | Time 0.088 (0.078) | NFE-F 32.6 | NFE-B 0.0 | Train Loss 40.5013
Epoch 0127 | Time 0.093 (0.078) | NFE-F 32.7 | NFE-B 0.0 | Train Loss 40.1796
Epoch 0128 | Time 0.094 (0.079) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 39.8617
Epoch 0129 | Time 0.095 (0.079) | NFE-F 32.8 | NFE-B 0.0 | Train Loss 39.5473
Epoch 0130 | Time 0.094 (0.079) | NFE-F 32.9 | NFE-B 0.0 | Train Loss 39.2364
Epoch 0131 | Time 0.087 (0.079) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 38.9292
Epoch 0132 | Time 0.090 (0.079) | NFE-F 33.0 | NFE-B 0.0 | Train Loss 38.6257
Epoch 0133 | Time 0.089 (0.079) | NFE-F 33.1 | NFE-B 0.0 | Train Loss 38.3263
Epoch 0134 | Time 0.091 (0.079) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 38.0309
Epoch 0135 | Time 0.088 (0.079) | NFE-F 33.2 | NFE-B 0.0 | Train Loss 37.7397
Epoch 0136 | Time 0.091 (0.080) | NFE-F 33.3 | NFE-B 0.0 | Train Loss 37.4524
Epoch 0137 | Time 0.089 (0.080) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 37.1689
Epoch 0138 | Time 0.090 (0.080) | NFE-F 33.4 | NFE-B 0.0 | Train Loss 36.8891
Epoch 0139 | Time 0.092 (0.080) | NFE-F 33.5 | NFE-B 0.0 | Train Loss 36.6128
Epoch 0140 | Time 0.090 (0.080) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 36.3398
Epoch 0141 | Time 0.091 (0.080) | NFE-F 33.6 | NFE-B 0.0 | Train Loss 36.0702
Epoch 0142 | Time 0.090 (0.080) | NFE-F 33.7 | NFE-B 0.0 | Train Loss 35.8041
Epoch 0143 | Time 0.089 (0.080) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 35.5414
Epoch 0144 | Time 0.089 (0.080) | NFE-F 33.8 | NFE-B 0.0 | Train Loss 35.2821
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=True, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.137 (0.137) | NFE-F 14.0 | NFE-B 21.0 | Train Loss 9903.9014
Epoch 0001 | Time 0.157 (0.137) | NFE-F 14.1 | NFE-B 21.0 | Train Loss 8329.0352
Epoch 0002 | Time 0.143 (0.137) | NFE-F 14.3 | NFE-B 21.0 | Train Loss 6879.3237
Epoch 0003 | Time 0.162 (0.138) | NFE-F 14.5 | NFE-B 21.1 | Train Loss 5616.9932
Epoch 0004 | Time 0.170 (0.138) | NFE-F 14.8 | NFE-B 21.1 | Train Loss 4533.1128
Epoch 0005 | Time 0.164 (0.138) | NFE-F 15.0 | NFE-B 21.2 | Train Loss 3587.6733
Epoch 0006 | Time 0.159 (0.138) | NFE-F 15.3 | NFE-B 21.2 | Train Loss 2775.3435
Epoch 0007 | Time 0.185 (0.139) | NFE-F 15.5 | NFE-B 21.4 | Train Loss 2090.9207
Epoch 0008 | Time 0.180 (0.139) | NFE-F 15.8 | NFE-B 21.5 | Train Loss 1528.7151
Epoch 0009 | Time 0.182 (0.140) | NFE-F 16.0 | NFE-B 21.6 | Train Loss 1082.0280
Epoch 0010 | Time 0.199 (0.140) | NFE-F 16.3 | NFE-B 21.8 | Train Loss 742.5571
Epoch 0011 | Time 0.196 (0.141) | NFE-F 16.5 | NFE-B 21.9 | Train Loss 500.3907
Epoch 0012 | Time 0.205 (0.142) | NFE-F 16.7 | NFE-B 22.1 | Train Loss 344.1437
Epoch 0013 | Time 0.205 (0.142) | NFE-F 17.0 | NFE-B 22.3 | Train Loss 261.2145
Epoch 0014 | Time 0.205 (0.143) | NFE-F 17.2 | NFE-B 22.4 | Train Loss 238.1588
Epoch 0015 | Time 0.196 (0.143) | NFE-F 17.4 | NFE-B 22.6 | Train Loss 261.1693
Epoch 0016 | Time 0.196 (0.144) | NFE-F 17.7 | NFE-B 22.8 | Train Loss 316.6254
Epoch 0017 | Time 0.205 (0.144) | NFE-F 17.9 | NFE-B 22.9 | Train Loss 391.6656
Epoch 0018 | Time 0.202 (0.145) | NFE-F 18.1 | NFE-B 23.1 | Train Loss 474.7232
Epoch 0019 | Time 0.213 (0.146) | NFE-F 18.3 | NFE-B 23.3 | Train Loss 555.9674
Epoch 0020 | Time 0.195 (0.146) | NFE-F 18.5 | NFE-B 23.4 | Train Loss 627.6110
Epoch 0021 | Time 0.209 (0.147) | NFE-F 18.7 | NFE-B 23.6 | Train Loss 684.0536
Epoch 0022 | Time 0.203 (0.147) | NFE-F 19.0 | NFE-B 23.7 | Train Loss 721.8697
Epoch 0023 | Time 0.208 (0.148) | NFE-F 19.2 | NFE-B 23.9 | Train Loss 739.6506
Epoch 0024 | Time 0.197 (0.148) | NFE-F 19.4 | NFE-B 24.0 | Train Loss 737.7410
Epoch 0025 | Time 0.198 (0.149) | NFE-F 19.6 | NFE-B 24.2 | Train Loss 717.9037
Epoch 0026 | Time 0.200 (0.149) | NFE-F 19.8 | NFE-B 24.3 | Train Loss 682.9486
Epoch 0027 | Time 0.199 (0.150) | NFE-F 20.0 | NFE-B 24.5 | Train Loss 636.3619
Epoch 0028 | Time 0.200 (0.150) | NFE-F 20.2 | NFE-B 24.6 | Train Loss 581.9537
Epoch 0029 | Time 0.201 (0.151) | NFE-F 20.4 | NFE-B 24.8 | Train Loss 523.5499
Epoch 0030 | Time 0.204 (0.151) | NFE-F 20.6 | NFE-B 24.9 | Train Loss 464.7296
Epoch 0031 | Time 0.210 (0.152) | NFE-F 20.8 | NFE-B 25.0 | Train Loss 408.6227
Epoch 0032 | Time 0.206 (0.153) | NFE-F 21.0 | NFE-B 25.2 | Train Loss 357.7658
Epoch 0033 | Time 0.204 (0.153) | NFE-F 21.2 | NFE-B 25.3 | Train Loss 314.0154
Epoch 0034 | Time 0.196 (0.154) | NFE-F 21.4 | NFE-B 25.5 | Train Loss 278.5159
Epoch 0035 | Time 0.202 (0.154) | NFE-F 21.5 | NFE-B 25.6 | Train Loss 251.7194
Epoch 0036 | Time 0.203 (0.155) | NFE-F 21.7 | NFE-B 25.7 | Train Loss 233.4479
Epoch 0037 | Time 0.199 (0.155) | NFE-F 21.9 | NFE-B 25.9 | Train Loss 222.9895
Epoch 0038 | Time 0.200 (0.155) | NFE-F 22.1 | NFE-B 26.0 | Train Loss 219.2205
Epoch 0039 | Time 0.201 (0.156) | NFE-F 22.3 | NFE-B 26.1 | Train Loss 220.7421
Epoch 0040 | Time 0.200 (0.156) | NFE-F 22.4 | NFE-B 26.2 | Train Loss 226.0184
Epoch 0041 | Time 0.195 (0.157) | NFE-F 22.6 | NFE-B 26.4 | Train Loss 233.5089
Epoch 0042 | Time 0.202 (0.157) | NFE-F 22.8 | NFE-B 26.5 | Train Loss 241.7836
Epoch 0043 | Time 0.195 (0.158) | NFE-F 23.0 | NFE-B 26.6 | Train Loss 249.6140
Epoch 0044 | Time 0.195 (0.158) | NFE-F 23.1 | NFE-B 26.8 | Train Loss 256.0377
Epoch 0045 | Time 0.198 (0.158) | NFE-F 23.3 | NFE-B 26.9 | Train Loss 260.3914
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=True, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.114 (0.114) | NFE-F 14.0 | NFE-B 21.0 | Train Loss 3999.5828
Epoch 0001 | Time 0.142 (0.115) | NFE-F 14.1 | NFE-B 21.0 | Train Loss 3900.6785
Epoch 0002 | Time 0.148 (0.115) | NFE-F 14.3 | NFE-B 21.0 | Train Loss 3802.9688
Epoch 0003 | Time 0.143 (0.115) | NFE-F 14.4 | NFE-B 21.0 | Train Loss 3706.3660
Epoch 0004 | Time 0.149 (0.115) | NFE-F 14.6 | NFE-B 21.0 | Train Loss 3610.7859
Epoch 0005 | Time 0.156 (0.116) | NFE-F 14.7 | NFE-B 21.0 | Train Loss 3516.1611
Epoch 0006 | Time 0.155 (0.116) | NFE-F 14.8 | NFE-B 21.0 | Train Loss 3422.4226
Epoch 0007 | Time 0.153 (0.117) | NFE-F 15.0 | NFE-B 21.0 | Train Loss 3329.5103
Epoch 0008 | Time 0.159 (0.117) | NFE-F 15.1 | NFE-B 21.0 | Train Loss 3237.3730
Epoch 0009 | Time 0.144 (0.117) | NFE-F 15.2 | NFE-B 21.0 | Train Loss 3145.9541
Epoch 0010 | Time 0.149 (0.118) | NFE-F 15.3 | NFE-B 21.0 | Train Loss 3055.2090
Epoch 0011 | Time 0.143 (0.118) | NFE-F 15.5 | NFE-B 21.0 | Train Loss 2965.1089
Epoch 0012 | Time 0.143 (0.118) | NFE-F 15.6 | NFE-B 21.0 | Train Loss 2875.6514
Epoch 0013 | Time 0.148 (0.118) | NFE-F 15.7 | NFE-B 21.0 | Train Loss 2786.8862
Epoch 0014 | Time 0.142 (0.119) | NFE-F 15.8 | NFE-B 21.0 | Train Loss 2698.9065
Epoch 0015 | Time 0.148 (0.119) | NFE-F 16.0 | NFE-B 21.0 | Train Loss 2611.8867
Epoch 0016 | Time 0.148 (0.119) | NFE-F 16.1 | NFE-B 21.0 | Train Loss 2526.1006
Epoch 0017 | Time 0.151 (0.120) | NFE-F 16.3 | NFE-B 21.0 | Train Loss 2441.8865
Epoch 0018 | Time 0.168 (0.120) | NFE-F 16.6 | NFE-B 21.1 | Train Loss 2359.6494
Epoch 0019 | Time 0.165 (0.121) | NFE-F 16.8 | NFE-B 21.1 | Train Loss 2279.8008
Epoch 0020 | Time 0.171 (0.121) | NFE-F 17.0 | NFE-B 21.2 | Train Loss 2202.6897
Epoch 0021 | Time 0.171 (0.122) | NFE-F 17.3 | NFE-B 21.2 | Train Loss 2128.5303
Epoch 0022 | Time 0.172 (0.122) | NFE-F 17.5 | NFE-B 21.3 | Train Loss 2057.3909
Epoch 0023 | Time 0.164 (0.122) | NFE-F 17.7 | NFE-B 21.4 | Train Loss 1989.1842
Epoch 0024 | Time 0.151 (0.123) | NFE-F 17.9 | NFE-B 21.3 | Train Loss 1923.7086
Epoch 0025 | Time 0.185 (0.123) | NFE-F 18.1 | NFE-B 21.5 | Train Loss 1860.7151
Epoch 0026 | Time 0.181 (0.124) | NFE-F 18.4 | NFE-B 21.5 | Train Loss 1799.9525
Epoch 0027 | Time 0.173 (0.124) | NFE-F 18.6 | NFE-B 21.6 | Train Loss 1741.2002
Epoch 0028 | Time 0.167 (0.125) | NFE-F 18.8 | NFE-B 21.6 | Train Loss 1684.2833
Epoch 0029 | Time 0.166 (0.125) | NFE-F 19.0 | NFE-B 21.7 | Train Loss 1629.0681
Epoch 0030 | Time 0.168 (0.126) | NFE-F 19.2 | NFE-B 21.7 | Train Loss 1575.4594
Epoch 0031 | Time 0.173 (0.126) | NFE-F 19.4 | NFE-B 21.8 | Train Loss 1523.3892
Epoch 0032 | Time 0.166 (0.127) | NFE-F 19.6 | NFE-B 21.8 | Train Loss 1472.8115
Epoch 0033 | Time 0.192 (0.127) | NFE-F 19.8 | NFE-B 21.9 | Train Loss 1423.6919
Epoch 0034 | Time 0.172 (0.128) | NFE-F 20.0 | NFE-B 21.9 | Train Loss 1376.0022
Epoch 0035 | Time 0.169 (0.128) | NFE-F 20.2 | NFE-B 22.0 | Train Loss 1329.7162
Epoch 0036 | Time 0.170 (0.128) | NFE-F 20.4 | NFE-B 22.0 | Train Loss 1284.8080
Epoch 0037 | Time 0.166 (0.129) | NFE-F 20.6 | NFE-B 22.1 | Train Loss 1241.2500
Epoch 0038 | Time 0.166 (0.129) | NFE-F 20.8 | NFE-B 22.1 | Train Loss 1199.0157
Epoch 0039 | Time 0.183 (0.130) | NFE-F 21.0 | NFE-B 22.2 | Train Loss 1158.0774
Epoch 0040 | Time 0.168 (0.130) | NFE-F 21.2 | NFE-B 22.2 | Train Loss 1118.4095
Epoch 0041 | Time 0.168 (0.131) | NFE-F 21.4 | NFE-B 22.3 | Train Loss 1079.9863
Epoch 0042 | Time 0.167 (0.131) | NFE-F 21.6 | NFE-B 22.3 | Train Loss 1042.7825
Epoch 0043 | Time 0.165 (0.131) | NFE-F 21.8 | NFE-B 22.4 | Train Loss 1006.7739
Epoch 0044 | Time 0.164 (0.132) | NFE-F 21.9 | NFE-B 22.4 | Train Loss 971.9359
Epoch 0045 | Time 0.172 (0.132) | NFE-F 22.1 | NFE-B 22.5 | Train Loss 938.2449
Epoch 0046 | Time 0.179 (0.132) | NFE-F 22.3 | NFE-B 22.5 | Train Loss 905.6766
Epoch 0047 | Time 0.177 (0.133) | NFE-F 22.5 | NFE-B 22.6 | Train Loss 874.2073
Epoch 0048 | Time 0.181 (0.133) | NFE-F 22.7 | NFE-B 22.6 | Train Loss 843.8131
Epoch 0049 | Time 0.167 (0.134) | NFE-F 22.8 | NFE-B 22.7 | Train Loss 814.4703
Epoch 0050 | Time 0.167 (0.134) | NFE-F 23.0 | NFE-B 22.7 | Train Loss 786.1548
Epoch 0051 | Time 0.175 (0.134) | NFE-F 23.2 | NFE-B 22.7 | Train Loss 758.8432
Epoch 0052 | Time 0.169 (0.135) | NFE-F 23.3 | NFE-B 22.8 | Train Loss 732.5110
Epoch 0053 | Time 0.170 (0.135) | NFE-F 23.5 | NFE-B 22.8 | Train Loss 707.1355
Epoch 0054 | Time 0.187 (0.136) | NFE-F 23.7 | NFE-B 22.9 | Train Loss 682.6922
Epoch 0055 | Time 0.186 (0.136) | NFE-F 23.8 | NFE-B 23.0 | Train Loss 659.1581
Epoch 0056 | Time 0.188 (0.137) | NFE-F 24.0 | NFE-B 23.1 | Train Loss 636.5095
Epoch 0057 | Time 0.185 (0.137) | NFE-F 24.2 | NFE-B 23.2 | Train Loss 614.7228
Epoch 0058 | Time 0.166 (0.137) | NFE-F 24.3 | NFE-B 23.3 | Train Loss 593.7752
Epoch 0059 | Time 0.174 (0.138) | NFE-F 24.5 | NFE-B 23.3 | Train Loss 573.6438
Epoch 0060 | Time 0.169 (0.138) | NFE-F 24.6 | NFE-B 23.3 | Train Loss 554.3057
Epoch 0061 | Time 0.168 (0.138) | NFE-F 24.8 | NFE-B 23.4 | Train Loss 535.7383
Epoch 0062 | Time 0.170 (0.139) | NFE-F 24.9 | NFE-B 23.4 | Train Loss 517.9191
Epoch 0063 | Time 0.171 (0.139) | NFE-F 25.1 | NFE-B 23.4 | Train Loss 500.8260
Epoch 0064 | Time 0.164 (0.139) | NFE-F 25.2 | NFE-B 23.5 | Train Loss 484.4373
Epoch 0065 | Time 0.166 (0.140) | NFE-F 25.4 | NFE-B 23.5 | Train Loss 468.7315
Epoch 0066 | Time 0.169 (0.140) | NFE-F 25.5 | NFE-B 23.5 | Train Loss 453.6870
Epoch 0067 | Time 0.164 (0.140) | NFE-F 25.7 | NFE-B 23.6 | Train Loss 439.2833
Epoch 0068 | Time 0.174 (0.140) | NFE-F 25.8 | NFE-B 23.6 | Train Loss 425.4996
Epoch 0069 | Time 0.166 (0.141) | NFE-F 26.0 | NFE-B 23.7 | Train Loss 412.3153
Epoch 0070 | Time 0.176 (0.141) | NFE-F 26.1 | NFE-B 23.7 | Train Loss 399.7108
Epoch 0071 | Time 0.170 (0.141) | NFE-F 26.2 | NFE-B 23.7 | Train Loss 387.6661
Epoch 0072 | Time 0.184 (0.142) | NFE-F 26.4 | NFE-B 23.8 | Train Loss 376.1622
Epoch 0073 | Time 0.163 (0.142) | NFE-F 26.5 | NFE-B 23.8 | Train Loss 365.1801
Epoch 0074 | Time 0.175 (0.142) | NFE-F 26.6 | NFE-B 23.8 | Train Loss 354.7011
Epoch 0075 | Time 0.166 (0.143) | NFE-F 26.8 | NFE-B 23.8 | Train Loss 344.7073
Epoch 0076 | Time 0.170 (0.143) | NFE-F 26.9 | NFE-B 23.9 | Train Loss 335.1808
Epoch 0077 | Time 0.169 (0.143) | NFE-F 27.0 | NFE-B 23.9 | Train Loss 326.1043
Epoch 0078 | Time 0.171 (0.143) | NFE-F 27.2 | NFE-B 23.9 | Train Loss 317.4607
Epoch 0079 | Time 0.171 (0.144) | NFE-F 27.3 | NFE-B 24.0 | Train Loss 309.2336
Epoch 0080 | Time 0.172 (0.144) | NFE-F 27.4 | NFE-B 24.0 | Train Loss 301.4066
Epoch 0081 | Time 0.167 (0.144) | NFE-F 27.6 | NFE-B 24.0 | Train Loss 293.9640
Epoch 0082 | Time 0.166 (0.144) | NFE-F 27.7 | NFE-B 24.1 | Train Loss 286.8905
Epoch 0083 | Time 0.168 (0.145) | NFE-F 27.8 | NFE-B 24.1 | Train Loss 280.1711
Epoch 0084 | Time 0.166 (0.145) | NFE-F 27.9 | NFE-B 24.1 | Train Loss 273.7912
Epoch 0085 | Time 0.166 (0.145) | NFE-F 28.0 | NFE-B 24.1 | Train Loss 267.7367
Epoch 0086 | Time 0.176 (0.145) | NFE-F 28.2 | NFE-B 24.2 | Train Loss 261.9939
Epoch 0087 | Time 0.169 (0.146) | NFE-F 28.3 | NFE-B 24.2 | Train Loss 256.5493
Epoch 0088 | Time 0.166 (0.146) | NFE-F 28.4 | NFE-B 24.2 | Train Loss 251.3899
Epoch 0089 | Time 0.164 (0.146) | NFE-F 28.5 | NFE-B 24.3 | Train Loss 246.5034
Epoch 0090 | Time 0.171 (0.146) | NFE-F 28.6 | NFE-B 24.3 | Train Loss 241.8774
Epoch 0091 | Time 0.170 (0.146) | NFE-F 28.7 | NFE-B 24.3 | Train Loss 237.5002
Epoch 0092 | Time 0.170 (0.147) | NFE-F 28.9 | NFE-B 24.3 | Train Loss 233.3604
Epoch 0093 | Time 0.165 (0.147) | NFE-F 29.0 | NFE-B 24.4 | Train Loss 229.4471
Epoch 0094 | Time 0.166 (0.147) | NFE-F 29.1 | NFE-B 24.4 | Train Loss 225.7496
Epoch 0095 | Time 0.168 (0.147) | NFE-F 29.2 | NFE-B 24.4 | Train Loss 222.2576
Epoch 0096 | Time 0.171 (0.147) | NFE-F 29.3 | NFE-B 24.4 | Train Loss 218.9612
Epoch 0097 | Time 0.177 (0.148) | NFE-F 29.4 | NFE-B 24.5 | Train Loss 215.8509
Epoch 0098 | Time 0.177 (0.148) | NFE-F 29.5 | NFE-B 24.5 | Train Loss 212.9176
Epoch 0099 | Time 0.185 (0.148) | NFE-F 29.6 | NFE-B 24.5 | Train Loss 210.1523
Epoch 0100 | Time 0.176 (0.149) | NFE-F 29.7 | NFE-B 24.5 | Train Loss 207.5466
Epoch 0101 | Time 0.173 (0.149) | NFE-F 29.8 | NFE-B 24.6 | Train Loss 205.0922
Epoch 0102 | Time 0.174 (0.149) | NFE-F 29.9 | NFE-B 24.6 | Train Loss 202.7815
Epoch 0103 | Time 0.172 (0.149) | NFE-F 30.0 | NFE-B 24.6 | Train Loss 200.6067
Epoch 0104 | Time 0.174 (0.150) | NFE-F 30.1 | NFE-B 24.6 | Train Loss 198.5608
Epoch 0105 | Time 0.169 (0.150) | NFE-F 30.2 | NFE-B 24.7 | Train Loss 196.6368
Epoch 0106 | Time 0.165 (0.150) | NFE-F 30.3 | NFE-B 24.7 | Train Loss 194.8281
Epoch 0107 | Time 0.166 (0.150) | NFE-F 30.4 | NFE-B 24.7 | Train Loss 193.1283
Epoch 0108 | Time 0.174 (0.150) | NFE-F 30.5 | NFE-B 24.7 | Train Loss 191.5314
Epoch 0109 | Time 0.174 (0.151) | NFE-F 30.6 | NFE-B 24.8 | Train Loss 190.0317
Epoch 0110 | Time 0.172 (0.151) | NFE-F 30.7 | NFE-B 24.8 | Train Loss 188.6235
Epoch 0111 | Time 0.177 (0.151) | NFE-F 30.8 | NFE-B 24.8 | Train Loss 187.3017
Epoch 0112 | Time 0.166 (0.151) | NFE-F 30.9 | NFE-B 24.8 | Train Loss 186.0611
Epoch 0113 | Time 0.166 (0.151) | NFE-F 31.0 | NFE-B 24.8 | Train Loss 184.8971
Epoch 0114 | Time 0.174 (0.152) | NFE-F 31.1 | NFE-B 24.9 | Train Loss 183.8051
Epoch 0115 | Time 0.184 (0.152) | NFE-F 31.2 | NFE-B 24.9 | Train Loss 182.7806
Epoch 0116 | Time 0.168 (0.152) | NFE-F 31.2 | NFE-B 24.9 | Train Loss 181.8198
Epoch 0117 | Time 0.165 (0.152) | NFE-F 31.3 | NFE-B 24.9 | Train Loss 180.9186
Epoch 0118 | Time 0.168 (0.152) | NFE-F 31.4 | NFE-B 25.0 | Train Loss 180.0733
Epoch 0119 | Time 0.167 (0.153) | NFE-F 31.5 | NFE-B 25.0 | Train Loss 179.2803
Epoch 0120 | Time 0.183 (0.153) | NFE-F 31.6 | NFE-B 25.0 | Train Loss 178.5365
Epoch 0121 | Time 0.170 (0.153) | NFE-F 31.7 | NFE-B 25.0 | Train Loss 177.8385
Epoch 0122 | Time 0.166 (0.153) | NFE-F 31.8 | NFE-B 25.0 | Train Loss 177.1835
Epoch 0123 | Time 0.187 (0.154) | NFE-F 31.8 | NFE-B 25.1 | Train Loss 176.5685
Epoch 0124 | Time 0.167 (0.154) | NFE-F 31.9 | NFE-B 25.1 | Train Loss 175.9911
Epoch 0125 | Time 0.170 (0.154) | NFE-F 32.0 | NFE-B 25.1 | Train Loss 175.4485
Epoch 0126 | Time 0.177 (0.154) | NFE-F 32.1 | NFE-B 25.1 | Train Loss 174.9385
Epoch 0127 | Time 0.169 (0.154) | NFE-F 32.2 | NFE-B 25.1 | Train Loss 174.4589
Epoch 0128 | Time 0.168 (0.154) | NFE-F 32.2 | NFE-B 25.1 | Train Loss 174.0074
Epoch 0129 | Time 0.177 (0.155) | NFE-F 32.3 | NFE-B 25.2 | Train Loss 173.5822
Epoch 0130 | Time 0.178 (0.155) | NFE-F 32.4 | NFE-B 25.2 | Train Loss 173.1814
Epoch 0131 | Time 0.179 (0.155) | NFE-F 32.5 | NFE-B 25.2 | Train Loss 172.8033
Epoch 0132 | Time 0.176 (0.155) | NFE-F 32.5 | NFE-B 25.2 | Train Loss 172.4461
Epoch 0133 | Time 0.173 (0.155) | NFE-F 32.6 | NFE-B 25.2 | Train Loss 172.1083
Epoch 0134 | Time 0.173 (0.156) | NFE-F 32.7 | NFE-B 25.3 | Train Loss 171.7886
Epoch 0135 | Time 0.168 (0.156) | NFE-F 32.8 | NFE-B 25.3 | Train Loss 171.4855
Epoch 0136 | Time 0.192 (0.156) | NFE-F 32.8 | NFE-B 25.3 | Train Loss 171.1978
Epoch 0137 | Time 0.199 (0.157) | NFE-F 32.9 | NFE-B 25.3 | Train Loss 170.9243
Epoch 0138 | Time 0.188 (0.157) | NFE-F 33.0 | NFE-B 25.3 | Train Loss 170.6639
Epoch 0139 | Time 0.175 (0.157) | NFE-F 33.1 | NFE-B 25.3 | Train Loss 170.4156
Epoch 0140 | Time 0.170 (0.157) | NFE-F 33.1 | NFE-B 25.4 | Train Loss 170.1784
Epoch 0141 | Time 0.171 (0.157) | NFE-F 33.2 | NFE-B 25.4 | Train Loss 169.9515
Epoch 0142 | Time 0.171 (0.157) | NFE-F 33.3 | NFE-B 25.4 | Train Loss 169.7339
Epoch 0143 | Time 0.185 (0.158) | NFE-F 33.3 | NFE-B 25.4 | Train Loss 169.5249
Epoch 0144 | Time 0.164 (0.158) | NFE-F 33.4 | NFE-B 25.4 | Train Loss 169.3238
Epoch 0145 | Time 0.171 (0.158) | NFE-F 33.5 | NFE-B 25.4 | Train Loss 169.1299
Epoch 0146 | Time 0.184 (0.158) | NFE-F 33.5 | NFE-B 25.5 | Train Loss 168.9427
Epoch 0147 | Time 0.167 (0.158) | NFE-F 33.6 | NFE-B 25.5 | Train Loss 168.7614
Epoch 0148 | Time 0.170 (0.158) | NFE-F 33.7 | NFE-B 25.5 | Train Loss 168.5857
Epoch 0149 | Time 0.177 (0.159) | NFE-F 33.7 | NFE-B 25.5 | Train Loss 168.4150
Epoch 0150 | Time 0.171 (0.159) | NFE-F 33.8 | NFE-B 25.5 | Train Loss 168.2488
Epoch 0151 | Time 0.184 (0.159) | NFE-F 33.8 | NFE-B 25.5 | Train Loss 168.0868
Epoch 0152 | Time 0.169 (0.159) | NFE-F 33.9 | NFE-B 25.5 | Train Loss 167.9285
Epoch 0153 | Time 0.168 (0.159) | NFE-F 34.0 | NFE-B 25.6 | Train Loss 167.7736
Epoch 0154 | Time 0.165 (0.159) | NFE-F 34.0 | NFE-B 25.6 | Train Loss 167.6217
Epoch 0155 | Time 0.185 (0.159) | NFE-F 34.1 | NFE-B 25.6 | Train Loss 167.4726
Epoch 0156 | Time 0.184 (0.160) | NFE-F 34.1 | NFE-B 25.6 | Train Loss 167.3260
Epoch 0157 | Time 0.168 (0.160) | NFE-F 34.2 | NFE-B 25.6 | Train Loss 167.1816
Epoch 0158 | Time 0.167 (0.160) | NFE-F 34.3 | NFE-B 25.6 | Train Loss 167.0392
Epoch 0159 | Time 0.175 (0.160) | NFE-F 34.3 | NFE-B 25.6 | Train Loss 166.8986
Epoch 0160 | Time 0.180 (0.160) | NFE-F 34.4 | NFE-B 25.7 | Train Loss 166.7596
Epoch 0161 | Time 0.187 (0.160) | NFE-F 34.4 | NFE-B 25.7 | Train Loss 166.6219
Epoch 0162 | Time 0.170 (0.161) | NFE-F 34.5 | NFE-B 25.7 | Train Loss 166.4856
Epoch 0163 | Time 0.176 (0.161) | NFE-F 34.5 | NFE-B 25.7 | Train Loss 166.3503
Epoch 0164 | Time 0.167 (0.161) | NFE-F 34.6 | NFE-B 25.7 | Train Loss 166.2160
Epoch 0165 | Time 0.167 (0.161) | NFE-F 34.6 | NFE-B 25.7 | Train Loss 166.0825
Epoch 0166 | Time 0.181 (0.161) | NFE-F 34.7 | NFE-B 25.7 | Train Loss 165.9498
Epoch 0167 | Time 0.187 (0.161) | NFE-F 34.8 | NFE-B 25.7 | Train Loss 165.8176
Epoch 0168 | Time 0.169 (0.161) | NFE-F 34.8 | NFE-B 25.8 | Train Loss 165.6861
Epoch 0169 | Time 0.181 (0.162) | NFE-F 34.9 | NFE-B 25.8 | Train Loss 165.5549
Epoch 0170 | Time 0.166 (0.162) | NFE-F 34.9 | NFE-B 25.8 | Train Loss 165.4242
Epoch 0171 | Time 0.171 (0.162) | NFE-F 35.0 | NFE-B 25.8 | Train Loss 165.2937
Epoch 0172 | Time 0.188 (0.162) | NFE-F 35.0 | NFE-B 25.8 | Train Loss 165.1635
Epoch 0173 | Time 0.170 (0.162) | NFE-F 35.1 | NFE-B 25.8 | Train Loss 165.0336
Epoch 0174 | Time 0.176 (0.162) | NFE-F 35.1 | NFE-B 25.8 | Train Loss 164.9037
Epoch 0175 | Time 0.171 (0.162) | NFE-F 35.2 | NFE-B 25.8 | Train Loss 164.7740
Epoch 0176 | Time 0.172 (0.162) | NFE-F 35.2 | NFE-B 25.9 | Train Loss 164.6443
Epoch 0177 | Time 0.167 (0.162) | NFE-F 35.3 | NFE-B 25.9 | Train Loss 164.5147
Epoch 0178 | Time 0.168 (0.162) | NFE-F 35.3 | NFE-B 25.9 | Train Loss 164.3851
Epoch 0179 | Time 0.166 (0.163) | NFE-F 35.4 | NFE-B 25.9 | Train Loss 164.2555
Epoch 0180 | Time 0.167 (0.163) | NFE-F 35.4 | NFE-B 25.9 | Train Loss 164.1258
Epoch 0181 | Time 0.171 (0.163) | NFE-F 35.4 | NFE-B 25.9 | Train Loss 163.9961
Epoch 0182 | Time 0.168 (0.163) | NFE-F 35.5 | NFE-B 25.9 | Train Loss 163.8663
Epoch 0183 | Time 0.173 (0.163) | NFE-F 35.5 | NFE-B 25.9 | Train Loss 163.7363
Epoch 0184 | Time 0.176 (0.163) | NFE-F 35.6 | NFE-B 25.9 | Train Loss 163.6063
Epoch 0185 | Time 0.166 (0.163) | NFE-F 35.6 | NFE-B 26.0 | Train Loss 163.4761
Epoch 0186 | Time 0.169 (0.163) | NFE-F 35.7 | NFE-B 26.0 | Train Loss 163.3458
Epoch 0187 | Time 0.172 (0.163) | NFE-F 35.7 | NFE-B 26.0 | Train Loss 163.2153
Epoch 0188 | Time 0.171 (0.163) | NFE-F 35.8 | NFE-B 26.0 | Train Loss 163.0847
Epoch 0189 | Time 0.169 (0.163) | NFE-F 35.8 | NFE-B 26.0 | Train Loss 162.9539
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=64):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=True, batch_size=64, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.126 (0.126) | NFE-F 14.0 | NFE-B 21.0 | Train Loss 9148.4502
Epoch 0001 | Time 0.143 (0.126) | NFE-F 14.1 | NFE-B 21.0 | Train Loss 7636.7046
Epoch 0002 | Time 0.147 (0.126) | NFE-F 14.3 | NFE-B 21.0 | Train Loss 6255.5752
Epoch 0003 | Time 0.165 (0.126) | NFE-F 14.5 | NFE-B 21.1 | Train Loss 5070.9644
Epoch 0004 | Time 0.175 (0.127) | NFE-F 14.8 | NFE-B 21.1 | Train Loss 4064.6428
Epoch 0005 | Time 0.182 (0.127) | NFE-F 15.0 | NFE-B 21.2 | Train Loss 3195.3997
Epoch 0006 | Time 0.176 (0.128) | NFE-F 15.3 | NFE-B 21.2 | Train Loss 2455.5674
Epoch 0007 | Time 0.179 (0.128) | NFE-F 15.5 | NFE-B 21.4 | Train Loss 1840.7457
Epoch 0008 | Time 0.183 (0.129) | NFE-F 15.8 | NFE-B 21.5 | Train Loss 1345.0804
Epoch 0009 | Time 0.185 (0.130) | NFE-F 16.0 | NFE-B 21.6 | Train Loss 961.1309
Epoch 0010 | Time 0.208 (0.130) | NFE-F 16.3 | NFE-B 21.8 | Train Loss 679.7312
Epoch 0011 | Time 0.213 (0.131) | NFE-F 16.5 | NFE-B 21.9 | Train Loss 490.0417
Epoch 0012 | Time 0.201 (0.132) | NFE-F 16.7 | NFE-B 22.1 | Train Loss 379.7443
Epoch 0013 | Time 0.196 (0.132) | NFE-F 17.0 | NFE-B 22.3 | Train Loss 335.3781
Epoch 0014 | Time 0.206 (0.133) | NFE-F 17.2 | NFE-B 22.4 | Train Loss 342.8108
Epoch 0015 | Time 0.199 (0.134) | NFE-F 17.4 | NFE-B 22.6 | Train Loss 387.8163
Epoch 0016 | Time 0.197 (0.134) | NFE-F 17.7 | NFE-B 22.8 | Train Loss 456.7087
Epoch 0017 | Time 0.198 (0.135) | NFE-F 17.9 | NFE-B 22.9 | Train Loss 536.9637
Epoch 0018 | Time 0.195 (0.136) | NFE-F 18.1 | NFE-B 23.1 | Train Loss 617.7590
Epoch 0019 | Time 0.207 (0.136) | NFE-F 18.3 | NFE-B 23.3 | Train Loss 690.3715
Epoch 0020 | Time 0.205 (0.137) | NFE-F 18.5 | NFE-B 23.4 | Train Loss 748.3943
Epoch 0021 | Time 0.200 (0.138) | NFE-F 18.7 | NFE-B 23.6 | Train Loss 787.7685
Epoch 0022 | Time 0.204 (0.138) | NFE-F 19.0 | NFE-B 23.7 | Train Loss 806.6449
Epoch 0023 | Time 0.208 (0.139) | NFE-F 19.2 | NFE-B 23.9 | Train Loss 805.1121
Epoch 0024 | Time 0.205 (0.140) | NFE-F 19.4 | NFE-B 24.0 | Train Loss 784.8380
Epoch 0025 | Time 0.206 (0.140) | NFE-F 19.6 | NFE-B 24.2 | Train Loss 748.6664
Epoch 0026 | Time 0.199 (0.141) | NFE-F 19.8 | NFE-B 24.3 | Train Loss 700.2080
Epoch 0027 | Time 0.199 (0.142) | NFE-F 20.0 | NFE-B 24.5 | Train Loss 643.4534
Epoch 0028 | Time 0.199 (0.142) | NFE-F 20.2 | NFE-B 24.6 | Train Loss 582.4299
Epoch 0029 | Time 0.216 (0.143) | NFE-F 20.4 | NFE-B 24.8 | Train Loss 520.9159
Epoch 0030 | Time 0.216 (0.144) | NFE-F 20.6 | NFE-B 24.9 | Train Loss 462.2139
Epoch 0031 | Time 0.203 (0.144) | NFE-F 20.8 | NFE-B 25.0 | Train Loss 408.9934
Epoch 0032 | Time 0.200 (0.145) | NFE-F 21.0 | NFE-B 25.2 | Train Loss 363.1945
Epoch 0033 | Time 0.197 (0.145) | NFE-F 21.2 | NFE-B 25.3 | Train Loss 325.9932
Epoch 0034 | Time 0.207 (0.146) | NFE-F 21.4 | NFE-B 25.5 | Train Loss 297.8235
Epoch 0035 | Time 0.207 (0.147) | NFE-F 21.5 | NFE-B 25.6 | Train Loss 278.4455
Epoch 0036 | Time 0.202 (0.147) | NFE-F 21.7 | NFE-B 25.7 | Train Loss 267.0536
Epoch 0037 | Time 0.211 (0.148) | NFE-F 21.9 | NFE-B 25.9 | Train Loss 262.4113
Epoch 0038 | Time 0.207 (0.148) | NFE-F 22.1 | NFE-B 26.0 | Train Loss 263.0016
Epoch 0039 | Time 0.205 (0.149) | NFE-F 22.3 | NFE-B 26.1 | Train Loss 267.1803
Epoch 0040 | Time 0.208 (0.149) | NFE-F 22.4 | NFE-B 26.2 | Train Loss 273.3196
Epoch 0041 | Time 0.200 (0.150) | NFE-F 22.6 | NFE-B 26.4 | Train Loss 279.9321
Epoch 0042 | Time 0.202 (0.151) | NFE-F 22.8 | NFE-B 26.5 | Train Loss 285.7678
Epoch 0043 | Time 0.197 (0.151) | NFE-F 23.0 | NFE-B 26.6 | Train Loss 289.8784
Epoch 0044 | Time 0.211 (0.152) | NFE-F 23.1 | NFE-B 26.8 | Train Loss 291.6472
Epoch 0045 | Time 0.211 (0.152) | NFE-F 23.3 | NFE-B 26.9 | Train Loss 290.7889
Epoch 0046 | Time 0.211 (0.153) | NFE-F 23.5 | NFE-B 27.0 | Train Loss 287.3196
Epoch 0047 | Time 0.199 (0.153) | NFE-F 23.6 | NFE-B 27.1 | Train Loss 281.5071
Epoch 0048 | Time 0.200 (0.154) | NFE-F 23.8 | NFE-B 27.2 | Train Loss 273.8059
Epoch 0049 | Time 0.203 (0.154) | NFE-F 24.0 | NFE-B 27.4 | Train Loss 264.7867
Epoch 0050 | Time 0.197 (0.155) | NFE-F 24.1 | NFE-B 27.5 | Train Loss 255.0658
Epoch 0051 | Time 0.205 (0.155) | NFE-F 24.3 | NFE-B 27.6 | Train Loss 245.2416
Epoch 0052 | Time 0.206 (0.156) | NFE-F 24.4 | NFE-B 27.7 | Train Loss 235.8416
Epoch 0053 | Time 0.202 (0.156) | NFE-F 24.6 | NFE-B 27.8 | Train Loss 227.2845
Epoch 0054 | Time 0.202 (0.157) | NFE-F 24.7 | NFE-B 27.9 | Train Loss 219.8573
Epoch 0055 | Time 0.205 (0.157) | NFE-F 24.9 | NFE-B 28.0 | Train Loss 213.7085
Epoch 0056 | Time 0.196 (0.157) | NFE-F 25.1 | NFE-B 28.1 | Train Loss 208.8544
Epoch 0057 | Time 0.201 (0.158) | NFE-F 25.2 | NFE-B 28.3 | Train Loss 205.1975
Epoch 0058 | Time 0.199 (0.158) | NFE-F 25.3 | NFE-B 28.4 | Train Loss 202.5531
Epoch 0059 | Time 0.203 (0.159) | NFE-F 25.5 | NFE-B 28.5 | Train Loss 200.6793
Epoch 0060 | Time 0.208 (0.159) | NFE-F 25.6 | NFE-B 28.6 | Train Loss 199.3093
Epoch 0061 | Time 0.203 (0.160) | NFE-F 25.8 | NFE-B 28.7 | Train Loss 198.1807
Epoch 0062 | Time 0.204 (0.160) | NFE-F 25.9 | NFE-B 28.8 | Train Loss 197.0601
Epoch 0063 | Time 0.209 (0.161) | NFE-F 26.1 | NFE-B 28.9 | Train Loss 195.7612
Epoch 0064 | Time 0.216 (0.161) | NFE-F 26.2 | NFE-B 29.0 | Train Loss 194.1553
Epoch 0065 | Time 0.203 (0.162) | NFE-F 26.3 | NFE-B 29.1 | Train Loss 192.1747
Epoch 0066 | Time 0.205 (0.162) | NFE-F 26.5 | NFE-B 29.2 | Train Loss 189.8094
Epoch 0067 | Time 0.201 (0.162) | NFE-F 26.6 | NFE-B 29.3 | Train Loss 187.0983
Epoch 0068 | Time 0.211 (0.163) | NFE-F 26.7 | NFE-B 29.4 | Train Loss 184.1167
Epoch 0069 | Time 0.200 (0.163) | NFE-F 26.9 | NFE-B 29.5 | Train Loss 180.9624
Epoch 0070 | Time 0.202 (0.164) | NFE-F 27.0 | NFE-B 29.6 | Train Loss 177.7410
Epoch 0071 | Time 0.201 (0.164) | NFE-F 27.1 | NFE-B 29.7 | Train Loss 174.5535
Epoch 0072 | Time 0.211 (0.164) | NFE-F 27.3 | NFE-B 29.8 | Train Loss 171.4859
Epoch 0073 | Time 0.210 (0.165) | NFE-F 27.4 | NFE-B 29.8 | Train Loss 168.6019
Epoch 0074 | Time 0.203 (0.165) | NFE-F 27.5 | NFE-B 29.9 | Train Loss 165.9396
Epoch 0075 | Time 0.201 (0.166) | NFE-F 27.7 | NFE-B 30.0 | Train Loss 163.5112
Epoch 0076 | Time 0.207 (0.166) | NFE-F 27.8 | NFE-B 30.1 | Train Loss 161.3058
Epoch 0077 | Time 0.203 (0.166) | NFE-F 27.9 | NFE-B 30.2 | Train Loss 159.2944
Epoch 0078 | Time 0.213 (0.167) | NFE-F 28.0 | NFE-B 30.3 | Train Loss 157.4354
Epoch 0079 | Time 0.204 (0.167) | NFE-F 28.1 | NFE-B 30.4 | Train Loss 155.6819
Epoch 0080 | Time 0.206 (0.168) | NFE-F 28.3 | NFE-B 30.5 | Train Loss 153.9873
Epoch 0081 | Time 0.211 (0.168) | NFE-F 28.4 | NFE-B 30.6 | Train Loss 152.3102
Epoch 0082 | Time 0.203 (0.168) | NFE-F 28.5 | NFE-B 30.6 | Train Loss 150.6190
Epoch 0083 | Time 0.200 (0.169) | NFE-F 28.6 | NFE-B 30.7 | Train Loss 148.8928
Epoch 0084 | Time 0.208 (0.169) | NFE-F 28.7 | NFE-B 30.8 | Train Loss 147.1224
Epoch 0085 | Time 0.201 (0.169) | NFE-F 28.8 | NFE-B 30.9 | Train Loss 145.3094
Epoch 0086 | Time 0.205 (0.170) | NFE-F 28.9 | NFE-B 31.0 | Train Loss 143.4639
Epoch 0087 | Time 0.203 (0.170) | NFE-F 29.1 | NFE-B 31.0 | Train Loss 141.6017
Epoch 0088 | Time 0.201 (0.170) | NFE-F 29.2 | NFE-B 31.1 | Train Loss 139.7415
Epoch 0089 | Time 0.199 (0.171) | NFE-F 29.3 | NFE-B 31.2 | Train Loss 137.9020
Epoch 0090 | Time 0.205 (0.171) | NFE-F 29.4 | NFE-B 31.3 | Train Loss 136.0998
Epoch 0091 | Time 0.199 (0.171) | NFE-F 29.5 | NFE-B 31.4 | Train Loss 134.3471
Epoch 0092 | Time 0.201 (0.172) | NFE-F 29.6 | NFE-B 31.4 | Train Loss 132.6515
Epoch 0093 | Time 0.204 (0.172) | NFE-F 29.7 | NFE-B 31.5 | Train Loss 131.0153
Epoch 0094 | Time 0.200 (0.172) | NFE-F 29.8 | NFE-B 31.6 | Train Loss 129.4363
Epoch 0095 | Time 0.203 (0.173) | NFE-F 29.9 | NFE-B 31.7 | Train Loss 127.9087
Epoch 0096 | Time 0.205 (0.173) | NFE-F 30.0 | NFE-B 31.7 | Train Loss 126.4242
Epoch 0097 | Time 0.202 (0.173) | NFE-F 30.1 | NFE-B 31.8 | Train Loss 124.9736
Epoch 0098 | Time 0.207 (0.174) | NFE-F 30.2 | NFE-B 31.9 | Train Loss 123.5480
Epoch 0099 | Time 0.201 (0.174) | NFE-F 30.3 | NFE-B 32.0 | Train Loss 122.1398
Epoch 0100 | Time 0.198 (0.174) | NFE-F 30.4 | NFE-B 32.0 | Train Loss 120.7433
Epoch 0101 | Time 0.200 (0.174) | NFE-F 30.5 | NFE-B 32.1 | Train Loss 119.3551
Epoch 0102 | Time 0.204 (0.175) | NFE-F 30.6 | NFE-B 32.2 | Train Loss 117.9741
Epoch 0103 | Time 0.211 (0.175) | NFE-F 30.7 | NFE-B 32.2 | Train Loss 116.6013
Epoch 0104 | Time 0.203 (0.175) | NFE-F 30.8 | NFE-B 32.3 | Train Loss 115.2390
Epoch 0105 | Time 0.216 (0.176) | NFE-F 30.9 | NFE-B 32.4 | Train Loss 113.8904
Epoch 0106 | Time 0.201 (0.176) | NFE-F 31.0 | NFE-B 32.4 | Train Loss 112.5587
Epoch 0107 | Time 0.207 (0.176) | NFE-F 31.0 | NFE-B 32.5 | Train Loss 111.2473
Epoch 0108 | Time 0.203 (0.176) | NFE-F 31.1 | NFE-B 32.6 | Train Loss 109.9583
Epoch 0109 | Time 0.207 (0.177) | NFE-F 31.2 | NFE-B 32.6 | Train Loss 108.6932
Epoch 0110 | Time 0.201 (0.177) | NFE-F 31.3 | NFE-B 32.7 | Train Loss 107.4523
Epoch 0111 | Time 0.203 (0.177) | NFE-F 31.4 | NFE-B 32.8 | Train Loss 106.2350
Epoch 0112 | Time 0.211 (0.178) | NFE-F 31.5 | NFE-B 32.8 | Train Loss 105.0398
Epoch 0113 | Time 0.204 (0.178) | NFE-F 31.6 | NFE-B 32.9 | Train Loss 103.8650
Epoch 0114 | Time 0.216 (0.178) | NFE-F 31.7 | NFE-B 32.9 | Train Loss 102.7085
Epoch 0115 | Time 0.219 (0.179) | NFE-F 31.7 | NFE-B 33.0 | Train Loss 101.5682
Epoch 0116 | Time 0.211 (0.179) | NFE-F 31.8 | NFE-B 33.1 | Train Loss 100.4425
Epoch 0117 | Time 0.207 (0.179) | NFE-F 31.9 | NFE-B 33.1 | Train Loss 99.3301
Epoch 0118 | Time 0.205 (0.180) | NFE-F 32.0 | NFE-B 33.2 | Train Loss 98.2303
Epoch 0119 | Time 0.198 (0.180) | NFE-F 32.1 | NFE-B 33.2 | Train Loss 97.1426
Epoch 0120 | Time 0.204 (0.180) | NFE-F 32.1 | NFE-B 33.3 | Train Loss 96.0674
Epoch 0121 | Time 0.202 (0.180) | NFE-F 32.2 | NFE-B 33.4 | Train Loss 95.0047
Epoch 0122 | Time 0.208 (0.180) | NFE-F 32.3 | NFE-B 33.4 | Train Loss 93.9553
Epoch 0123 | Time 0.196 (0.181) | NFE-F 32.4 | NFE-B 33.5 | Train Loss 92.9194
Epoch 0124 | Time 0.197 (0.181) | NFE-F 32.5 | NFE-B 33.5 | Train Loss 91.8976
Epoch 0125 | Time 0.200 (0.181) | NFE-F 32.5 | NFE-B 33.6 | Train Loss 90.8900
Epoch 0126 | Time 0.204 (0.181) | NFE-F 32.6 | NFE-B 33.6 | Train Loss 89.8966
Epoch 0127 | Time 0.205 (0.181) | NFE-F 32.7 | NFE-B 33.7 | Train Loss 88.9172
Epoch 0128 | Time 0.203 (0.182) | NFE-F 32.8 | NFE-B 33.7 | Train Loss 87.9514
Epoch 0129 | Time 0.205 (0.182) | NFE-F 32.8 | NFE-B 33.8 | Train Loss 86.9986
Epoch 0130 | Time 0.205 (0.182) | NFE-F 32.9 | NFE-B 33.8 | Train Loss 86.0583
Epoch 0131 | Time 0.199 (0.182) | NFE-F 33.0 | NFE-B 33.9 | Train Loss 85.1299
Epoch 0132 | Time 0.212 (0.183) | NFE-F 33.0 | NFE-B 33.9 | Train Loss 84.2129
Epoch 0133 | Time 0.212 (0.183) | NFE-F 33.1 | NFE-B 34.0 | Train Loss 83.3069
Epoch 0134 | Time 0.202 (0.183) | NFE-F 33.2 | NFE-B 34.0 | Train Loss 82.4116
Epoch 0135 | Time 0.205 (0.183) | NFE-F 33.2 | NFE-B 34.1 | Train Loss 81.5267
Epoch 0136 | Time 0.199 (0.183) | NFE-F 33.3 | NFE-B 34.1 | Train Loss 80.6521
Epoch 0137 | Time 0.208 (0.184) | NFE-F 33.4 | NFE-B 34.2 | Train Loss 79.7879
Epoch 0138 | Time 0.241 (0.184) | NFE-F 33.4 | NFE-B 34.2 | Train Loss 78.9339
Epoch 0139 | Time 0.205 (0.184) | NFE-F 33.5 | NFE-B 34.3 | Train Loss 78.0902
Epoch 0140 | Time 0.218 (0.185) | NFE-F 33.6 | NFE-B 34.3 | Train Loss 77.2568
Epoch 0141 | Time 0.206 (0.185) | NFE-F 33.6 | NFE-B 34.4 | Train Loss 76.4335
Epoch 0142 | Time 0.200 (0.185) | NFE-F 33.7 | NFE-B 34.4 | Train Loss 75.6202
Epoch 0143 | Time 0.203 (0.185) | NFE-F 33.8 | NFE-B 34.5 | Train Loss 74.8169
Epoch 0144 | Time 0.199 (0.186) | NFE-F 33.8 | NFE-B 34.5 | Train Loss 74.0232
Epoch 0145 | Time 0.203 (0.186) | NFE-F 33.9 | NFE-B 34.6 | Train Loss 73.2390
Epoch 0146 | Time 0.200 (0.186) | NFE-F 33.9 | NFE-B 34.6 | Train Loss 72.4640
Epoch 0147 | Time 0.199 (0.186) | NFE-F 34.0 | NFE-B 34.6 | Train Loss 71.6981
Epoch 0148 | Time 0.209 (0.186) | NFE-F 34.1 | NFE-B 34.7 | Train Loss 70.9410
Epoch 0149 | Time 0.207 (0.186) | NFE-F 34.1 | NFE-B 34.7 | Train Loss 70.1926
Epoch 0150 | Time 0.200 (0.187) | NFE-F 34.2 | NFE-B 34.8 | Train Loss 69.4527
Epoch 0151 | Time 0.219 (0.187) | NFE-F 34.2 | NFE-B 34.8 | Train Loss 68.7213
Epoch 0152 | Time 0.212 (0.187) | NFE-F 34.3 | NFE-B 34.9 | Train Loss 67.9982
Epoch 0153 | Time 0.213 (0.187) | NFE-F 34.4 | NFE-B 34.9 | Train Loss 67.2834
Epoch 0154 | Time 0.200 (0.187) | NFE-F 34.4 | NFE-B 34.9 | Train Loss 66.5767
Epoch 0155 | Time 0.203 (0.188) | NFE-F 34.5 | NFE-B 35.0 | Train Loss 65.8782
Epoch 0156 | Time 0.209 (0.188) | NFE-F 34.5 | NFE-B 35.0 | Train Loss 65.1876
Epoch 0157 | Time 0.213 (0.188) | NFE-F 34.6 | NFE-B 35.1 | Train Loss 64.5050
Epoch 0158 | Time 0.207 (0.188) | NFE-F 34.6 | NFE-B 35.1 | Train Loss 63.8302
Epoch 0159 | Time 0.218 (0.189) | NFE-F 34.7 | NFE-B 35.1 | Train Loss 63.1632
Epoch 0160 | Time 0.207 (0.189) | NFE-F 34.7 | NFE-B 35.2 | Train Loss 62.5037
Epoch 0161 | Time 0.210 (0.189) | NFE-F 34.8 | NFE-B 35.2 | Train Loss 61.8518
Epoch 0162 | Time 0.208 (0.189) | NFE-F 34.8 | NFE-B 35.3 | Train Loss 61.2073
Epoch 0163 | Time 0.205 (0.189) | NFE-F 34.9 | NFE-B 35.3 | Train Loss 60.5700
Epoch 0164 | Time 0.207 (0.190) | NFE-F 35.0 | NFE-B 35.3 | Train Loss 59.9400
Epoch 0165 | Time 0.204 (0.190) | NFE-F 35.0 | NFE-B 35.4 | Train Loss 59.3171
Epoch 0166 | Time 0.203 (0.190) | NFE-F 35.1 | NFE-B 35.4 | Train Loss 58.7013
Epoch 0167 | Time 0.201 (0.190) | NFE-F 35.1 | NFE-B 35.4 | Train Loss 58.0924
Epoch 0168 | Time 0.200 (0.190) | NFE-F 35.2 | NFE-B 35.5 | Train Loss 57.4904
Epoch 0169 | Time 0.196 (0.190) | NFE-F 35.2 | NFE-B 35.5 | Train Loss 56.8953
Epoch 0170 | Time 0.206 (0.190) | NFE-F 35.2 | NFE-B 35.5 | Train Loss 56.3069
Epoch 0171 | Time 0.214 (0.190) | NFE-F 35.3 | NFE-B 35.6 | Train Loss 55.7252
Epoch 0172 | Time 0.212 (0.191) | NFE-F 35.3 | NFE-B 35.6 | Train Loss 55.1502
Epoch 0173 | Time 0.207 (0.191) | NFE-F 35.4 | NFE-B 35.6 | Train Loss 54.5817
Epoch 0174 | Time 0.205 (0.191) | NFE-F 35.4 | NFE-B 35.7 | Train Loss 54.0197
Epoch 0175 | Time 0.208 (0.191) | NFE-F 35.5 | NFE-B 35.7 | Train Loss 53.4642
Epoch 0176 | Time 0.200 (0.191) | NFE-F 35.5 | NFE-B 35.7 | Train Loss 52.9150
Epoch 0177 | Time 0.197 (0.191) | NFE-F 35.6 | NFE-B 35.8 | Train Loss 52.3721
Epoch 0178 | Time 0.202 (0.191) | NFE-F 35.6 | NFE-B 35.8 | Train Loss 51.8355
Epoch 0179 | Time 0.203 (0.192) | NFE-F 35.7 | NFE-B 35.8 | Train Loss 51.3050
Epoch 0180 | Time 0.209 (0.192) | NFE-F 35.7 | NFE-B 35.9 | Train Loss 50.7806
Epoch 0181 | Time 0.205 (0.192) | NFE-F 35.7 | NFE-B 35.9 | Train Loss 50.2623
Epoch 0182 | Time 0.204 (0.192) | NFE-F 35.8 | NFE-B 35.9 | Train Loss 49.7500
Epoch 0183 | Time 0.218 (0.192) | NFE-F 35.8 | NFE-B 36.0 | Train Loss 49.2436
Epoch 0184 | Time 0.209 (0.192) | NFE-F 35.9 | NFE-B 36.0 | Train Loss 48.7431
Epoch 0185 | Time 0.222 (0.193) | NFE-F 35.9 | NFE-B 36.0 | Train Loss 48.2485
Epoch 0186 | Time 0.206 (0.193) | NFE-F 36.0 | NFE-B 36.1 | Train Loss 47.7596
Epoch 0187 | Time 0.203 (0.193) | NFE-F 36.0 | NFE-B 36.1 | Train Loss 47.2765
Epoch 0188 | Time 0.201 (0.193) | NFE-F 36.0 | NFE-B 36.1 | Train Loss 46.7990
Epoch 0189 | Time 0.204 (0.193) | NFE-F 36.1 | NFE-B 36.1 | Train Loss 46.3272
Epoch 0190 | Time 0.212 (0.193) | NFE-F 36.1 | NFE-B 36.2 | Train Loss 45.8610
Epoch 0191 | Time 0.206 (0.193) | NFE-F 36.2 | NFE-B 36.2 | Train Loss 45.4002
Epoch 0192 | Time 0.204 (0.194) | NFE-F 36.2 | NFE-B 36.2 | Train Loss 44.9450
Epoch 0193 | Time 0.209 (0.194) | NFE-F 36.2 | NFE-B 36.3 | Train Loss 44.4952
Epoch 0194 | Time 0.209 (0.194) | NFE-F 36.3 | NFE-B 36.3 | Train Loss 44.0507
Epoch 0195 | Time 0.211 (0.194) | NFE-F 36.3 | NFE-B 36.3 | Train Loss 43.6116
Epoch 0196 | Time 0.210 (0.194) | NFE-F 36.3 | NFE-B 36.3 | Train Loss 43.1778
Epoch 0197 | Time 0.209 (0.194) | NFE-F 36.4 | NFE-B 36.4 | Train Loss 42.7491
Epoch 0198 | Time 0.206 (0.194) | NFE-F 36.4 | NFE-B 36.4 | Train Loss 42.3257
Epoch 0199 | Time 0.218 (0.195) | NFE-F 36.4 | NFE-B 36.4 | Train Loss 41.9074
Epoch 0200 | Time 0.208 (0.195) | NFE-F 36.5 | NFE-B 36.4 | Train Loss 41.4943
Epoch 0201 | Time 0.206 (0.195) | NFE-F 36.5 | NFE-B 36.5 | Train Loss 41.0861
Epoch 0202 | Time 0.202 (0.195) | NFE-F 36.6 | NFE-B 36.5 | Train Loss 40.6830
Epoch 0203 | Time 0.206 (0.195) | NFE-F 36.6 | NFE-B 36.5 | Train Loss 40.2849
Epoch 0204 | Time 0.207 (0.195) | NFE-F 36.6 | NFE-B 36.5 | Train Loss 39.8916
Epoch 0205 | Time 0.210 (0.195) | NFE-F 36.7 | NFE-B 36.6 | Train Loss 39.5032
Epoch 0206 | Time 0.224 (0.196) | NFE-F 36.7 | NFE-B 36.6 | Train Loss 39.1197
Epoch 0207 | Time 0.210 (0.196) | NFE-F 36.7 | NFE-B 36.6 | Train Loss 38.7409
Epoch 0208 | Time 0.209 (0.196) | NFE-F 36.8 | NFE-B 36.6 | Train Loss 38.3669
Epoch 0209 | Time 0.213 (0.196) | NFE-F 36.8 | NFE-B 36.7 | Train Loss 37.9976
Epoch 0210 | Time 0.214 (0.196) | NFE-F 36.8 | NFE-B 36.7 | Train Loss 37.6329
Epoch 0211 | Time 0.205 (0.196) | NFE-F 36.9 | NFE-B 36.7 | Train Loss 37.2729
Epoch 0212 | Time 0.203 (0.196) | NFE-F 36.9 | NFE-B 36.7 | Train Loss 36.9174
Epoch 0213 | Time 0.201 (0.196) | NFE-F 36.9 | NFE-B 36.8 | Train Loss 36.5664
Epoch 0214 | Time 0.215 (0.197) | NFE-F 36.9 | NFE-B 36.8 | Train Loss 36.2199
Epoch 0215 | Time 0.220 (0.197) | NFE-F 37.0 | NFE-B 36.8 | Train Loss 35.8779
Epoch 0216 | Time 0.206 (0.197) | NFE-F 37.0 | NFE-B 36.8 | Train Loss 35.5403
Epoch 0217 | Time 0.205 (0.197) | NFE-F 37.0 | NFE-B 36.8 | Train Loss 35.2070
Epoch 0218 | Time 0.209 (0.197) | NFE-F 37.1 | NFE-B 36.9 | Train Loss 34.8781
Epoch 0219 | Time 0.212 (0.197) | NFE-F 37.1 | NFE-B 36.9 | Train Loss 34.5534
Epoch 0220 | Time 0.207 (0.197) | NFE-F 37.1 | NFE-B 36.9 | Train Loss 34.2330
Epoch 0221 | Time 0.202 (0.197) | NFE-F 37.2 | NFE-B 36.9 | Train Loss 33.9168
Epoch 0222 | Time 0.204 (0.198) | NFE-F 37.2 | NFE-B 37.0 | Train Loss 33.6047
Epoch 0223 | Time 0.207 (0.198) | NFE-F 37.2 | NFE-B 37.0 | Train Loss 33.2968
Epoch 0224 | Time 0.205 (0.198) | NFE-F 37.2 | NFE-B 37.0 | Train Loss 32.9929
Epoch 0225 | Time 0.208 (0.198) | NFE-F 37.3 | NFE-B 37.0 | Train Loss 32.6930
Epoch 0226 | Time 0.202 (0.198) | NFE-F 37.3 | NFE-B 37.0 | Train Loss 32.3972
Epoch 0227 | Time 0.203 (0.198) | NFE-F 37.3 | NFE-B 37.1 | Train Loss 32.1053
Epoch 0228 | Time 0.218 (0.198) | NFE-F 37.3 | NFE-B 37.1 | Train Loss 31.8174
Epoch 0229 | Time 0.212 (0.198) | NFE-F 37.4 | NFE-B 37.1 | Train Loss 31.5333
Epoch 0230 | Time 0.206 (0.198) | NFE-F 37.4 | NFE-B 37.1 | Train Loss 31.2530
Epoch 0231 | Time 0.212 (0.198) | NFE-F 37.4 | NFE-B 37.1 | Train Loss 30.9766
Epoch 0232 | Time 0.201 (0.198) | NFE-F 37.5 | NFE-B 37.1 | Train Loss 30.7039
Epoch 0233 | Time 0.211 (0.199) | NFE-F 37.5 | NFE-B 37.2 | Train Loss 30.4350
Epoch 0234 | Time 0.207 (0.199) | NFE-F 37.5 | NFE-B 37.2 | Train Loss 30.1697
Epoch 0235 | Time 0.207 (0.199) | NFE-F 37.5 | NFE-B 37.2 | Train Loss 29.9081
Epoch 0236 | Time 0.209 (0.199) | NFE-F 37.6 | NFE-B 37.2 | Train Loss 29.6501
Epoch 0237 | Time 0.203 (0.199) | NFE-F 37.6 | NFE-B 37.2 | Train Loss 29.3956
Epoch 0238 | Time 0.207 (0.199) | NFE-F 37.6 | NFE-B 37.3 | Train Loss 29.1447
Epoch 0239 | Time 0.218 (0.199) | NFE-F 37.6 | NFE-B 37.3 | Train Loss 28.8973
Epoch 0240 | Time 0.204 (0.199) | NFE-F 37.6 | NFE-B 37.3 | Train Loss 28.6534
Epoch 0241 | Time 0.205 (0.199) | NFE-F 37.7 | NFE-B 37.3 | Train Loss 28.4128
Epoch 0242 | Time 0.209 (0.199) | NFE-F 37.7 | NFE-B 37.3 | Train Loss 28.1757
Epoch 0243 | Time 0.205 (0.199) | NFE-F 37.7 | NFE-B 37.3 | Train Loss 27.9419
Epoch 0244 | Time 0.211 (0.200) | NFE-F 37.7 | NFE-B 37.4 | Train Loss 27.7114
Epoch 0245 | Time 0.208 (0.200) | NFE-F 37.8 | NFE-B 37.4 | Train Loss 27.4842
Epoch 0246 | Time 0.211 (0.200) | NFE-F 37.8 | NFE-B 37.4 | Train Loss 27.2602
Epoch 0247 | Time 0.205 (0.200) | NFE-F 37.8 | NFE-B 37.4 | Train Loss 27.0395
Epoch 0248 | Time 0.229 (0.200) | NFE-F 37.8 | NFE-B 37.4 | Train Loss 26.8219
Epoch 0249 | Time 0.208 (0.200) | NFE-F 37.9 | NFE-B 37.4 | Train Loss 26.6074
Epoch 0250 | Time 0.203 (0.200) | NFE-F 37.9 | NFE-B 37.5 | Train Loss 26.3961
Epoch 0251 | Time 0.203 (0.200) | NFE-F 37.9 | NFE-B 37.5 | Train Loss 26.1878
Epoch 0252 | Time 0.212 (0.200) | NFE-F 37.9 | NFE-B 37.5 | Train Loss 25.9825
Epoch 0253 | Time 0.215 (0.200) | NFE-F 37.9 | NFE-B 37.5 | Train Loss 25.7802
Epoch 0254 | Time 0.211 (0.201) | NFE-F 38.0 | NFE-B 37.5 | Train Loss 25.5809
Epoch 0255 | Time 0.205 (0.201) | NFE-F 38.0 | NFE-B 37.5 | Train Loss 25.3845
Epoch 0256 | Time 0.211 (0.201) | NFE-F 38.0 | NFE-B 37.5 | Train Loss 25.1910
Epoch 0257 | Time 0.205 (0.201) | NFE-F 38.0 | NFE-B 37.6 | Train Loss 25.0003
Epoch 0258 | Time 0.203 (0.201) | NFE-F 38.0 | NFE-B 37.6 | Train Loss 24.8125
Epoch 0259 | Time 0.199 (0.201) | NFE-F 38.1 | NFE-B 37.6 | Train Loss 24.6275
Epoch 0260 | Time 0.215 (0.201) | NFE-F 38.1 | NFE-B 37.6 | Train Loss 24.4452
Epoch 0261 | Time 0.209 (0.201) | NFE-F 38.1 | NFE-B 37.6 | Train Loss 24.2656
Epoch 0262 | Time 0.216 (0.201) | NFE-F 38.1 | NFE-B 37.6 | Train Loss 24.0887
Epoch 0263 | Time 0.199 (0.201) | NFE-F 38.1 | NFE-B 37.6 | Train Loss 23.9145
Epoch 0264 | Time 0.210 (0.201) | NFE-F 38.2 | NFE-B 37.7 | Train Loss 23.7429
Epoch 0265 | Time 0.204 (0.201) | NFE-F 38.2 | NFE-B 37.7 | Train Loss 23.5739
Epoch 0266 | Time 0.203 (0.201) | NFE-F 38.2 | NFE-B 37.7 | Train Loss 23.4075
Epoch 0267 | Time 0.205 (0.201) | NFE-F 38.2 | NFE-B 37.7 | Train Loss 23.2436
Epoch 0268 | Time 0.198 (0.201) | NFE-F 38.2 | NFE-B 37.7 | Train Loss 23.0821
Epoch 0269 | Time 0.196 (0.201) | NFE-F 38.2 | NFE-B 37.7 | Train Loss 22.9232
Epoch 0270 | Time 0.199 (0.201) | NFE-F 38.3 | NFE-B 37.7 | Train Loss 22.7667
Epoch 0271 | Time 0.210 (0.201) | NFE-F 38.3 | NFE-B 37.7 | Train Loss 22.6126
Epoch 0272 | Time 0.220 (0.201) | NFE-F 38.3 | NFE-B 37.8 | Train Loss 22.4608
Epoch 0273 | Time 0.216 (0.202) | NFE-F 38.3 | NFE-B 37.8 | Train Loss 22.3115
Epoch 0274 | Time 0.205 (0.202) | NFE-F 38.3 | NFE-B 37.8 | Train Loss 22.1644
Epoch 0275 | Time 0.204 (0.202) | NFE-F 38.3 | NFE-B 37.8 | Train Loss 22.0196
Epoch 0276 | Time 0.202 (0.202) | NFE-F 38.4 | NFE-B 37.8 | Train Loss 21.8771
Epoch 0277 | Time 0.212 (0.202) | NFE-F 38.4 | NFE-B 37.8 | Train Loss 21.7368
Epoch 0278 | Time 0.207 (0.202) | NFE-F 38.4 | NFE-B 37.8 | Train Loss 21.5987
Epoch 0279 | Time 0.203 (0.202) | NFE-F 38.4 | NFE-B 37.8 | Train Loss 21.4627
Epoch 0280 | Time 0.204 (0.202) | NFE-F 38.4 | NFE-B 37.9 | Train Loss 21.3289
Epoch 0281 | Time 0.211 (0.202) | NFE-F 38.4 | NFE-B 37.9 | Train Loss 21.1972
Epoch 0282 | Time 0.199 (0.202) | NFE-F 38.5 | NFE-B 37.9 | Train Loss 21.0676
Epoch 0283 | Time 0.202 (0.202) | NFE-F 38.5 | NFE-B 37.9 | Train Loss 20.9401
Epoch 0284 | Time 0.203 (0.202) | NFE-F 38.5 | NFE-B 37.9 | Train Loss 20.8146
Epoch 0285 | Time 0.204 (0.202) | NFE-F 38.5 | NFE-B 37.9 | Train Loss 20.6911
Epoch 0286 | Time 0.221 (0.202) | NFE-F 38.5 | NFE-B 37.9 | Train Loss 20.5695
Epoch 0287 | Time 0.220 (0.202) | NFE-F 38.5 | NFE-B 37.9 | Train Loss 20.4499
Epoch 0288 | Time 0.214 (0.202) | NFE-F 38.5 | NFE-B 37.9 | Train Loss 20.3323
Epoch 0289 | Time 0.213 (0.203) | NFE-F 38.6 | NFE-B 38.0 | Train Loss 20.2165
Epoch 0290 | Time 0.207 (0.203) | NFE-F 38.6 | NFE-B 38.0 | Train Loss 20.1026
Epoch 0291 | Time 0.212 (0.203) | NFE-F 38.6 | NFE-B 38.0 | Train Loss 19.9905
Epoch 0292 | Time 0.202 (0.203) | NFE-F 38.6 | NFE-B 38.0 | Train Loss 19.8803
Epoch 0293 | Time 0.204 (0.203) | NFE-F 38.6 | NFE-B 38.0 | Train Loss 19.7718
Epoch 0294 | Time 0.202 (0.203) | NFE-F 38.6 | NFE-B 38.0 | Train Loss 19.6651
Epoch 0295 | Time 0.205 (0.203) | NFE-F 38.6 | NFE-B 38.0 | Train Loss 19.5602
Epoch 0296 | Time 0.210 (0.203) | NFE-F 38.7 | NFE-B 38.0 | Train Loss 19.4570
Epoch 0297 | Time 0.203 (0.203) | NFE-F 38.7 | NFE-B 38.0 | Train Loss 19.3554
Epoch 0298 | Time 0.205 (0.203) | NFE-F 38.7 | NFE-B 38.0 | Train Loss 19.2556
Epoch 0299 | Time 0.203 (0.203) | NFE-F 38.7 | NFE-B 38.1 | Train Loss 19.1574
Epoch 0300 | Time 0.202 (0.203) | NFE-F 38.7 | NFE-B 38.1 | Train Loss 19.0608
Epoch 0301 | Time 0.211 (0.203) | NFE-F 38.7 | NFE-B 38.1 | Train Loss 18.9658
Epoch 0302 | Time 0.206 (0.203) | NFE-F 38.7 | NFE-B 38.1 | Train Loss 18.8724
Epoch 0303 | Time 0.202 (0.203) | NFE-F 38.8 | NFE-B 38.1 | Train Loss 18.7805
Epoch 0304 | Time 0.201 (0.203) | NFE-F 38.8 | NFE-B 38.1 | Train Loss 18.6902
Epoch 0305 | Time 0.219 (0.203) | NFE-F 38.8 | NFE-B 38.1 | Train Loss 18.6014
Epoch 0306 | Time 0.205 (0.203) | NFE-F 38.8 | NFE-B 38.1 | Train Loss 18.5141
Epoch 0307 | Time 0.202 (0.203) | NFE-F 38.8 | NFE-B 38.1 | Train Loss 18.4282
Epoch 0308 | Time 0.200 (0.203) | NFE-F 38.8 | NFE-B 38.1 | Train Loss 18.3438
Epoch 0309 | Time 0.201 (0.203) | NFE-F 38.8 | NFE-B 38.1 | Train Loss 18.2608
Epoch 0310 | Time 0.209 (0.203) | NFE-F 38.8 | NFE-B 38.2 | Train Loss 18.1792
Epoch 0311 | Time 0.206 (0.203) | NFE-F 38.8 | NFE-B 38.2 | Train Loss 18.0990
Epoch 0312 | Time 0.201 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 18.0201
Epoch 0313 | Time 0.201 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 17.9426
Epoch 0314 | Time 0.216 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 17.8664
Epoch 0315 | Time 0.208 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 17.7915
Epoch 0316 | Time 0.212 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 17.7179
Epoch 0317 | Time 0.199 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 17.6455
Epoch 0318 | Time 0.201 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 17.5744
Epoch 0319 | Time 0.203 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 17.5045
Epoch 0320 | Time 0.217 (0.203) | NFE-F 38.9 | NFE-B 38.2 | Train Loss 17.4358
Epoch 0321 | Time 0.221 (0.204) | NFE-F 39.0 | NFE-B 38.2 | Train Loss 17.3683
Epoch 0322 | Time 0.204 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 17.3020
Epoch 0323 | Time 0.201 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 17.2368
Epoch 0324 | Time 0.202 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 17.1727
Epoch 0325 | Time 0.214 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 17.1097
Epoch 0326 | Time 0.206 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 17.0479
Epoch 0327 | Time 0.208 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 16.9871
Epoch 0328 | Time 0.206 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 16.9274
Epoch 0329 | Time 0.199 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 16.8687
Epoch 0330 | Time 0.203 (0.204) | NFE-F 39.0 | NFE-B 38.3 | Train Loss 16.8111
Epoch 0331 | Time 0.204 (0.204) | NFE-F 39.1 | NFE-B 38.3 | Train Loss 16.7545
Epoch 0332 | Time 0.202 (0.204) | NFE-F 39.1 | NFE-B 38.3 | Train Loss 16.6988
Epoch 0333 | Time 0.207 (0.204) | NFE-F 39.1 | NFE-B 38.3 | Train Loss 16.6442
Epoch 0334 | Time 0.213 (0.204) | NFE-F 39.1 | NFE-B 38.3 | Train Loss 16.5905
Epoch 0335 | Time 0.214 (0.204) | NFE-F 39.1 | NFE-B 38.3 | Train Loss 16.5377
Epoch 0336 | Time 0.204 (0.204) | NFE-F 39.1 | NFE-B 38.3 | Train Loss 16.4859
Epoch 0337 | Time 0.222 (0.204) | NFE-F 39.1 | NFE-B 38.4 | Train Loss 16.4350
Epoch 0338 | Time 0.205 (0.204) | NFE-F 39.1 | NFE-B 38.4 | Train Loss 16.3850
Epoch 0339 | Time 0.209 (0.204) | NFE-F 39.1 | NFE-B 38.4 | Train Loss 16.3359
Epoch 0340 | Time 0.206 (0.204) | NFE-F 39.1 | NFE-B 38.4 | Train Loss 16.2877
Epoch 0341 | Time 0.211 (0.204) | NFE-F 39.1 | NFE-B 38.4 | Train Loss 16.2403
Epoch 0342 | Time 0.208 (0.204) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 16.1938
Epoch 0343 | Time 0.207 (0.204) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 16.1481
Epoch 0344 | Time 0.207 (0.204) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 16.1032
Epoch 0345 | Time 0.210 (0.204) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 16.0591
Epoch 0346 | Time 0.212 (0.204) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 16.0158
Epoch 0347 | Time 0.209 (0.205) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 15.9733
Epoch 0348 | Time 0.205 (0.205) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 15.9315
Epoch 0349 | Time 0.218 (0.205) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 15.8905
Epoch 0350 | Time 0.210 (0.205) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 15.8503
Epoch 0351 | Time 0.219 (0.205) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 15.8107
Epoch 0352 | Time 0.211 (0.205) | NFE-F 39.2 | NFE-B 38.4 | Train Loss 15.7719
Epoch 0353 | Time 0.218 (0.205) | NFE-F 39.2 | NFE-B 38.5 | Train Loss 15.7338
Epoch 0354 | Time 0.211 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.6963
Epoch 0355 | Time 0.203 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.6596
Epoch 0356 | Time 0.201 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.6235
Epoch 0357 | Time 0.200 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.5880
Epoch 0358 | Time 0.199 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.5532
Epoch 0359 | Time 0.205 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.5191
Epoch 0360 | Time 0.205 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.4855
Epoch 0361 | Time 0.197 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.4526
Epoch 0362 | Time 0.204 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.4203
Epoch 0363 | Time 0.204 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.3885
Epoch 0364 | Time 0.208 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.3574
Epoch 0365 | Time 0.203 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.3268
Epoch 0366 | Time 0.204 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.2967
Epoch 0367 | Time 0.202 (0.205) | NFE-F 39.3 | NFE-B 38.5 | Train Loss 15.2672
Epoch 0368 | Time 0.212 (0.205) | NFE-F 39.4 | NFE-B 38.5 | Train Loss 15.2383
Epoch 0369 | Time 0.201 (0.205) | NFE-F 39.4 | NFE-B 38.5 | Train Loss 15.2098
Epoch 0370 | Time 0.224 (0.205) | NFE-F 39.4 | NFE-B 38.5 | Train Loss 15.1819
Epoch 0371 | Time 0.209 (0.205) | NFE-F 39.4 | NFE-B 38.5 | Train Loss 15.1545
Epoch 0372 | Time 0.205 (0.205) | NFE-F 39.4 | NFE-B 38.5 | Train Loss 15.1276
Epoch 0373 | Time 0.208 (0.205) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 15.1012
Epoch 0374 | Time 0.199 (0.205) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 15.0753
Epoch 0375 | Time 0.202 (0.205) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 15.0498
Epoch 0376 | Time 0.205 (0.205) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 15.0249
Epoch 0377 | Time 0.220 (0.205) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 15.0003
Epoch 0378 | Time 0.246 (0.206) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 14.9763
Epoch 0379 | Time 0.225 (0.206) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 14.9526
Epoch 0380 | Time 0.212 (0.206) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 14.9294
Epoch 0381 | Time 0.224 (0.206) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 14.9066
Epoch 0382 | Time 0.210 (0.206) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 14.8843
Epoch 0383 | Time 0.207 (0.206) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 14.8623
Epoch 0384 | Time 0.202 (0.206) | NFE-F 39.4 | NFE-B 38.6 | Train Loss 14.8408
Epoch 0385 | Time 0.203 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.8196
Epoch 0386 | Time 0.207 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.7989
Epoch 0387 | Time 0.215 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.7785
Epoch 0388 | Time 0.207 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.7584
Epoch 0389 | Time 0.209 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.7388
Epoch 0390 | Time 0.208 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.7195
Epoch 0391 | Time 0.203 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.7005
Epoch 0392 | Time 0.205 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.6819
Epoch 0393 | Time 0.201 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.6637
Epoch 0394 | Time 0.211 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.6458
Epoch 0395 | Time 0.199 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.6282
Epoch 0396 | Time 0.206 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.6109
Epoch 0397 | Time 0.213 (0.206) | NFE-F 39.5 | NFE-B 38.6 | Train Loss 14.5939
Epoch 0398 | Time 0.213 (0.206) | NFE-F 39.5 | NFE-B 38.7 | Train Loss 14.5773
Epoch 0399 | Time 0.204 (0.206) | NFE-F 39.5 | NFE-B 38.7 | Train Loss 14.5609
Epoch 0400 | Time 0.202 (0.206) | NFE-F 39.5 | NFE-B 38.7 | Train Loss 14.5448
Epoch 0401 | Time 0.204 (0.206) | NFE-F 39.5 | NFE-B 38.7 | Train Loss 14.5291
Epoch 0402 | Time 0.223 (0.206) | NFE-F 39.5 | NFE-B 38.7 | Train Loss 14.5136
Epoch 0403 | Time 0.199 (0.206) | NFE-F 39.5 | NFE-B 38.7 | Train Loss 14.4984
Epoch 0404 | Time 0.212 (0.206) | NFE-F 39.5 | NFE-B 38.7 | Train Loss 14.4834
Epoch 0405 | Time 0.209 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.4688
Epoch 0406 | Time 0.216 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.4544
Epoch 0407 | Time 0.203 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.4402
Epoch 0408 | Time 0.199 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.4263
Epoch 0409 | Time 0.203 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.4127
Epoch 0410 | Time 0.204 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3993
Epoch 0411 | Time 0.215 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3861
Epoch 0412 | Time 0.208 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3732
Epoch 0413 | Time 0.204 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3605
Epoch 0414 | Time 0.202 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3480
Epoch 0415 | Time 0.205 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3358
Epoch 0416 | Time 0.205 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3237
Epoch 0417 | Time 0.204 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3119
Epoch 0418 | Time 0.206 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.3003
Epoch 0419 | Time 0.205 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2888
Epoch 0420 | Time 0.206 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2776
Epoch 0421 | Time 0.207 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2666
Epoch 0422 | Time 0.203 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2558
Epoch 0423 | Time 0.206 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2451
Epoch 0424 | Time 0.210 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2347
Epoch 0425 | Time 0.203 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2244
Epoch 0426 | Time 0.220 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2143
Epoch 0427 | Time 0.208 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.2043
Epoch 0428 | Time 0.201 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.1946
Epoch 0429 | Time 0.206 (0.206) | NFE-F 39.6 | NFE-B 38.7 | Train Loss 14.1850
Epoch 0430 | Time 0.206 (0.206) | NFE-F 39.7 | NFE-B 38.7 | Train Loss 14.1755
Epoch 0431 | Time 0.212 (0.206) | NFE-F 39.7 | NFE-B 38.7 | Train Loss 14.1662
Epoch 0432 | Time 0.206 (0.206) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.1571
Epoch 0433 | Time 0.202 (0.206) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.1482
Epoch 0434 | Time 0.211 (0.206) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.1393
Epoch 0435 | Time 0.218 (0.206) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.1307
Epoch 0436 | Time 0.209 (0.206) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.1221
Epoch 0437 | Time 0.223 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.1137
Epoch 0438 | Time 0.198 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.1055
Epoch 0439 | Time 0.236 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0973
Epoch 0440 | Time 0.224 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0894
Epoch 0441 | Time 0.215 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0815
Epoch 0442 | Time 0.216 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0738
Epoch 0443 | Time 0.197 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0661
Epoch 0444 | Time 0.199 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0587
Epoch 0445 | Time 0.205 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0513
Epoch 0446 | Time 0.201 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0440
Epoch 0447 | Time 0.201 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0369
Epoch 0448 | Time 0.202 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0298
Epoch 0449 | Time 0.209 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0229
Epoch 0450 | Time 0.212 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0161
Epoch 0451 | Time 0.213 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0094
Epoch 0452 | Time 0.204 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 14.0028
Epoch 0453 | Time 0.208 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9963
Epoch 0454 | Time 0.204 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9898
Epoch 0455 | Time 0.207 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9835
Epoch 0456 | Time 0.200 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9773
Epoch 0457 | Time 0.201 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9712
Epoch 0458 | Time 0.200 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9651
Epoch 0459 | Time 0.206 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9592
Epoch 0460 | Time 0.214 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9533
Epoch 0461 | Time 0.213 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9475
Epoch 0462 | Time 0.200 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9418
Epoch 0463 | Time 0.201 (0.207) | NFE-F 39.7 | NFE-B 38.8 | Train Loss 13.9362
Epoch 0464 | Time 0.203 (0.207) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.9306
Epoch 0465 | Time 0.204 (0.207) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.9251
Epoch 0466 | Time 0.200 (0.207) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.9197
Epoch 0467 | Time 0.197 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.9144
Epoch 0468 | Time 0.198 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.9092
Epoch 0469 | Time 0.206 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.9040
Epoch 0470 | Time 0.199 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8989
Epoch 0471 | Time 0.204 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8938
Epoch 0472 | Time 0.198 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8888
Epoch 0473 | Time 0.213 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8839
Epoch 0474 | Time 0.201 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8791
Epoch 0475 | Time 0.212 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8743
Epoch 0476 | Time 0.204 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8696
Epoch 0477 | Time 0.202 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8649
Epoch 0478 | Time 0.202 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8603
Epoch 0479 | Time 0.207 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8557
Epoch 0480 | Time 0.198 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8512
Epoch 0481 | Time 0.201 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8468
Epoch 0482 | Time 0.199 (0.206) | NFE-F 39.8 | NFE-B 38.8 | Train Loss 13.8424
Epoch 0483 | Time 0.199 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8380
Epoch 0484 | Time 0.210 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8337
Epoch 0485 | Time 0.207 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8295
Epoch 0486 | Time 0.201 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8253
Epoch 0487 | Time 0.207 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8211
Epoch 0488 | Time 0.209 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8170
Epoch 0489 | Time 0.218 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8130
Epoch 0490 | Time 0.205 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8089
Epoch 0491 | Time 0.205 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8050
Epoch 0492 | Time 0.204 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.8010
Epoch 0493 | Time 0.203 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7971
Epoch 0494 | Time 0.218 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7933
Epoch 0495 | Time 0.199 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7895
Epoch 0496 | Time 0.209 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7857
Epoch 0497 | Time 0.204 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7820
Epoch 0498 | Time 0.204 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7783
Epoch 0499 | Time 0.209 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7746
Epoch 0500 | Time 0.205 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7710
Epoch 0501 | Time 0.207 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7674
Epoch 0502 | Time 0.210 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7638
Epoch 0503 | Time 0.206 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7603
Epoch 0504 | Time 0.208 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7568
Epoch 0505 | Time 0.205 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7533
Epoch 0506 | Time 0.211 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7499
Epoch 0507 | Time 0.208 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7465
Epoch 0508 | Time 0.204 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7431
Epoch 0509 | Time 0.203 (0.206) | NFE-F 39.8 | NFE-B 38.9 | Train Loss 13.7398
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=True, batch_size=16, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.089 (0.089) | NFE-F 14.0 | NFE-B 15.0 | Train Loss 69.0061
Epoch 0001 | Time 0.127 (0.090) | NFE-F 14.1 | NFE-B 15.0 | Train Loss 17.8707
Epoch 0002 | Time 0.119 (0.090) | NFE-F 14.3 | NFE-B 15.0 | Train Loss 59.9366
Epoch 0003 | Time 0.175 (0.091) | NFE-F 14.5 | NFE-B 15.1 | Train Loss 93.9462
Epoch 0004 | Time 0.150 (0.091) | NFE-F 14.7 | NFE-B 15.2 | Train Loss 87.2274
Epoch 0005 | Time 0.147 (0.092) | NFE-F 14.8 | NFE-B 15.2 | Train Loss 58.6282
Epoch 0006 | Time 0.146 (0.092) | NFE-F 14.9 | NFE-B 15.3 | Train Loss 30.0507
Epoch 0007 | Time 0.149 (0.093) | NFE-F 15.2 | NFE-B 15.4 | Train Loss 16.6512
Epoch 0008 | Time 0.172 (0.094) | NFE-F 15.4 | NFE-B 15.5 | Train Loss 21.8936
Epoch 0009 | Time 0.175 (0.095) | NFE-F 15.7 | NFE-B 15.6 | Train Loss 35.2043
Epoch 0010 | Time 0.167 (0.095) | NFE-F 15.9 | NFE-B 15.7 | Train Loss 44.2835
Epoch 0011 | Time 0.154 (0.096) | NFE-F 16.2 | NFE-B 15.8 | Train Loss 43.2281
Epoch 0012 | Time 0.171 (0.097) | NFE-F 16.4 | NFE-B 15.9 | Train Loss 33.3070
Epoch 0013 | Time 0.170 (0.097) | NFE-F 16.6 | NFE-B 16.0 | Train Loss 19.5911
Epoch 0014 | Time 0.193 (0.098) | NFE-F 16.9 | NFE-B 16.1 | Train Loss 17.1727
Epoch 0015 | Time 0.172 (0.099) | NFE-F 17.1 | NFE-B 16.3 | Train Loss 22.5024
Epoch 0016 | Time 0.194 (0.100) | NFE-F 17.3 | NFE-B 16.4 | Train Loss 26.8315
Epoch 0017 | Time 0.183 (0.101) | NFE-F 17.6 | NFE-B 16.5 | Train Loss 27.2895
Epoch 0018 | Time 0.171 (0.102) | NFE-F 17.8 | NFE-B 16.6 | Train Loss 24.3511
Epoch 0019 | Time 0.173 (0.102) | NFE-F 18.0 | NFE-B 16.7 | Train Loss 19.9471
Epoch 0020 | Time 0.167 (0.103) | NFE-F 18.2 | NFE-B 16.8 | Train Loss 16.3987
Epoch 0021 | Time 0.172 (0.104) | NFE-F 18.4 | NFE-B 16.9 | Train Loss 15.3239
Epoch 0022 | Time 0.183 (0.104) | NFE-F 18.7 | NFE-B 17.1 | Train Loss 16.6361
Epoch 0023 | Time 0.168 (0.105) | NFE-F 18.9 | NFE-B 17.2 | Train Loss 18.8442
Epoch 0024 | Time 0.182 (0.106) | NFE-F 19.1 | NFE-B 17.4 | Train Loss 20.1779
Epoch 0025 | Time 0.189 (0.107) | NFE-F 19.3 | NFE-B 17.5 | Train Loss 19.7202
Epoch 0026 | Time 0.184 (0.107) | NFE-F 19.5 | NFE-B 17.7 | Train Loss 17.8363
Epoch 0027 | Time 0.186 (0.108) | NFE-F 19.7 | NFE-B 17.8 | Train Loss 15.7350
Epoch 0028 | Time 0.187 (0.109) | NFE-F 19.9 | NFE-B 18.0 | Train Loss 14.5809
Epoch 0029 | Time 0.185 (0.110) | NFE-F 20.1 | NFE-B 18.1 | Train Loss 14.7675
Epoch 0030 | Time 0.199 (0.111) | NFE-F 20.3 | NFE-B 18.3 | Train Loss 15.7756
Epoch 0031 | Time 0.200 (0.112) | NFE-F 20.5 | NFE-B 18.4 | Train Loss 16.6425
Epoch 0032 | Time 0.186 (0.112) | NFE-F 20.7 | NFE-B 18.6 | Train Loss 16.6642
Epoch 0033 | Time 0.184 (0.113) | NFE-F 20.9 | NFE-B 18.7 | Train Loss 15.8248
Epoch 0034 | Time 0.184 (0.114) | NFE-F 21.1 | NFE-B 18.9 | Train Loss 14.6998
Epoch 0035 | Time 0.189 (0.115) | NFE-F 21.3 | NFE-B 19.0 | Train Loss 13.9751
Epoch 0036 | Time 0.184 (0.115) | NFE-F 21.5 | NFE-B 19.1 | Train Loss 13.9623
Epoch 0037 | Time 0.190 (0.116) | NFE-F 21.6 | NFE-B 19.3 | Train Loss 14.4377
Epoch 0038 | Time 0.190 (0.117) | NFE-F 21.8 | NFE-B 19.4 | Train Loss 14.8778
Epoch 0039 | Time 0.190 (0.117) | NFE-F 22.0 | NFE-B 19.5 | Train Loss 14.8754
Epoch 0040 | Time 0.188 (0.118) | NFE-F 22.2 | NFE-B 19.7 | Train Loss 14.4127
Epoch 0041 | Time 0.196 (0.119) | NFE-F 22.4 | NFE-B 19.8 | Train Loss 13.8143
Epoch 0042 | Time 0.192 (0.120) | NFE-F 22.5 | NFE-B 19.9 | Train Loss 13.4570
Epoch 0043 | Time 0.189 (0.120) | NFE-F 22.7 | NFE-B 20.1 | Train Loss 13.4851
Epoch 0044 | Time 0.191 (0.121) | NFE-F 22.9 | NFE-B 20.2 | Train Loss 13.7376
Epoch 0045 | Time 0.187 (0.122) | NFE-F 23.1 | NFE-B 20.3 | Train Loss 13.9160
Epoch 0046 | Time 0.195 (0.122) | NFE-F 23.2 | NFE-B 20.5 | Train Loss 13.8310
Epoch 0047 | Time 0.189 (0.123) | NFE-F 23.4 | NFE-B 20.6 | Train Loss 13.5307
Epoch 0048 | Time 0.189 (0.124) | NFE-F 23.6 | NFE-B 20.7 | Train Loss 13.2261
Epoch 0049 | Time 0.199 (0.124) | NFE-F 23.7 | NFE-B 20.8 | Train Loss 13.1009
Epoch 0050 | Time 0.191 (0.125) | NFE-F 23.9 | NFE-B 21.0 | Train Loss 13.1712
Epoch 0051 | Time 0.192 (0.126) | NFE-F 24.1 | NFE-B 21.1 | Train Loss 13.2985
Epoch 0052 | Time 0.190 (0.126) | NFE-F 24.2 | NFE-B 21.2 | Train Loss 13.3243
Epoch 0053 | Time 0.194 (0.127) | NFE-F 24.4 | NFE-B 21.3 | Train Loss 13.2016
Epoch 0054 | Time 0.183 (0.128) | NFE-F 24.5 | NFE-B 21.4 | Train Loss 13.0152
Epoch 0055 | Time 0.191 (0.128) | NFE-F 24.7 | NFE-B 21.5 | Train Loss 12.8913
Epoch 0056 | Time 0.187 (0.129) | NFE-F 24.8 | NFE-B 21.7 | Train Loss 12.8854
Epoch 0057 | Time 0.195 (0.130) | NFE-F 25.0 | NFE-B 21.8 | Train Loss 12.9477
Epoch 0058 | Time 0.186 (0.130) | NFE-F 25.1 | NFE-B 21.9 | Train Loss 12.9813
Epoch 0059 | Time 0.194 (0.131) | NFE-F 25.3 | NFE-B 22.0 | Train Loss 12.9320
Epoch 0060 | Time 0.189 (0.131) | NFE-F 25.4 | NFE-B 22.1 | Train Loss 12.8278
Epoch 0061 | Time 0.196 (0.132) | NFE-F 25.6 | NFE-B 22.2 | Train Loss 12.7410
Epoch 0062 | Time 0.189 (0.133) | NFE-F 25.7 | NFE-B 22.3 | Train Loss 12.7185
Epoch 0063 | Time 0.195 (0.133) | NFE-F 25.9 | NFE-B 22.4 | Train Loss 12.7446
Epoch 0064 | Time 0.190 (0.134) | NFE-F 26.0 | NFE-B 22.5 | Train Loss 12.7647
Epoch 0065 | Time 0.187 (0.134) | NFE-F 26.1 | NFE-B 22.6 | Train Loss 12.7402
Epoch 0066 | Time 0.187 (0.135) | NFE-F 26.3 | NFE-B 22.7 | Train Loss 12.6804
Epoch 0067 | Time 0.190 (0.135) | NFE-F 26.4 | NFE-B 22.8 | Train Loss 12.6259
Epoch 0068 | Time 0.184 (0.136) | NFE-F 26.6 | NFE-B 22.9 | Train Loss 12.6070
Epoch 0069 | Time 0.190 (0.136) | NFE-F 26.7 | NFE-B 23.0 | Train Loss 12.6171
Epoch 0070 | Time 0.198 (0.137) | NFE-F 26.8 | NFE-B 23.1 | Train Loss 12.6251
Epoch 0071 | Time 0.197 (0.138) | NFE-F 27.0 | NFE-B 23.2 | Train Loss 12.6080
Epoch 0072 | Time 0.203 (0.138) | NFE-F 27.1 | NFE-B 23.3 | Train Loss 12.5711
Epoch 0073 | Time 0.183 (0.139) | NFE-F 27.2 | NFE-B 23.4 | Train Loss 12.5381
Epoch 0074 | Time 0.185 (0.139) | NFE-F 27.3 | NFE-B 23.5 | Train Loss 12.5257
Epoch 0075 | Time 0.190 (0.140) | NFE-F 27.5 | NFE-B 23.6 | Train Loss 12.5288
Epoch 0076 | Time 0.187 (0.140) | NFE-F 27.6 | NFE-B 23.7 | Train Loss 12.5289
Epoch 0077 | Time 0.185 (0.141) | NFE-F 27.7 | NFE-B 23.8 | Train Loss 12.5143
Epoch 0078 | Time 0.191 (0.141) | NFE-F 27.8 | NFE-B 23.9 | Train Loss 12.4899
Epoch 0079 | Time 0.186 (0.142) | NFE-F 28.0 | NFE-B 24.0 | Train Loss 12.4699
Epoch 0080 | Time 0.186 (0.142) | NFE-F 28.1 | NFE-B 24.1 | Train Loss 12.4622
Epoch 0081 | Time 0.187 (0.142) | NFE-F 28.2 | NFE-B 24.2 | Train Loss 12.4615
Epoch 0082 | Time 0.190 (0.143) | NFE-F 28.3 | NFE-B 24.3 | Train Loss 12.4569
Epoch 0083 | Time 0.203 (0.144) | NFE-F 28.4 | NFE-B 24.4 | Train Loss 12.4437
Epoch 0084 | Time 0.189 (0.144) | NFE-F 28.6 | NFE-B 24.4 | Train Loss 12.4268
Epoch 0085 | Time 0.192 (0.144) | NFE-F 28.7 | NFE-B 24.5 | Train Loss 12.4144
Epoch 0086 | Time 0.182 (0.145) | NFE-F 28.8 | NFE-B 24.6 | Train Loss 12.4090
Epoch 0087 | Time 0.180 (0.145) | NFE-F 28.9 | NFE-B 24.7 | Train Loss 12.4057
Epoch 0088 | Time 0.187 (0.146) | NFE-F 29.0 | NFE-B 24.8 | Train Loss 12.3985
Epoch 0089 | Time 0.185 (0.146) | NFE-F 29.1 | NFE-B 24.9 | Train Loss 12.3867
Epoch 0090 | Time 0.190 (0.146) | NFE-F 29.2 | NFE-B 24.9 | Train Loss 12.3748
Epoch 0091 | Time 0.193 (0.147) | NFE-F 29.3 | NFE-B 25.0 | Train Loss 12.3666
Epoch 0092 | Time 0.187 (0.147) | NFE-F 29.4 | NFE-B 25.1 | Train Loss 12.3616
Epoch 0093 | Time 0.196 (0.148) | NFE-F 29.5 | NFE-B 25.2 | Train Loss 12.3562
Epoch 0094 | Time 0.184 (0.148) | NFE-F 29.6 | NFE-B 25.3 | Train Loss 12.3479
Epoch 0095 | Time 0.193 (0.149) | NFE-F 29.8 | NFE-B 25.3 | Train Loss 12.3379
Epoch 0096 | Time 0.187 (0.149) | NFE-F 29.9 | NFE-B 25.4 | Train Loss 12.3292
Epoch 0097 | Time 0.191 (0.149) | NFE-F 30.0 | NFE-B 25.5 | Train Loss 12.3228
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=1):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=True, batch_size=1, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.112 (0.112) | NFE-F 14.0 | NFE-B 21.0 | Train Loss 7668.9604
Epoch 0001 | Time 0.137 (0.112) | NFE-F 14.1 | NFE-B 21.0 | Train Loss 6238.5869
Epoch 0002 | Time 0.142 (0.112) | NFE-F 14.3 | NFE-B 21.0 | Train Loss 4938.9360
Epoch 0003 | Time 0.163 (0.113) | NFE-F 14.5 | NFE-B 21.1 | Train Loss 3832.8821
Epoch 0004 | Time 0.167 (0.113) | NFE-F 14.8 | NFE-B 21.1 | Train Loss 2917.4966
Epoch 0005 | Time 0.168 (0.114) | NFE-F 15.0 | NFE-B 21.2 | Train Loss 2145.7632
Epoch 0006 | Time 0.165 (0.114) | NFE-F 15.3 | NFE-B 21.2 | Train Loss 1504.4919
Epoch 0007 | Time 0.179 (0.115) | NFE-F 15.5 | NFE-B 21.4 | Train Loss 990.4551
Epoch 0008 | Time 0.185 (0.116) | NFE-F 15.8 | NFE-B 21.5 | Train Loss 597.6441
Epoch 0009 | Time 0.185 (0.116) | NFE-F 16.0 | NFE-B 21.6 | Train Loss 314.9131
Epoch 0010 | Time 0.177 (0.117) | NFE-F 16.3 | NFE-B 21.7 | Train Loss 130.3407
Epoch 0011 | Time 0.185 (0.118) | NFE-F 16.5 | NFE-B 21.8 | Train Loss 30.3823
Epoch 0012 | Time 0.201 (0.118) | NFE-F 16.7 | NFE-B 22.0 | Train Loss 0.0069
Epoch 0013 | Time 0.207 (0.119) | NFE-F 17.0 | NFE-B 22.2 | Train Loss 23.2638
Epoch 0014 | Time 0.200 (0.120) | NFE-F 17.2 | NFE-B 22.3 | Train Loss 84.0287
Epoch 0015 | Time 0.192 (0.121) | NFE-F 17.4 | NFE-B 22.5 | Train Loss 166.8393
Epoch 0016 | Time 0.200 (0.122) | NFE-F 17.7 | NFE-B 22.7 | Train Loss 257.7145
Epoch 0017 | Time 0.201 (0.122) | NFE-F 17.9 | NFE-B 22.8 | Train Loss 344.8524
Epoch 0018 | Time 0.195 (0.123) | NFE-F 18.1 | NFE-B 23.0 | Train Loss 419.1106
Epoch 0019 | Time 0.203 (0.124) | NFE-F 18.3 | NFE-B 23.1 | Train Loss 474.2299
Epoch 0020 | Time 0.189 (0.125) | NFE-F 18.5 | NFE-B 23.3 | Train Loss 506.7933
Epoch 0021 | Time 0.198 (0.125) | NFE-F 18.7 | NFE-B 23.5 | Train Loss 515.9657
Epoch 0022 | Time 0.192 (0.126) | NFE-F 19.0 | NFE-B 23.6 | Train Loss 503.0760
Epoch 0023 | Time 0.192 (0.127) | NFE-F 19.2 | NFE-B 23.8 | Train Loss 471.1038
Epoch 0024 | Time 0.190 (0.127) | NFE-F 19.4 | NFE-B 23.9 | Train Loss 424.1484
Epoch 0025 | Time 0.197 (0.128) | NFE-F 19.6 | NFE-B 24.1 | Train Loss 366.9056
Epoch 0026 | Time 0.208 (0.129) | NFE-F 19.8 | NFE-B 24.2 | Train Loss 304.2090
Epoch 0027 | Time 0.202 (0.130) | NFE-F 20.0 | NFE-B 24.4 | Train Loss 240.6373
Epoch 0028 | Time 0.202 (0.130) | NFE-F 20.2 | NFE-B 24.5 | Train Loss 180.2084
Epoch 0029 | Time 0.198 (0.131) | NFE-F 20.4 | NFE-B 24.7 | Train Loss 126.1590
Epoch 0030 | Time 0.191 (0.132) | NFE-F 20.6 | NFE-B 24.8 | Train Loss 80.8138
Epoch 0031 | Time 0.200 (0.132) | NFE-F 20.8 | NFE-B 24.9 | Train Loss 45.5336
Epoch 0032 | Time 0.195 (0.133) | NFE-F 21.0 | NFE-B 25.1 | Train Loss 20.7415
Epoch 0033 | Time 0.195 (0.133) | NFE-F 21.2 | NFE-B 25.2 | Train Loss 6.0152
Epoch 0034 | Time 0.196 (0.134) | NFE-F 21.4 | NFE-B 25.4 | Train Loss 0.2322
Epoch 0035 | Time 0.197 (0.135) | NFE-F 21.5 | NFE-B 25.5 | Train Loss 1.7533
Epoch 0036 | Time 0.208 (0.135) | NFE-F 21.7 | NFE-B 25.6 | Train Loss 8.6268
Epoch 0037 | Time 0.189 (0.136) | NFE-F 21.9 | NFE-B 25.8 | Train Loss 18.7938
Epoch 0038 | Time 0.197 (0.137) | NFE-F 22.1 | NFE-B 25.9 | Train Loss 30.2778
Epoch 0039 | Time 0.193 (0.137) | NFE-F 22.3 | NFE-B 26.0 | Train Loss 41.3437
Epoch 0040 | Time 0.196 (0.138) | NFE-F 22.4 | NFE-B 26.2 | Train Loss 50.6136
Epoch 0041 | Time 0.198 (0.138) | NFE-F 22.6 | NFE-B 26.3 | Train Loss 57.1356
Epoch 0042 | Time 0.192 (0.139) | NFE-F 22.8 | NFE-B 26.4 | Train Loss 60.4033
Epoch 0043 | Time 0.200 (0.140) | NFE-F 23.0 | NFE-B 26.5 | Train Loss 60.3324
Epoch 0044 | Time 0.192 (0.140) | NFE-F 23.1 | NFE-B 26.7 | Train Loss 57.2002
Epoch 0045 | Time 0.197 (0.141) | NFE-F 23.3 | NFE-B 26.8 | Train Loss 51.5612
Epoch 0046 | Time 0.194 (0.141) | NFE-F 23.5 | NFE-B 26.9 | Train Loss 44.1480
Epoch 0047 | Time 0.195 (0.142) | NFE-F 23.6 | NFE-B 27.0 | Train Loss 35.7713
Epoch 0048 | Time 0.204 (0.142) | NFE-F 23.8 | NFE-B 27.2 | Train Loss 27.2266
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=1):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=True, batch_size=1, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.087 (0.087) | NFE-F 14.0 | NFE-B 15.0 | Train Loss 771.8973
Epoch 0001 | Time 0.129 (0.088) | NFE-F 14.1 | NFE-B 15.0 | Train Loss 727.0522
Epoch 0002 | Time 0.131 (0.088) | NFE-F 14.3 | NFE-B 15.0 | Train Loss 683.5258
Epoch 0003 | Time 0.118 (0.089) | NFE-F 14.4 | NFE-B 15.0 | Train Loss 641.2979
Epoch 0004 | Time 0.136 (0.089) | NFE-F 14.6 | NFE-B 15.0 | Train Loss 600.3624
Epoch 0005 | Time 0.156 (0.090) | NFE-F 14.7 | NFE-B 15.1 | Train Loss 560.7086
Epoch 0006 | Time 0.144 (0.090) | NFE-F 14.8 | NFE-B 15.1 | Train Loss 522.3381
Epoch 0007 | Time 0.139 (0.091) | NFE-F 15.0 | NFE-B 15.2 | Train Loss 485.2486
Epoch 0008 | Time 0.145 (0.091) | NFE-F 15.1 | NFE-B 15.2 | Train Loss 449.4439
Epoch 0009 | Time 0.154 (0.092) | NFE-F 15.2 | NFE-B 15.3 | Train Loss 414.9249
Epoch 0010 | Time 0.138 (0.092) | NFE-F 15.3 | NFE-B 15.4 | Train Loss 381.7010
Epoch 0011 | Time 0.144 (0.093) | NFE-F 15.5 | NFE-B 15.4 | Train Loss 349.7969
Epoch 0012 | Time 0.140 (0.093) | NFE-F 15.6 | NFE-B 15.5 | Train Loss 319.2288
Epoch 0013 | Time 0.146 (0.094) | NFE-F 15.8 | NFE-B 15.5 | Train Loss 290.0531
Epoch 0014 | Time 0.144 (0.094) | NFE-F 16.0 | NFE-B 15.6 | Train Loss 262.3084
Epoch 0015 | Time 0.148 (0.095) | NFE-F 16.1 | NFE-B 15.6 | Train Loss 236.0856
Epoch 0016 | Time 0.165 (0.096) | NFE-F 16.3 | NFE-B 15.7 | Train Loss 211.4622
Epoch 0017 | Time 0.161 (0.096) | NFE-F 16.6 | NFE-B 15.9 | Train Loss 188.5203
Epoch 0018 | Time 0.167 (0.097) | NFE-F 16.8 | NFE-B 16.0 | Train Loss 167.3265
Epoch 0019 | Time 0.162 (0.098) | NFE-F 17.0 | NFE-B 16.1 | Train Loss 147.9124
Epoch 0020 | Time 0.165 (0.098) | NFE-F 17.2 | NFE-B 16.2 | Train Loss 130.2635
Epoch 0021 | Time 0.160 (0.099) | NFE-F 17.5 | NFE-B 16.3 | Train Loss 114.3149
Epoch 0022 | Time 0.161 (0.100) | NFE-F 17.7 | NFE-B 16.4 | Train Loss 99.9667
Epoch 0023 | Time 0.169 (0.100) | NFE-F 17.9 | NFE-B 16.5 | Train Loss 87.0943
Epoch 0024 | Time 0.175 (0.101) | NFE-F 18.1 | NFE-B 16.6 | Train Loss 75.5671
Epoch 0025 | Time 0.169 (0.102) | NFE-F 18.4 | NFE-B 16.7 | Train Loss 65.2606
Epoch 0026 | Time 0.168 (0.102) | NFE-F 18.6 | NFE-B 16.8 | Train Loss 56.0631
Epoch 0027 | Time 0.166 (0.103) | NFE-F 18.8 | NFE-B 16.9 | Train Loss 47.8753
Epoch 0028 | Time 0.168 (0.104) | NFE-F 19.0 | NFE-B 17.0 | Train Loss 40.6106
Epoch 0029 | Time 0.170 (0.104) | NFE-F 19.2 | NFE-B 17.1 | Train Loss 34.1911
Epoch 0030 | Time 0.172 (0.105) | NFE-F 19.4 | NFE-B 17.2 | Train Loss 28.5464
Epoch 0031 | Time 0.170 (0.106) | NFE-F 19.6 | NFE-B 17.3 | Train Loss 23.6114
Epoch 0032 | Time 0.167 (0.106) | NFE-F 19.8 | NFE-B 17.4 | Train Loss 19.3251
Epoch 0033 | Time 0.169 (0.107) | NFE-F 20.0 | NFE-B 17.5 | Train Loss 15.6298
Epoch 0034 | Time 0.166 (0.107) | NFE-F 20.2 | NFE-B 17.5 | Train Loss 12.4706
Epoch 0035 | Time 0.167 (0.108) | NFE-F 20.4 | NFE-B 17.6 | Train Loss 9.7953
Epoch 0036 | Time 0.165 (0.109) | NFE-F 20.6 | NFE-B 17.7 | Train Loss 7.5544
Epoch 0037 | Time 0.165 (0.109) | NFE-F 20.8 | NFE-B 17.8 | Train Loss 5.7007
Epoch 0038 | Time 0.165 (0.110) | NFE-F 21.0 | NFE-B 17.9 | Train Loss 4.1897
Epoch 0039 | Time 0.165 (0.110) | NFE-F 21.2 | NFE-B 18.0 | Train Loss 2.9796
Epoch 0040 | Time 0.164 (0.111) | NFE-F 21.4 | NFE-B 18.1 | Train Loss 2.0313
Epoch 0041 | Time 0.161 (0.111) | NFE-F 21.6 | NFE-B 18.2 | Train Loss 1.3084
Epoch 0042 | Time 0.178 (0.112) | NFE-F 21.8 | NFE-B 18.3 | Train Loss 0.7774
Epoch 0043 | Time 0.167 (0.113) | NFE-F 21.9 | NFE-B 18.4 | Train Loss 0.4076
Epoch 0044 | Time 0.168 (0.113) | NFE-F 22.1 | NFE-B 18.5 | Train Loss 0.1712
Epoch 0045 | Time 0.164 (0.114) | NFE-F 22.3 | NFE-B 18.5 | Train Loss 0.0429
Epoch 0046 | Time 0.165 (0.114) | NFE-F 22.5 | NFE-B 18.6 | Train Loss 0.0003
Epoch 0047 | Time 0.164 (0.115) | NFE-F 22.7 | NFE-B 18.7 | Train Loss 0.0237
Epoch 0048 | Time 0.177 (0.115) | NFE-F 22.8 | NFE-B 18.8 | Train Loss 0.0958
Epoch 0049 | Time 0.164 (0.116) | NFE-F 23.0 | NFE-B 18.9 | Train Loss 0.2015
Epoch 0050 | Time 0.183 (0.116) | NFE-F 23.2 | NFE-B 19.0 | Train Loss 0.3284
Epoch 0051 | Time 0.164 (0.117) | NFE-F 23.3 | NFE-B 19.1 | Train Loss 0.4657
Epoch 0052 | Time 0.171 (0.117) | NFE-F 23.5 | NFE-B 19.2 | Train Loss 0.6048
Epoch 0053 | Time 0.170 (0.118) | NFE-F 23.7 | NFE-B 19.3 | Train Loss 0.7387
Epoch 0054 | Time 0.165 (0.118) | NFE-F 23.8 | NFE-B 19.3 | Train Loss 0.8623
Epoch 0055 | Time 0.164 (0.119) | NFE-F 24.0 | NFE-B 19.4 | Train Loss 0.9715
Epoch 0056 | Time 0.164 (0.119) | NFE-F 24.2 | NFE-B 19.5 | Train Loss 1.0636
Epoch 0057 | Time 0.163 (0.120) | NFE-F 24.3 | NFE-B 19.6 | Train Loss 1.1371
Epoch 0058 | Time 0.165 (0.120) | NFE-F 24.5 | NFE-B 19.6 | Train Loss 1.1912
Epoch 0059 | Time 0.170 (0.121) | NFE-F 24.6 | NFE-B 19.7 | Train Loss 1.2260
Epoch 0060 | Time 0.164 (0.121) | NFE-F 24.8 | NFE-B 19.8 | Train Loss 1.2422
Epoch 0061 | Time 0.163 (0.122) | NFE-F 24.9 | NFE-B 19.8 | Train Loss 1.2408
Epoch 0062 | Time 0.164 (0.122) | NFE-F 25.1 | NFE-B 19.9 | Train Loss 1.2235
Epoch 0063 | Time 0.165 (0.122) | NFE-F 25.2 | NFE-B 20.0 | Train Loss 1.1920
Epoch 0064 | Time 0.170 (0.123) | NFE-F 25.4 | NFE-B 20.1 | Train Loss 1.1483
Epoch 0065 | Time 0.170 (0.123) | NFE-F 25.5 | NFE-B 20.1 | Train Loss 1.0943
Epoch 0066 | Time 0.165 (0.124) | NFE-F 25.7 | NFE-B 20.2 | Train Loss 1.0321
Epoch 0067 | Time 0.166 (0.124) | NFE-F 25.8 | NFE-B 20.3 | Train Loss 0.9638
Epoch 0068 | Time 0.180 (0.125) | NFE-F 26.0 | NFE-B 20.4 | Train Loss 0.8912
Epoch 0069 | Time 0.178 (0.125) | NFE-F 26.1 | NFE-B 20.5 | Train Loss 0.8160
Epoch 0070 | Time 0.176 (0.126) | NFE-F 26.2 | NFE-B 20.6 | Train Loss 0.7400
Epoch 0071 | Time 0.181 (0.126) | NFE-F 26.4 | NFE-B 20.8 | Train Loss 0.6645
Epoch 0072 | Time 0.175 (0.127) | NFE-F 26.5 | NFE-B 20.9 | Train Loss 0.5907
Epoch 0073 | Time 0.186 (0.127) | NFE-F 26.6 | NFE-B 21.0 | Train Loss 0.5197
Epoch 0074 | Time 0.161 (0.128) | NFE-F 26.8 | NFE-B 21.1 | Train Loss 0.4524
Epoch 0075 | Time 0.164 (0.128) | NFE-F 26.9 | NFE-B 21.1 | Train Loss 0.3894
Epoch 0076 | Time 0.163 (0.128) | NFE-F 27.0 | NFE-B 21.2 | Train Loss 0.3312
Epoch 0077 | Time 0.173 (0.129) | NFE-F 27.2 | NFE-B 21.2 | Train Loss 0.2781
Epoch 0078 | Time 0.164 (0.129) | NFE-F 27.3 | NFE-B 21.3 | Train Loss 0.2303
Epoch 0079 | Time 0.162 (0.130) | NFE-F 27.4 | NFE-B 21.4 | Train Loss 0.1878
Epoch 0080 | Time 0.163 (0.130) | NFE-F 27.6 | NFE-B 21.4 | Train Loss 0.1505
Epoch 0081 | Time 0.166 (0.130) | NFE-F 27.7 | NFE-B 21.5 | Train Loss 0.1184
Epoch 0082 | Time 0.168 (0.131) | NFE-F 27.8 | NFE-B 21.5 | Train Loss 0.0910
Epoch 0083 | Time 0.176 (0.131) | NFE-F 27.9 | NFE-B 21.6 | Train Loss 0.0681
Epoch 0084 | Time 0.168 (0.131) | NFE-F 28.0 | NFE-B 21.6 | Train Loss 0.0493
Epoch 0085 | Time 0.171 (0.132) | NFE-F 28.2 | NFE-B 21.7 | Train Loss 0.0343
Epoch 0086 | Time 0.165 (0.132) | NFE-F 28.3 | NFE-B 21.7 | Train Loss 0.0227
Epoch 0087 | Time 0.173 (0.133) | NFE-F 28.4 | NFE-B 21.8 | Train Loss 0.0139
Epoch 0088 | Time 0.162 (0.133) | NFE-F 28.5 | NFE-B 21.9 | Train Loss 0.0076
Epoch 0089 | Time 0.172 (0.133) | NFE-F 28.6 | NFE-B 21.9 | Train Loss 0.0035
Epoch 0090 | Time 0.161 (0.134) | NFE-F 28.7 | NFE-B 22.0 | Train Loss 0.0011
Epoch 0091 | Time 0.191 (0.134) | NFE-F 28.9 | NFE-B 22.1 | Train Loss 0.0001
Epoch 0092 | Time 0.183 (0.135) | NFE-F 29.0 | NFE-B 22.2 | Train Loss 0.0002
Epoch 0093 | Time 0.181 (0.135) | NFE-F 29.1 | NFE-B 22.3 | Train Loss 0.0010
Epoch 0094 | Time 0.180 (0.136) | NFE-F 29.2 | NFE-B 22.4 | Train Loss 0.0024
Epoch 0095 | Time 0.169 (0.136) | NFE-F 29.3 | NFE-B 22.4 | Train Loss 0.0042
Epoch 0096 | Time 0.182 (0.136) | NFE-F 29.4 | NFE-B 22.5 | Train Loss 0.0061
Epoch 0097 | Time 0.171 (0.137) | NFE-F 29.5 | NFE-B 22.6 | Train Loss 0.0080
Epoch 0098 | Time 0.171 (0.137) | NFE-F 29.6 | NFE-B 22.6 | Train Loss 0.0098
Epoch 0099 | Time 0.168 (0.137) | NFE-F 29.7 | NFE-B 22.7 | Train Loss 0.0115
Epoch 0100 | Time 0.171 (0.138) | NFE-F 29.8 | NFE-B 22.7 | Train Loss 0.0128
Epoch 0101 | Time 0.165 (0.138) | NFE-F 29.9 | NFE-B 22.8 | Train Loss 0.0139
Epoch 0102 | Time 0.174 (0.138) | NFE-F 30.0 | NFE-B 22.8 | Train Loss 0.0147
Epoch 0103 | Time 0.165 (0.139) | NFE-F 30.1 | NFE-B 22.8 | Train Loss 0.0152
Epoch 0104 | Time 0.166 (0.139) | NFE-F 30.2 | NFE-B 22.9 | Train Loss 0.0153
Epoch 0105 | Time 0.161 (0.139) | NFE-F 30.3 | NFE-B 22.9 | Train Loss 0.0152
Epoch 0106 | Time 0.173 (0.139) | NFE-F 30.4 | NFE-B 23.0 | Train Loss 0.0149
Epoch 0107 | Time 0.162 (0.140) | NFE-F 30.5 | NFE-B 23.0 | Train Loss 0.0143
Epoch 0108 | Time 0.167 (0.140) | NFE-F 30.6 | NFE-B 23.0 | Train Loss 0.0136
Epoch 0109 | Time 0.162 (0.140) | NFE-F 30.7 | NFE-B 23.1 | Train Loss 0.0127
Epoch 0110 | Time 0.166 (0.140) | NFE-F 30.8 | NFE-B 23.1 | Train Loss 0.0117
Epoch 0111 | Time 0.161 (0.141) | NFE-F 30.9 | NFE-B 23.2 | Train Loss 0.0106
Epoch 0112 | Time 0.167 (0.141) | NFE-F 31.0 | NFE-B 23.2 | Train Loss 0.0095
Epoch 0113 | Time 0.170 (0.141) | NFE-F 31.1 | NFE-B 23.2 | Train Loss 0.0085
Epoch 0114 | Time 0.163 (0.141) | NFE-F 31.2 | NFE-B 23.3 | Train Loss 0.0074
Epoch 0115 | Time 0.167 (0.142) | NFE-F 31.2 | NFE-B 23.3 | Train Loss 0.0063
Epoch 0116 | Time 0.169 (0.142) | NFE-F 31.3 | NFE-B 23.4 | Train Loss 0.0054
Epoch 0117 | Time 0.189 (0.142) | NFE-F 31.4 | NFE-B 23.4 | Train Loss 0.0045
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing
import pdb

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w, b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	# pdb.set_trace()
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=True, batch_size=16, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.001, nepochs=1000, save='./experiment1', tol=0.0001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.130 (0.130) | NFE-F 14.0 | NFE-B 21.0 | Train Loss 5341.1455
Epoch 0001 | Time 0.144 (0.130) | NFE-F 14.1 | NFE-B 21.0 | Train Loss 5222.4849
Epoch 0002 | Time 0.141 (0.130) | NFE-F 14.3 | NFE-B 21.0 | Train Loss 5104.9404
Epoch 0003 | Time 0.147 (0.130) | NFE-F 14.4 | NFE-B 21.0 | Train Loss 4988.3770
Epoch 0004 | Time 0.145 (0.130) | NFE-F 14.6 | NFE-B 21.0 | Train Loss 4872.6826
Epoch 0005 | Time 0.149 (0.130) | NFE-F 14.7 | NFE-B 21.0 | Train Loss 4757.7456
Epoch 0006 | Time 0.155 (0.131) | NFE-F 14.8 | NFE-B 21.0 | Train Loss 4643.4697
Epoch 0007 | Time 0.146 (0.131) | NFE-F 15.0 | NFE-B 21.0 | Train Loss 4529.7632
Epoch 0008 | Time 0.146 (0.131) | NFE-F 15.1 | NFE-B 21.0 | Train Loss 4416.5469
Epoch 0009 | Time 0.149 (0.131) | NFE-F 15.2 | NFE-B 21.0 | Train Loss 4303.7148
Epoch 0010 | Time 0.144 (0.131) | NFE-F 15.3 | NFE-B 21.0 | Train Loss 4191.2109
Epoch 0011 | Time 0.152 (0.131) | NFE-F 15.5 | NFE-B 21.0 | Train Loss 4078.9688
Epoch 0012 | Time 0.143 (0.132) | NFE-F 15.6 | NFE-B 21.0 | Train Loss 3967.0066
Epoch 0013 | Time 0.153 (0.132) | NFE-F 15.7 | NFE-B 21.0 | Train Loss 3855.3699
Epoch 0014 | Time 0.148 (0.132) | NFE-F 15.8 | NFE-B 21.0 | Train Loss 3744.2349
Epoch 0015 | Time 0.147 (0.132) | NFE-F 16.0 | NFE-B 21.0 | Train Loss 3633.8870
Epoch 0016 | Time 0.159 (0.132) | NFE-F 16.2 | NFE-B 21.0 | Train Loss 3524.7378
Epoch 0017 | Time 0.151 (0.133) | NFE-F 16.4 | NFE-B 21.0 | Train Loss 3417.3303
Epoch 0018 | Time 0.167 (0.133) | NFE-F 16.7 | NFE-B 21.1 | Train Loss 3312.2798
Epoch 0019 | Time 0.177 (0.133) | NFE-F 16.9 | NFE-B 21.1 | Train Loss 3210.2034
Epoch 0020 | Time 0.173 (0.134) | NFE-F 17.1 | NFE-B 21.2 | Train Loss 3111.5950
Epoch 0021 | Time 0.165 (0.134) | NFE-F 17.4 | NFE-B 21.2 | Train Loss 3016.7356
Epoch 0022 | Time 0.170 (0.134) | NFE-F 17.6 | NFE-B 21.3 | Train Loss 2925.7046
Epoch 0023 | Time 0.170 (0.135) | NFE-F 17.8 | NFE-B 21.4 | Train Loss 2838.3328
Epoch 0024 | Time 0.149 (0.135) | NFE-F 18.0 | NFE-B 21.3 | Train Loss 2754.3066
Epoch 0025 | Time 0.184 (0.135) | NFE-F 18.3 | NFE-B 21.5 | Train Loss 2673.2600
Epoch 0026 | Time 0.171 (0.136) | NFE-F 18.5 | NFE-B 21.5 | Train Loss 2594.8391
Epoch 0027 | Time 0.173 (0.136) | NFE-F 18.7 | NFE-B 21.6 | Train Loss 2518.7432
Epoch 0028 | Time 0.169 (0.136) | NFE-F 18.9 | NFE-B 21.6 | Train Loss 2444.7371
Epoch 0029 | Time 0.171 (0.137) | NFE-F 19.1 | NFE-B 21.7 | Train Loss 2372.6453
Epoch 0030 | Time 0.174 (0.137) | NFE-F 19.3 | NFE-B 21.7 | Train Loss 2302.3401
Epoch 0031 | Time 0.173 (0.138) | NFE-F 19.5 | NFE-B 21.8 | Train Loss 2233.7290
Epoch 0032 | Time 0.190 (0.138) | NFE-F 19.7 | NFE-B 21.8 | Train Loss 2166.7446
Epoch 0033 | Time 0.171 (0.138) | NFE-F 19.9 | NFE-B 21.9 | Train Loss 2101.3379
Epoch 0034 | Time 0.184 (0.139) | NFE-F 20.1 | NFE-B 21.9 | Train Loss 2037.4675
Epoch 0035 | Time 0.169 (0.139) | NFE-F 20.3 | NFE-B 22.0 | Train Loss 1975.1028
Epoch 0036 | Time 0.187 (0.140) | NFE-F 20.5 | NFE-B 22.0 | Train Loss 1914.2155
Epoch 0037 | Time 0.170 (0.140) | NFE-F 20.7 | NFE-B 22.1 | Train Loss 1854.7820
Epoch 0038 | Time 0.167 (0.140) | NFE-F 20.9 | NFE-B 22.1 | Train Loss 1796.7788
Epoch 0039 | Time 0.167 (0.140) | NFE-F 21.1 | NFE-B 22.2 | Train Loss 1740.1862
Epoch 0040 | Time 0.170 (0.141) | NFE-F 21.3 | NFE-B 22.2 | Train Loss 1684.9814
Epoch 0041 | Time 0.166 (0.141) | NFE-F 21.5 | NFE-B 22.3 | Train Loss 1631.1455
Epoch 0042 | Time 0.169 (0.141) | NFE-F 21.7 | NFE-B 22.3 | Train Loss 1578.6586
Epoch 0043 | Time 0.170 (0.142) | NFE-F 21.9 | NFE-B 22.4 | Train Loss 1527.5011
Epoch 0044 | Time 0.166 (0.142) | NFE-F 22.0 | NFE-B 22.4 | Train Loss 1477.6519
Epoch 0045 | Time 0.169 (0.142) | NFE-F 22.2 | NFE-B 22.5 | Train Loss 1429.0917
Epoch 0046 | Time 0.165 (0.142) | NFE-F 22.4 | NFE-B 22.5 | Train Loss 1381.8008
Epoch 0047 | Time 0.171 (0.143) | NFE-F 22.6 | NFE-B 22.6 | Train Loss 1335.7587
Epoch 0048 | Time 0.165 (0.143) | NFE-F 22.7 | NFE-B 22.6 | Train Loss 1290.9449
Epoch 0049 | Time 0.170 (0.143) | NFE-F 22.9 | NFE-B 22.7 | Train Loss 1247.3395
Epoch 0050 | Time 0.167 (0.143) | NFE-F 23.1 | NFE-B 22.7 | Train Loss 1204.9219
Epoch 0051 | Time 0.163 (0.144) | NFE-F 23.3 | NFE-B 22.7 | Train Loss 1163.6725
Epoch 0052 | Time 0.169 (0.144) | NFE-F 23.4 | NFE-B 22.8 | Train Loss 1123.5701
Epoch 0053 | Time 0.191 (0.144) | NFE-F 23.6 | NFE-B 22.9 | Train Loss 1084.5939
Epoch 0054 | Time 0.189 (0.145) | NFE-F 23.8 | NFE-B 23.0 | Train Loss 1046.7240
Epoch 0055 | Time 0.181 (0.145) | NFE-F 23.9 | NFE-B 23.1 | Train Loss 1009.9393
Epoch 0056 | Time 0.174 (0.145) | NFE-F 24.1 | NFE-B 23.1 | Train Loss 974.2196
Epoch 0057 | Time 0.178 (0.146) | NFE-F 24.2 | NFE-B 23.2 | Train Loss 939.5441
Epoch 0058 | Time 0.170 (0.146) | NFE-F 24.4 | NFE-B 23.2 | Train Loss 905.8920
Epoch 0059 | Time 0.171 (0.146) | NFE-F 24.6 | NFE-B 23.2 | Train Loss 873.2433
Epoch 0060 | Time 0.171 (0.146) | NFE-F 24.7 | NFE-B 23.3 | Train Loss 841.5771
Epoch 0061 | Time 0.175 (0.147) | NFE-F 24.9 | NFE-B 23.3 | Train Loss 810.8734
Epoch 0062 | Time 0.170 (0.147) | NFE-F 25.0 | NFE-B 23.3 | Train Loss 781.1118
Epoch 0063 | Time 0.168 (0.147) | NFE-F 25.2 | NFE-B 23.4 | Train Loss 752.2717
Epoch 0064 | Time 0.165 (0.147) | NFE-F 25.3 | NFE-B 23.4 | Train Loss 724.3339
Epoch 0065 | Time 0.170 (0.148) | NFE-F 25.5 | NFE-B 23.5 | Train Loss 697.2776
Epoch 0066 | Time 0.178 (0.148) | NFE-F 25.6 | NFE-B 23.5 | Train Loss 671.0834
Epoch 0067 | Time 0.173 (0.148) | NFE-F 25.7 | NFE-B 23.5 | Train Loss 645.7315
Epoch 0068 | Time 0.177 (0.148) | NFE-F 25.9 | NFE-B 23.6 | Train Loss 621.2023
Epoch 0069 | Time 0.166 (0.149) | NFE-F 26.0 | NFE-B 23.6 | Train Loss 597.4766
Epoch 0070 | Time 0.167 (0.149) | NFE-F 26.2 | NFE-B 23.6 | Train Loss 574.5349
Epoch 0071 | Time 0.170 (0.149) | NFE-F 26.3 | NFE-B 23.7 | Train Loss 552.3585
Epoch 0072 | Time 0.180 (0.149) | NFE-F 26.4 | NFE-B 23.7 | Train Loss 530.9289
Epoch 0073 | Time 0.171 (0.149) | NFE-F 26.6 | NFE-B 23.7 | Train Loss 510.2266
Epoch 0074 | Time 0.166 (0.150) | NFE-F 26.7 | NFE-B 23.8 | Train Loss 490.2337
Epoch 0075 | Time 0.170 (0.150) | NFE-F 26.8 | NFE-B 23.8 | Train Loss 470.9324
Epoch 0076 | Time 0.168 (0.150) | NFE-F 27.0 | NFE-B 23.8 | Train Loss 452.3041
Epoch 0077 | Time 0.172 (0.150) | NFE-F 27.1 | NFE-B 23.9 | Train Loss 434.3315
Epoch 0078 | Time 0.175 (0.151) | NFE-F 27.2 | NFE-B 23.9 | Train Loss 416.9967
Epoch 0079 | Time 0.173 (0.151) | NFE-F 27.4 | NFE-B 23.9 | Train Loss 400.2827
Epoch 0080 | Time 0.168 (0.151) | NFE-F 27.5 | NFE-B 24.0 | Train Loss 384.1725
Epoch 0081 | Time 0.170 (0.151) | NFE-F 27.6 | NFE-B 24.0 | Train Loss 368.6493
Epoch 0082 | Time 0.171 (0.151) | NFE-F 27.7 | NFE-B 24.0 | Train Loss 353.6966
Epoch 0083 | Time 0.174 (0.152) | NFE-F 27.9 | NFE-B 24.0 | Train Loss 339.2981
Epoch 0084 | Time 0.171 (0.152) | NFE-F 28.0 | NFE-B 24.1 | Train Loss 325.4380
Epoch 0085 | Time 0.169 (0.152) | NFE-F 28.1 | NFE-B 24.1 | Train Loss 312.1004
Epoch 0086 | Time 0.167 (0.152) | NFE-F 28.2 | NFE-B 24.1 | Train Loss 299.2699
Epoch 0087 | Time 0.167 (0.152) | NFE-F 28.3 | NFE-B 24.2 | Train Loss 286.9314
Epoch 0088 | Time 0.167 (0.152) | NFE-F 28.5 | NFE-B 24.2 | Train Loss 275.0700
Epoch 0089 | Time 0.173 (0.153) | NFE-F 28.6 | NFE-B 24.2 | Train Loss 263.6710
Epoch 0090 | Time 0.168 (0.153) | NFE-F 28.7 | NFE-B 24.2 | Train Loss 252.7202
Epoch 0091 | Time 0.172 (0.153) | NFE-F 28.8 | NFE-B 24.3 | Train Loss 242.2033
Epoch 0092 | Time 0.170 (0.153) | NFE-F 28.9 | NFE-B 24.3 | Train Loss 232.1070
Epoch 0093 | Time 0.165 (0.153) | NFE-F 29.0 | NFE-B 24.3 | Train Loss 222.4174
Epoch 0094 | Time 0.166 (0.153) | NFE-F 29.1 | NFE-B 24.4 | Train Loss 213.1215
Epoch 0095 | Time 0.179 (0.154) | NFE-F 29.2 | NFE-B 24.4 | Train Loss 204.2063
Epoch 0096 | Time 0.188 (0.154) | NFE-F 29.3 | NFE-B 24.4 | Train Loss 195.6592
Epoch 0097 | Time 0.188 (0.154) | NFE-F 29.5 | NFE-B 24.4 | Train Loss 187.4678
Epoch 0098 | Time 0.176 (0.154) | NFE-F 29.6 | NFE-B 24.5 | Train Loss 179.6202
Epoch 0099 | Time 0.173 (0.155) | NFE-F 29.7 | NFE-B 24.5 | Train Loss 172.1044
Epoch 0100 | Time 0.167 (0.155) | NFE-F 29.8 | NFE-B 24.5 | Train Loss 164.9089
Epoch 0101 | Time 0.173 (0.155) | NFE-F 29.9 | NFE-B 24.5 | Train Loss 158.0226
Epoch 0102 | Time 0.170 (0.155) | NFE-F 30.0 | NFE-B 24.6 | Train Loss 151.4345
Epoch 0103 | Time 0.171 (0.155) | NFE-F 30.1 | NFE-B 24.6 | Train Loss 145.1339
Epoch 0104 | Time 0.168 (0.155) | NFE-F 30.2 | NFE-B 24.6 | Train Loss 139.1105
Epoch 0105 | Time 0.166 (0.156) | NFE-F 30.3 | NFE-B 24.6 | Train Loss 133.3540
Epoch 0106 | Time 0.172 (0.156) | NFE-F 30.4 | NFE-B 24.7 | Train Loss 127.8547
Epoch 0107 | Time 0.175 (0.156) | NFE-F 30.5 | NFE-B 24.7 | Train Loss 122.6029
Epoch 0108 | Time 0.167 (0.156) | NFE-F 30.6 | NFE-B 24.7 | Train Loss 117.5894
Epoch 0109 | Time 0.169 (0.156) | NFE-F 30.7 | NFE-B 24.7 | Train Loss 112.8049
Epoch 0110 | Time 0.180 (0.156) | NFE-F 30.7 | NFE-B 24.7 | Train Loss 108.2409
Epoch 0111 | Time 0.167 (0.156) | NFE-F 30.8 | NFE-B 24.8 | Train Loss 103.8887
Epoch 0112 | Time 0.168 (0.157) | NFE-F 30.9 | NFE-B 24.8 | Train Loss 99.7399
Epoch 0113 | Time 0.183 (0.157) | NFE-F 31.0 | NFE-B 24.8 | Train Loss 95.7865
Epoch 0114 | Time 0.172 (0.157) | NFE-F 31.1 | NFE-B 24.8 | Train Loss 92.0207
Epoch 0115 | Time 0.169 (0.157) | NFE-F 31.2 | NFE-B 24.9 | Train Loss 88.4350
Epoch 0116 | Time 0.174 (0.157) | NFE-F 31.3 | NFE-B 24.9 | Train Loss 85.0219
Epoch 0117 | Time 0.173 (0.157) | NFE-F 31.4 | NFE-B 24.9 | Train Loss 81.7745
Epoch 0118 | Time 0.175 (0.158) | NFE-F 31.5 | NFE-B 24.9 | Train Loss 78.6857
Epoch 0119 | Time 0.171 (0.158) | NFE-F 31.5 | NFE-B 24.9 | Train Loss 75.7490
Epoch 0120 | Time 0.173 (0.158) | NFE-F 31.6 | NFE-B 25.0 | Train Loss 72.9580
Epoch 0121 | Time 0.170 (0.158) | NFE-F 31.7 | NFE-B 25.0 | Train Loss 70.3063
Epoch 0122 | Time 0.171 (0.158) | NFE-F 31.8 | NFE-B 25.0 | Train Loss 67.7880
Epoch 0123 | Time 0.169 (0.158) | NFE-F 31.9 | NFE-B 25.0 | Train Loss 65.3974
Epoch 0124 | Time 0.174 (0.158) | NFE-F 32.0 | NFE-B 25.0 | Train Loss 63.1287
Epoch 0125 | Time 0.168 (0.159) | NFE-F 32.0 | NFE-B 25.1 | Train Loss 60.9767
Epoch 0126 | Time 0.171 (0.159) | NFE-F 32.1 | NFE-B 25.1 | Train Loss 58.9360
Epoch 0127 | Time 0.172 (0.159) | NFE-F 32.2 | NFE-B 25.1 | Train Loss 57.0016
Epoch 0128 | Time 0.172 (0.159) | NFE-F 32.3 | NFE-B 25.1 | Train Loss 55.1689
Epoch 0129 | Time 0.174 (0.159) | NFE-F 32.4 | NFE-B 25.1 | Train Loss 53.4329
Epoch 0130 | Time 0.172 (0.159) | NFE-F 32.4 | NFE-B 25.2 | Train Loss 51.7894
Epoch 0131 | Time 0.168 (0.159) | NFE-F 32.5 | NFE-B 25.2 | Train Loss 50.2340
Epoch 0132 | Time 0.184 (0.160) | NFE-F 32.6 | NFE-B 25.2 | Train Loss 48.7625
Epoch 0133 | Time 0.182 (0.160) | NFE-F 32.7 | NFE-B 25.2 | Train Loss 47.3709
Epoch 0134 | Time 0.170 (0.160) | NFE-F 32.7 | NFE-B 25.2 | Train Loss 46.0555
Epoch 0135 | Time 0.173 (0.160) | NFE-F 32.8 | NFE-B 25.2 | Train Loss 44.8126
Epoch 0136 | Time 0.171 (0.160) | NFE-F 32.9 | NFE-B 25.3 | Train Loss 43.6385
Epoch 0137 | Time 0.173 (0.160) | NFE-F 32.9 | NFE-B 25.3 | Train Loss 42.5300
Epoch 0138 | Time 0.167 (0.160) | NFE-F 33.0 | NFE-B 25.3 | Train Loss 41.4839
Epoch 0139 | Time 0.168 (0.160) | NFE-F 33.1 | NFE-B 25.3 | Train Loss 40.4969
Epoch 0140 | Time 0.171 (0.160) | NFE-F 33.2 | NFE-B 25.3 | Train Loss 39.5661
Epoch 0141 | Time 0.166 (0.161) | NFE-F 33.2 | NFE-B 25.3 | Train Loss 38.6887
Epoch 0142 | Time 0.171 (0.161) | NFE-F 33.3 | NFE-B 25.4 | Train Loss 37.8619
Epoch 0143 | Time 0.171 (0.161) | NFE-F 33.4 | NFE-B 25.4 | Train Loss 37.0832
Epoch 0144 | Time 0.166 (0.161) | NFE-F 33.4 | NFE-B 25.4 | Train Loss 36.3500
Epoch 0145 | Time 0.168 (0.161) | NFE-F 33.5 | NFE-B 25.4 | Train Loss 35.6600
Epoch 0146 | Time 0.169 (0.161) | NFE-F 33.6 | NFE-B 25.4 | Train Loss 35.0108
Epoch 0147 | Time 0.165 (0.161) | NFE-F 33.6 | NFE-B 25.4 | Train Loss 34.4003
Epoch 0148 | Time 0.180 (0.161) | NFE-F 33.7 | NFE-B 25.5 | Train Loss 33.8265
Epoch 0149 | Time 0.174 (0.161) | NFE-F 33.7 | NFE-B 25.5 | Train Loss 33.2874
Epoch 0150 | Time 0.172 (0.161) | NFE-F 33.8 | NFE-B 25.5 | Train Loss 32.7810
Epoch 0151 | Time 0.170 (0.162) | NFE-F 33.9 | NFE-B 25.5 | Train Loss 32.3055
Epoch 0152 | Time 0.172 (0.162) | NFE-F 33.9 | NFE-B 25.5 | Train Loss 31.8594
Epoch 0153 | Time 0.170 (0.162) | NFE-F 34.0 | NFE-B 25.5 | Train Loss 31.4408
Epoch 0154 | Time 0.183 (0.162) | NFE-F 34.1 | NFE-B 25.6 | Train Loss 31.0483
Epoch 0155 | Time 0.168 (0.162) | NFE-F 34.1 | NFE-B 25.6 | Train Loss 30.6805
Epoch 0156 | Time 0.190 (0.162) | NFE-F 34.2 | NFE-B 25.6 | Train Loss 30.3358
Epoch 0157 | Time 0.166 (0.162) | NFE-F 34.2 | NFE-B 25.6 | Train Loss 30.0131
Epoch 0158 | Time 0.166 (0.162) | NFE-F 34.3 | NFE-B 25.6 | Train Loss 29.7109
Epoch 0159 | Time 0.183 (0.163) | NFE-F 34.3 | NFE-B 25.6 | Train Loss 29.4281
Epoch 0160 | Time 0.164 (0.163) | NFE-F 34.4 | NFE-B 25.6 | Train Loss 29.1636
Epoch 0161 | Time 0.166 (0.163) | NFE-F 34.5 | NFE-B 25.7 | Train Loss 28.9163
Epoch 0162 | Time 0.172 (0.163) | NFE-F 34.5 | NFE-B 25.7 | Train Loss 28.6852
Epoch 0163 | Time 0.164 (0.163) | NFE-F 34.6 | NFE-B 25.7 | Train Loss 28.4693
Epoch 0164 | Time 0.169 (0.163) | NFE-F 34.6 | NFE-B 25.7 | Train Loss 28.2676
Epoch 0165 | Time 0.173 (0.163) | NFE-F 34.7 | NFE-B 25.7 | Train Loss 28.0794
Epoch 0166 | Time 0.164 (0.163) | NFE-F 34.7 | NFE-B 25.7 | Train Loss 27.9037
Epoch 0167 | Time 0.169 (0.163) | NFE-F 34.8 | NFE-B 25.7 | Train Loss 27.7399
Epoch 0168 | Time 0.169 (0.163) | NFE-F 34.8 | NFE-B 25.7 | Train Loss 27.5871
Epoch 0169 | Time 0.167 (0.163) | NFE-F 34.9 | NFE-B 25.8 | Train Loss 27.4447
Epoch 0170 | Time 0.170 (0.163) | NFE-F 34.9 | NFE-B 25.8 | Train Loss 27.3121
Epoch 0171 | Time 0.172 (0.163) | NFE-F 35.0 | NFE-B 25.8 | Train Loss 27.1885
Epoch 0172 | Time 0.170 (0.163) | NFE-F 35.0 | NFE-B 25.8 | Train Loss 27.0735
Epoch 0173 | Time 0.169 (0.163) | NFE-F 35.1 | NFE-B 25.8 | Train Loss 26.9665
Epoch 0174 | Time 0.171 (0.163) | NFE-F 35.1 | NFE-B 25.8 | Train Loss 26.8669
Epoch 0175 | Time 0.167 (0.163) | NFE-F 35.2 | NFE-B 25.8 | Train Loss 26.7743
Epoch 0176 | Time 0.169 (0.163) | NFE-F 35.2 | NFE-B 25.8 | Train Loss 26.6882
Epoch 0177 | Time 0.172 (0.164) | NFE-F 35.3 | NFE-B 25.9 | Train Loss 26.6081
Epoch 0178 | Time 0.174 (0.164) | NFE-F 35.3 | NFE-B 25.9 | Train Loss 26.5338
Epoch 0179 | Time 0.169 (0.164) | NFE-F 35.4 | NFE-B 25.9 | Train Loss 26.4648
Epoch 0180 | Time 0.171 (0.164) | NFE-F 35.4 | NFE-B 25.9 | Train Loss 26.4006
Epoch 0181 | Time 0.172 (0.164) | NFE-F 35.5 | NFE-B 25.9 | Train Loss 26.3411
Epoch 0182 | Time 0.178 (0.164) | NFE-F 35.5 | NFE-B 25.9 | Train Loss 26.2859
Epoch 0183 | Time 0.177 (0.164) | NFE-F 35.6 | NFE-B 25.9 | Train Loss 26.2346
Epoch 0184 | Time 0.171 (0.164) | NFE-F 35.6 | NFE-B 25.9 | Train Loss 26.1871
Epoch 0185 | Time 0.166 (0.164) | NFE-F 35.6 | NFE-B 25.9 | Train Loss 26.1430
Epoch 0186 | Time 0.182 (0.164) | NFE-F 35.7 | NFE-B 26.0 | Train Loss 26.1021
Epoch 0187 | Time 0.171 (0.165) | NFE-F 35.7 | NFE-B 26.0 | Train Loss 26.0642
Epoch 0188 | Time 0.173 (0.165) | NFE-F 35.8 | NFE-B 26.0 | Train Loss 26.0290
Epoch 0189 | Time 0.172 (0.165) | NFE-F 35.8 | NFE-B 26.0 | Train Loss 25.9964
Epoch 0190 | Time 0.174 (0.165) | NFE-F 35.9 | NFE-B 26.0 | Train Loss 25.9662
Epoch 0191 | Time 0.165 (0.165) | NFE-F 35.9 | NFE-B 26.0 | Train Loss 25.9382
Epoch 0192 | Time 0.169 (0.165) | NFE-F 35.9 | NFE-B 26.0 | Train Loss 25.9122
Epoch 0193 | Time 0.172 (0.165) | NFE-F 36.0 | NFE-B 26.0 | Train Loss 25.8882
Epoch 0194 | Time 0.185 (0.165) | NFE-F 36.0 | NFE-B 26.0 | Train Loss 25.8658
Epoch 0195 | Time 0.172 (0.165) | NFE-F 36.1 | NFE-B 26.0 | Train Loss 25.8451
Epoch 0196 | Time 0.174 (0.165) | NFE-F 36.1 | NFE-B 26.1 | Train Loss 25.8259
Epoch 0197 | Time 0.170 (0.165) | NFE-F 36.1 | NFE-B 26.1 | Train Loss 25.8080
Epoch 0198 | Time 0.175 (0.165) | NFE-F 36.2 | NFE-B 26.1 | Train Loss 25.7914
Epoch 0199 | Time 0.169 (0.165) | NFE-F 36.2 | NFE-B 26.1 | Train Loss 25.7760
Epoch 0200 | Time 0.178 (0.166) | NFE-F 36.3 | NFE-B 26.1 | Train Loss 25.7617
Epoch 0201 | Time 0.173 (0.166) | NFE-F 36.3 | NFE-B 26.1 | Train Loss 25.7484
Epoch 0202 | Time 0.170 (0.166) | NFE-F 36.3 | NFE-B 26.1 | Train Loss 25.7360
Epoch 0203 | Time 0.179 (0.166) | NFE-F 36.4 | NFE-B 26.1 | Train Loss 25.7244
Epoch 0204 | Time 0.170 (0.166) | NFE-F 36.4 | NFE-B 26.1 | Train Loss 25.7136
Epoch 0205 | Time 0.186 (0.166) | NFE-F 36.4 | NFE-B 26.1 | Train Loss 25.7036
Epoch 0206 | Time 0.170 (0.166) | NFE-F 36.5 | NFE-B 26.1 | Train Loss 25.6941
Epoch 0207 | Time 0.189 (0.166) | NFE-F 36.5 | NFE-B 26.1 | Train Loss 25.6853
Epoch 0208 | Time 0.168 (0.166) | NFE-F 36.5 | NFE-B 26.2 | Train Loss 25.6771
Epoch 0209 | Time 0.169 (0.166) | NFE-F 36.6 | NFE-B 26.2 | Train Loss 25.6693
Epoch 0210 | Time 0.175 (0.166) | NFE-F 36.6 | NFE-B 26.2 | Train Loss 25.6620
Epoch 0211 | Time 0.195 (0.167) | NFE-F 36.6 | NFE-B 26.2 | Train Loss 25.6551
Epoch 0212 | Time 0.191 (0.167) | NFE-F 36.7 | NFE-B 26.3 | Train Loss 25.6486
Epoch 0213 | Time 0.191 (0.167) | NFE-F 36.7 | NFE-B 26.4 | Train Loss 25.6425
Epoch 0214 | Time 0.189 (0.167) | NFE-F 36.7 | NFE-B 26.4 | Train Loss 25.6366
Epoch 0215 | Time 0.184 (0.168) | NFE-F 36.8 | NFE-B 26.5 | Train Loss 25.6311
Epoch 0216 | Time 0.187 (0.168) | NFE-F 36.8 | NFE-B 26.6 | Train Loss 25.6259
Epoch 0217 | Time 0.188 (0.168) | NFE-F 36.8 | NFE-B 26.6 | Train Loss 25.6208
Epoch 0218 | Time 0.184 (0.168) | NFE-F 36.9 | NFE-B 26.7 | Train Loss 25.6160
Epoch 0219 | Time 0.186 (0.168) | NFE-F 36.9 | NFE-B 26.8 | Train Loss 25.6114
Epoch 0220 | Time 0.185 (0.169) | NFE-F 36.9 | NFE-B 26.8 | Train Loss 25.6070
Epoch 0221 | Time 0.183 (0.169) | NFE-F 37.0 | NFE-B 26.9 | Train Loss 25.6028
Epoch 0222 | Time 0.193 (0.169) | NFE-F 37.0 | NFE-B 27.0 | Train Loss 25.5987
Epoch 0223 | Time 0.185 (0.169) | NFE-F 37.0 | NFE-B 27.0 | Train Loss 25.5947
Epoch 0224 | Time 0.186 (0.169) | NFE-F 37.1 | NFE-B 27.1 | Train Loss 25.5909
Epoch 0225 | Time 0.215 (0.170) | NFE-F 37.1 | NFE-B 27.2 | Train Loss 25.5871
Epoch 0226 | Time 0.186 (0.170) | NFE-F 37.1 | NFE-B 27.2 | Train Loss 25.5835
Epoch 0227 | Time 0.186 (0.170) | NFE-F 37.1 | NFE-B 27.3 | Train Loss 25.5799
Epoch 0228 | Time 0.185 (0.170) | NFE-F 37.2 | NFE-B 27.4 | Train Loss 25.5765
Epoch 0229 | Time 0.191 (0.170) | NFE-F 37.2 | NFE-B 27.4 | Train Loss 25.5731
Epoch 0230 | Time 0.190 (0.171) | NFE-F 37.2 | NFE-B 27.5 | Train Loss 25.5697
Epoch 0231 | Time 0.182 (0.171) | NFE-F 37.3 | NFE-B 27.5 | Train Loss 25.5665
Epoch 0232 | Time 0.184 (0.171) | NFE-F 37.3 | NFE-B 27.6 | Train Loss 25.5632
Epoch 0233 | Time 0.188 (0.171) | NFE-F 37.3 | NFE-B 27.6 | Train Loss 25.5601
Epoch 0234 | Time 0.191 (0.171) | NFE-F 37.3 | NFE-B 27.7 | Train Loss 25.5569
Epoch 0235 | Time 0.182 (0.171) | NFE-F 37.4 | NFE-B 27.7 | Train Loss 25.5538
Epoch 0236 | Time 0.184 (0.171) | NFE-F 37.4 | NFE-B 27.8 | Train Loss 25.5507
Epoch 0237 | Time 0.184 (0.172) | NFE-F 37.4 | NFE-B 27.9 | Train Loss 25.5477
Epoch 0238 | Time 0.190 (0.172) | NFE-F 37.4 | NFE-B 27.9 | Train Loss 25.5447
Epoch 0239 | Time 0.190 (0.172) | NFE-F 37.5 | NFE-B 28.0 | Train Loss 25.5417
Epoch 0240 | Time 0.194 (0.172) | NFE-F 37.5 | NFE-B 28.0 | Train Loss 25.5387
Epoch 0241 | Time 0.187 (0.172) | NFE-F 37.5 | NFE-B 28.1 | Train Loss 25.5357
Epoch 0242 | Time 0.189 (0.172) | NFE-F 37.5 | NFE-B 28.1 | Train Loss 25.5327
Epoch 0243 | Time 0.194 (0.173) | NFE-F 37.6 | NFE-B 28.2 | Train Loss 25.5298
Epoch 0244 | Time 0.209 (0.173) | NFE-F 37.6 | NFE-B 28.2 | Train Loss 25.5269
Epoch 0245 | Time 0.189 (0.173) | NFE-F 37.6 | NFE-B 28.2 | Train Loss 25.5239
Epoch 0246 | Time 0.184 (0.173) | NFE-F 37.6 | NFE-B 28.3 | Train Loss 25.5210
Epoch 0247 | Time 0.193 (0.174) | NFE-F 37.7 | NFE-B 28.3 | Train Loss 25.5181
Epoch 0248 | Time 0.193 (0.174) | NFE-F 37.7 | NFE-B 28.4 | Train Loss 25.5152
Epoch 0249 | Time 0.186 (0.174) | NFE-F 37.7 | NFE-B 28.4 | Train Loss 25.5123
Epoch 0250 | Time 0.190 (0.174) | NFE-F 37.7 | NFE-B 28.5 | Train Loss 25.5093
Epoch 0251 | Time 0.197 (0.174) | NFE-F 37.8 | NFE-B 28.5 | Train Loss 25.5064
Epoch 0252 | Time 0.184 (0.174) | NFE-F 37.8 | NFE-B 28.6 | Train Loss 25.5035
Epoch 0253 | Time 0.185 (0.174) | NFE-F 37.8 | NFE-B 28.6 | Train Loss 25.5006
Epoch 0254 | Time 0.198 (0.175) | NFE-F 37.8 | NFE-B 28.7 | Train Loss 25.4977
Epoch 0255 | Time 0.193 (0.175) | NFE-F 37.8 | NFE-B 28.7 | Train Loss 25.4948
Epoch 0256 | Time 0.186 (0.175) | NFE-F 37.9 | NFE-B 28.7 | Train Loss 25.4919
Epoch 0257 | Time 0.190 (0.175) | NFE-F 37.9 | NFE-B 28.8 | Train Loss 25.4889
Epoch 0258 | Time 0.191 (0.175) | NFE-F 37.9 | NFE-B 28.8 | Train Loss 25.4860
Epoch 0259 | Time 0.194 (0.175) | NFE-F 37.9 | NFE-B 28.9 | Train Loss 25.4831
Epoch 0260 | Time 0.189 (0.176) | NFE-F 38.0 | NFE-B 28.9 | Train Loss 25.4802
Epoch 0261 | Time 0.188 (0.176) | NFE-F 38.0 | NFE-B 29.0 | Train Loss 25.4772
Epoch 0262 | Time 0.190 (0.176) | NFE-F 38.0 | NFE-B 29.0 | Train Loss 25.4743
Epoch 0263 | Time 0.183 (0.176) | NFE-F 38.0 | NFE-B 29.0 | Train Loss 25.4713
Epoch 0264 | Time 0.196 (0.176) | NFE-F 38.0 | NFE-B 29.1 | Train Loss 25.4684
Epoch 0265 | Time 0.189 (0.176) | NFE-F 38.1 | NFE-B 29.1 | Train Loss 25.4654
Epoch 0266 | Time 0.189 (0.176) | NFE-F 38.1 | NFE-B 29.2 | Train Loss 25.4625
Epoch 0267 | Time 0.184 (0.176) | NFE-F 38.1 | NFE-B 29.2 | Train Loss 25.4595
Epoch 0268 | Time 0.191 (0.177) | NFE-F 38.1 | NFE-B 29.2 | Train Loss 25.4565
Epoch 0269 | Time 0.197 (0.177) | NFE-F 38.1 | NFE-B 29.3 | Train Loss 25.4536
Epoch 0270 | Time 0.188 (0.177) | NFE-F 38.1 | NFE-B 29.3 | Train Loss 25.4506
Epoch 0271 | Time 0.193 (0.177) | NFE-F 38.2 | NFE-B 29.3 | Train Loss 25.4476
Epoch 0272 | Time 0.198 (0.177) | NFE-F 38.2 | NFE-B 29.4 | Train Loss 25.4446
Epoch 0273 | Time 0.185 (0.177) | NFE-F 38.2 | NFE-B 29.4 | Train Loss 25.4416
Epoch 0274 | Time 0.200 (0.178) | NFE-F 38.2 | NFE-B 29.4 | Train Loss 25.4386
Epoch 0275 | Time 0.188 (0.178) | NFE-F 38.2 | NFE-B 29.5 | Train Loss 25.4356
Epoch 0276 | Time 0.198 (0.178) | NFE-F 38.3 | NFE-B 29.5 | Train Loss 25.4326
Epoch 0277 | Time 0.188 (0.178) | NFE-F 38.3 | NFE-B 29.6 | Train Loss 25.4296
Epoch 0278 | Time 0.184 (0.178) | NFE-F 38.3 | NFE-B 29.6 | Train Loss 25.4266
Epoch 0279 | Time 0.186 (0.178) | NFE-F 38.3 | NFE-B 29.6 | Train Loss 25.4236
Epoch 0280 | Time 0.187 (0.178) | NFE-F 38.3 | NFE-B 29.7 | Train Loss 25.4205
Epoch 0281 | Time 0.185 (0.178) | NFE-F 38.3 | NFE-B 29.7 | Train Loss 25.4175
Epoch 0282 | Time 0.188 (0.178) | NFE-F 38.4 | NFE-B 29.7 | Train Loss 25.4144
Epoch 0283 | Time 0.187 (0.178) | NFE-F 38.4 | NFE-B 29.8 | Train Loss 25.4114
Epoch 0284 | Time 0.192 (0.179) | NFE-F 38.4 | NFE-B 29.8 | Train Loss 25.4083
Epoch 0285 | Time 0.195 (0.179) | NFE-F 38.4 | NFE-B 29.8 | Train Loss 25.4053
Epoch 0286 | Time 0.200 (0.179) | NFE-F 38.4 | NFE-B 29.9 | Train Loss 25.4022
Epoch 0287 | Time 0.184 (0.179) | NFE-F 38.4 | NFE-B 29.9 | Train Loss 25.3992
Epoch 0288 | Time 0.187 (0.179) | NFE-F 38.5 | NFE-B 29.9 | Train Loss 25.3961
Epoch 0289 | Time 0.189 (0.179) | NFE-F 38.5 | NFE-B 29.9 | Train Loss 25.3930
Epoch 0290 | Time 0.185 (0.179) | NFE-F 38.5 | NFE-B 30.0 | Train Loss 25.3899
Epoch 0291 | Time 0.188 (0.179) | NFE-F 38.5 | NFE-B 30.0 | Train Loss 25.3868
Epoch 0292 | Time 0.185 (0.179) | NFE-F 38.5 | NFE-B 30.0 | Train Loss 25.3837
Epoch 0293 | Time 0.212 (0.180) | NFE-F 38.5 | NFE-B 30.1 | Train Loss 25.3806
Epoch 0294 | Time 0.187 (0.180) | NFE-F 38.5 | NFE-B 30.1 | Train Loss 25.3776
Epoch 0295 | Time 0.192 (0.180) | NFE-F 38.6 | NFE-B 30.1 | Train Loss 25.3744
Epoch 0296 | Time 0.190 (0.180) | NFE-F 38.6 | NFE-B 30.2 | Train Loss 25.3713
Epoch 0297 | Time 0.188 (0.180) | NFE-F 38.6 | NFE-B 30.2 | Train Loss 25.3682
Epoch 0298 | Time 0.186 (0.180) | NFE-F 38.6 | NFE-B 30.2 | Train Loss 25.3651
Epoch 0299 | Time 0.186 (0.180) | NFE-F 38.6 | NFE-B 30.2 | Train Loss 25.3620
Epoch 0300 | Time 0.183 (0.180) | NFE-F 38.6 | NFE-B 30.3 | Train Loss 25.3588
Epoch 0301 | Time 0.195 (0.180) | NFE-F 38.6 | NFE-B 30.3 | Train Loss 25.3557
Epoch 0302 | Time 0.188 (0.181) | NFE-F 38.7 | NFE-B 30.3 | Train Loss 25.3525
Epoch 0303 | Time 0.190 (0.181) | NFE-F 38.7 | NFE-B 30.3 | Train Loss 25.3494
Epoch 0304 | Time 0.186 (0.181) | NFE-F 38.7 | NFE-B 30.4 | Train Loss 25.3462
Epoch 0305 | Time 0.193 (0.181) | NFE-F 38.7 | NFE-B 30.4 | Train Loss 25.3431
Epoch 0306 | Time 0.197 (0.181) | NFE-F 38.7 | NFE-B 30.4 | Train Loss 25.3399
Epoch 0307 | Time 0.186 (0.181) | NFE-F 38.7 | NFE-B 30.5 | Train Loss 25.3368
Epoch 0308 | Time 0.196 (0.181) | NFE-F 38.7 | NFE-B 30.5 | Train Loss 25.3336
Epoch 0309 | Time 0.190 (0.181) | NFE-F 38.7 | NFE-B 30.5 | Train Loss 25.3304
Epoch 0310 | Time 0.182 (0.181) | NFE-F 38.8 | NFE-B 30.5 | Train Loss 25.3272
Epoch 0311 | Time 0.184 (0.181) | NFE-F 38.8 | NFE-B 30.6 | Train Loss 25.3240
Epoch 0312 | Time 0.193 (0.181) | NFE-F 38.8 | NFE-B 30.6 | Train Loss 25.3208
Epoch 0313 | Time 0.185 (0.181) | NFE-F 38.8 | NFE-B 30.6 | Train Loss 25.3176
Epoch 0314 | Time 0.182 (0.181) | NFE-F 38.8 | NFE-B 30.6 | Train Loss 25.3144
Epoch 0315 | Time 0.188 (0.181) | NFE-F 38.8 | NFE-B 30.6 | Train Loss 25.3112
Epoch 0316 | Time 0.200 (0.182) | NFE-F 38.8 | NFE-B 30.7 | Train Loss 25.3080
Epoch 0317 | Time 0.192 (0.182) | NFE-F 38.8 | NFE-B 30.7 | Train Loss 25.3048
Epoch 0318 | Time 0.190 (0.182) | NFE-F 38.9 | NFE-B 30.7 | Train Loss 25.3016
Epoch 0319 | Time 0.192 (0.182) | NFE-F 38.9 | NFE-B 30.7 | Train Loss 25.2983
Epoch 0320 | Time 0.192 (0.182) | NFE-F 38.9 | NFE-B 30.8 | Train Loss 25.2951
Epoch 0321 | Time 0.194 (0.182) | NFE-F 38.9 | NFE-B 30.8 | Train Loss 25.2919
Epoch 0322 | Time 0.199 (0.182) | NFE-F 38.9 | NFE-B 30.8 | Train Loss 25.2886
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.049 (0.049) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 3907.4031
Epoch 0001 | Time 0.090 (0.049) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 495.5300
Epoch 0002 | Time 0.086 (0.050) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 604.1257
Epoch 0003 | Time 0.080 (0.050) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 1975.4950
Epoch 0004 | Time 0.082 (0.050) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 1298.2194
Epoch 0005 | Time 0.082 (0.051) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 177.0814
Epoch 0006 | Time 0.082 (0.051) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 182.3425
Epoch 0007 | Time 0.079 (0.051) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 899.9203
Epoch 0008 | Time 0.082 (0.052) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 1042.2947
Epoch 0009 | Time 0.079 (0.052) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 493.1918
Epoch 0010 | Time 0.082 (0.052) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 38.9476
Epoch 0011 | Time 0.083 (0.052) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 164.4058
Epoch 0012 | Time 0.078 (0.053) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 544.7285
Epoch 0013 | Time 0.083 (0.053) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 603.9983
Epoch 0014 | Time 0.081 (0.053) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 297.3031
Epoch 0015 | Time 0.083 (0.054) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 32.4797
Epoch 0016 | Time 0.080 (0.054) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 90.6006
Epoch 0017 | Time 0.078 (0.054) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 313.8566
Epoch 0018 | Time 0.082 (0.054) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 370.0687
Epoch 0019 | Time 0.081 (0.055) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 198.3289
Epoch 0020 | Time 0.083 (0.055) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 29.8525
Epoch 0021 | Time 0.079 (0.055) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 52.9485
Epoch 0022 | Time 0.081 (0.055) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 188.5224
Epoch 0023 | Time 0.081 (0.056) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 229.8854
Epoch 0024 | Time 0.081 (0.056) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 130.9089
Epoch 0025 | Time 0.078 (0.056) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 32.7500
Epoch 0026 | Time 0.084 (0.056) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 31.4711
Epoch 0027 | Time 0.080 (0.057) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 121.8105
Epoch 0028 | Time 0.082 (0.057) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 158.7517
Epoch 0029 | Time 0.078 (0.057) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 92.9543
Epoch 0030 | Time 0.080 (0.057) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 20.3174
Epoch 0031 | Time 0.082 (0.058) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 30.8784
Epoch 0032 | Time 0.083 (0.058) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 88.6331
Epoch 0033 | Time 0.084 (0.058) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 99.5411
Epoch 0034 | Time 0.080 (0.058) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 50.2808
Epoch 0035 | Time 0.082 (0.059) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 13.7200
Epoch 0036 | Time 0.084 (0.059) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 33.6035
Epoch 0037 | Time 0.083 (0.059) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 67.2576
Epoch 0038 | Time 0.082 (0.059) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 59.7293
Epoch 0039 | Time 0.081 (0.060) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 24.8541
Epoch 0040 | Time 0.081 (0.060) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 14.2292
Epoch 0041 | Time 0.080 (0.060) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 35.8495
Epoch 0042 | Time 0.081 (0.060) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 48.5853
Epoch 0043 | Time 0.095 (0.060) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 32.8219
Epoch 0044 | Time 0.085 (0.061) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 13.8919
Epoch 0045 | Time 0.081 (0.061) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 19.0081
Epoch 0046 | Time 0.081 (0.061) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 33.9727
Epoch 0047 | Time 0.080 (0.061) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 31.9405
Epoch 0048 | Time 0.083 (0.062) | NFE-F 19.5 | NFE-B 0.0 | Train Loss 17.1696
Epoch 0049 | Time 0.080 (0.062) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 13.3440
Epoch 0050 | Time 0.081 (0.062) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 23.0616
Epoch 0051 | Time 0.080 (0.062) | NFE-F 19.7 | NFE-B 0.0 | Train Loss 27.0475
Epoch 0052 | Time 0.079 (0.062) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 18.5954
Epoch 0053 | Time 0.079 (0.062) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 12.3674
Epoch 0054 | Time 0.083 (0.063) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 17.0442
Epoch 0055 | Time 0.081 (0.063) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 22.0347
Epoch 0056 | Time 0.080 (0.063) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 18.1088
Epoch 0057 | Time 0.080 (0.063) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 12.5425
Epoch 0058 | Time 0.082 (0.063) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 14.2072
Epoch 0059 | Time 0.083 (0.064) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 18.3169
Epoch 0060 | Time 0.080 (0.064) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 16.8019
Epoch 0061 | Time 0.082 (0.064) | NFE-F 20.5 | NFE-B 0.0 | Train Loss 12.6660
Epoch 0062 | Time 0.081 (0.064) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 12.9742
Epoch 0063 | Time 0.081 (0.064) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 15.9148
Epoch 0064 | Time 0.078 (0.064) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 15.4290
Epoch 0065 | Time 0.084 (0.065) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 12.5671
Epoch 0066 | Time 0.095 (0.065) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 12.4424
Epoch 0067 | Time 0.082 (0.065) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 14.4453
Epoch 0068 | Time 0.080 (0.065) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 14.2883
Epoch 0069 | Time 0.092 (0.065) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 12.3480
Epoch 0070 | Time 0.085 (0.066) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 12.1971
Epoch 0071 | Time 0.084 (0.066) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 13.5471
Epoch 0072 | Time 0.083 (0.066) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 13.4258
Epoch 0073 | Time 0.082 (0.066) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 12.1133
Epoch 0074 | Time 0.092 (0.066) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 12.0707
Epoch 0075 | Time 0.085 (0.067) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 12.9756
Epoch 0076 | Time 0.083 (0.067) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 12.7977
Epoch 0077 | Time 0.077 (0.067) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 11.9191
Epoch 0078 | Time 0.088 (0.067) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 11.9972
Epoch 0079 | Time 0.083 (0.067) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 12.5854
Epoch 0080 | Time 0.081 (0.067) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 12.3473
Epoch 0081 | Time 0.086 (0.068) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 11.7836
Epoch 0082 | Time 0.079 (0.068) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 11.9466
Epoch 0083 | Time 0.079 (0.068) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 12.2963
Epoch 0084 | Time 0.079 (0.068) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 12.0296
Epoch 0085 | Time 0.078 (0.068) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 11.7040
Epoch 0086 | Time 0.090 (0.068) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 11.9010
Epoch 0087 | Time 0.081 (0.068) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 12.0673
Epoch 0088 | Time 0.083 (0.069) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 11.8138
Epoch 0089 | Time 0.085 (0.069) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 11.6663
Epoch 0090 | Time 0.080 (0.069) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 11.8484
Epoch 0091 | Time 0.081 (0.069) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 11.8812
Epoch 0092 | Time 0.076 (0.069) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 11.6778
Epoch 0093 | Time 0.079 (0.069) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 11.6521
Epoch 0094 | Time 0.078 (0.069) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 11.7832
Epoch 0095 | Time 0.080 (0.069) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 11.7338
Epoch 0096 | Time 0.078 (0.069) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 11.6025
Epoch 0097 | Time 0.083 (0.070) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 11.6428
Epoch 0098 | Time 0.082 (0.070) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 11.7072
Epoch 0099 | Time 0.086 (0.070) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 11.6261
Epoch 0100 | Time 0.086 (0.070) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 11.5679
Epoch 0101 | Time 0.084 (0.070) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 11.6242
Epoch 0102 | Time 0.085 (0.070) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 11.6291
Epoch 0103 | Time 0.085 (0.070) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 11.5571
Epoch 0104 | Time 0.082 (0.071) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 11.5529
Epoch 0105 | Time 0.078 (0.071) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 11.5910
Epoch 0106 | Time 0.080 (0.071) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 11.5605
Epoch 0107 | Time 0.079 (0.071) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 11.5197
Epoch 0108 | Time 0.083 (0.071) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 11.5396
Epoch 0109 | Time 0.078 (0.071) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 11.5472
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.001, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.058 (0.058) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 3021.5576
Epoch 0001 | Time 0.084 (0.058) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 2919.4507
Epoch 0002 | Time 0.079 (0.059) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 2802.1230
Epoch 0003 | Time 0.082 (0.059) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 2676.7778
Epoch 0004 | Time 0.084 (0.059) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 2566.2402
Epoch 0005 | Time 0.081 (0.059) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 2471.7783
Epoch 0006 | Time 0.080 (0.059) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 2384.4229
Epoch 0007 | Time 0.081 (0.060) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 2299.3967
Epoch 0008 | Time 0.083 (0.060) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 2215.6626
Epoch 0009 | Time 0.082 (0.060) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 2134.5479
Epoch 0010 | Time 0.086 (0.060) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 2057.5986
Epoch 0011 | Time 0.087 (0.061) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 1984.6777
Epoch 0012 | Time 0.079 (0.061) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 1914.7334
Epoch 0013 | Time 0.083 (0.061) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 1846.9818
Epoch 0014 | Time 0.078 (0.061) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 1781.0341
Epoch 0015 | Time 0.083 (0.061) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 1716.7335
Epoch 0016 | Time 0.088 (0.062) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 1654.0062
Epoch 0017 | Time 0.083 (0.062) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 1592.8229
Epoch 0018 | Time 0.083 (0.062) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 1533.1687
Epoch 0019 | Time 0.078 (0.062) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 1475.0297
Epoch 0020 | Time 0.077 (0.062) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 1418.4001
Epoch 0021 | Time 0.084 (0.063) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 1363.2682
Epoch 0022 | Time 0.088 (0.063) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 1309.6262
Epoch 0023 | Time 0.083 (0.063) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 1257.4640
Epoch 0024 | Time 0.082 (0.063) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 1206.7744
Epoch 0025 | Time 0.085 (0.063) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 1157.5447
Epoch 0026 | Time 0.083 (0.064) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 1109.7592
Epoch 0027 | Time 0.082 (0.064) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 1063.4039
Epoch 0028 | Time 0.078 (0.064) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 1018.4628
Epoch 0029 | Time 0.085 (0.064) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 974.9205
Epoch 0030 | Time 0.081 (0.064) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 932.7592
Epoch 0031 | Time 0.082 (0.065) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 891.9597
Epoch 0032 | Time 0.083 (0.065) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 852.5034
Epoch 0033 | Time 0.082 (0.065) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 814.3697
Epoch 0034 | Time 0.080 (0.065) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 777.5361
Epoch 0035 | Time 0.084 (0.065) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 741.9815
Epoch 0036 | Time 0.082 (0.065) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 707.6832
Epoch 0037 | Time 0.081 (0.066) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 674.6177
Epoch 0038 | Time 0.081 (0.066) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 642.7603
Epoch 0039 | Time 0.080 (0.066) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 612.0876
Epoch 0040 | Time 0.080 (0.066) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 582.5740
Epoch 0041 | Time 0.080 (0.066) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 554.1938
Epoch 0042 | Time 0.081 (0.066) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 526.9218
Epoch 0043 | Time 0.083 (0.066) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 500.7305
Epoch 0044 | Time 0.081 (0.067) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 475.5947
Epoch 0045 | Time 0.082 (0.067) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 451.4875
Epoch 0046 | Time 0.082 (0.067) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 428.3823
Epoch 0047 | Time 0.085 (0.067) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 406.2518
Epoch 0048 | Time 0.080 (0.067) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 385.0700
Epoch 0049 | Time 0.082 (0.067) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 364.8088
Epoch 0050 | Time 0.087 (0.068) | NFE-F 19.5 | NFE-B 0.0 | Train Loss 345.4421
Epoch 0051 | Time 0.080 (0.068) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 326.9430
Epoch 0052 | Time 0.078 (0.068) | NFE-F 19.7 | NFE-B 0.0 | Train Loss 309.2847
Epoch 0053 | Time 0.082 (0.068) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 292.4409
Epoch 0054 | Time 0.079 (0.068) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 276.3852
Epoch 0055 | Time 0.084 (0.068) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 261.0914
Epoch 0056 | Time 0.080 (0.068) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 246.5341
Epoch 0057 | Time 0.079 (0.068) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 232.6872
Epoch 0058 | Time 0.083 (0.069) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 219.5259
Epoch 0059 | Time 0.082 (0.069) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 207.0253
Epoch 0060 | Time 0.084 (0.069) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 195.1609
Epoch 0061 | Time 0.079 (0.069) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 183.9088
Epoch 0062 | Time 0.085 (0.069) | NFE-F 20.5 | NFE-B 0.0 | Train Loss 173.2452
Epoch 0063 | Time 0.085 (0.069) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 163.1469
Epoch 0064 | Time 0.081 (0.069) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 153.5914
Epoch 0065 | Time 0.086 (0.070) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 144.5563
Epoch 0066 | Time 0.083 (0.070) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 136.0200
Epoch 0067 | Time 0.080 (0.070) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 127.9610
Epoch 0068 | Time 0.084 (0.070) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 120.3588
Epoch 0069 | Time 0.082 (0.070) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 113.1931
Epoch 0070 | Time 0.083 (0.070) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 106.4443
Epoch 0071 | Time 0.081 (0.070) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 100.0932
Epoch 0072 | Time 0.085 (0.070) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 94.1213
Epoch 0073 | Time 0.085 (0.071) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 88.5105
Epoch 0074 | Time 0.081 (0.071) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 83.2437
Epoch 0075 | Time 0.081 (0.071) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 78.3035
Epoch 0076 | Time 0.080 (0.071) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 73.6739
Epoch 0077 | Time 0.084 (0.071) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 69.3389
Epoch 0078 | Time 0.082 (0.071) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 65.2835
Epoch 0079 | Time 0.082 (0.071) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 61.4927
Epoch 0080 | Time 0.081 (0.071) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 57.9525
Epoch 0081 | Time 0.087 (0.071) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 54.6494
Epoch 0082 | Time 0.081 (0.072) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 51.5701
Epoch 0083 | Time 0.081 (0.072) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 48.7022
Epoch 0084 | Time 0.081 (0.072) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 46.0336
Epoch 0085 | Time 0.083 (0.072) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 43.5529
Epoch 0086 | Time 0.082 (0.072) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 41.2488
Epoch 0087 | Time 0.082 (0.072) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 39.1111
Epoch 0088 | Time 0.085 (0.072) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 37.1295
Epoch 0089 | Time 0.081 (0.072) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 35.2944
Epoch 0090 | Time 0.081 (0.072) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 33.5968
Epoch 0091 | Time 0.082 (0.072) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 32.0278
Epoch 0092 | Time 0.082 (0.073) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 30.5793
Epoch 0093 | Time 0.081 (0.073) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 29.2433
Epoch 0094 | Time 0.080 (0.073) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 28.0125
Epoch 0095 | Time 0.083 (0.073) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 26.8796
Epoch 0096 | Time 0.083 (0.073) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 25.8382
Epoch 0097 | Time 0.083 (0.073) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 24.8818
Epoch 0098 | Time 0.087 (0.073) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 24.0044
Epoch 0099 | Time 0.083 (0.073) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 23.2006
Epoch 0100 | Time 0.084 (0.073) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 22.4649
Epoch 0101 | Time 0.079 (0.073) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 21.7923
Epoch 0102 | Time 0.081 (0.074) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 21.1782
Epoch 0103 | Time 0.081 (0.074) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 20.6181
Epoch 0104 | Time 0.081 (0.074) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 20.1079
Epoch 0105 | Time 0.085 (0.074) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 19.6438
Epoch 0106 | Time 0.084 (0.074) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 19.2222
Epoch 0107 | Time 0.082 (0.074) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 18.8395
Epoch 0108 | Time 0.085 (0.074) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 18.4928
Epoch 0109 | Time 0.087 (0.074) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 18.1790
Epoch 0110 | Time 0.083 (0.074) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 17.8953
Epoch 0111 | Time 0.078 (0.074) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 17.6394
Epoch 0112 | Time 0.080 (0.074) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 17.4087
Epoch 0113 | Time 0.082 (0.074) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 17.2010
Epoch 0114 | Time 0.083 (0.075) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 17.0144
Epoch 0115 | Time 0.080 (0.075) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 16.8470
Epoch 0116 | Time 0.080 (0.075) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 16.6969
Epoch 0117 | Time 0.077 (0.075) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 16.5627
Epoch 0118 | Time 0.080 (0.075) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 16.4428
Epoch 0119 | Time 0.082 (0.075) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 16.3359
Epoch 0120 | Time 0.085 (0.075) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 16.2407
Epoch 0121 | Time 0.080 (0.075) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 16.1560
Epoch 0122 | Time 0.081 (0.075) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 16.0808
Epoch 0123 | Time 0.080 (0.075) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 16.0142
Epoch 0124 | Time 0.077 (0.075) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 15.9553
Epoch 0125 | Time 0.081 (0.075) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 15.9032
Epoch 0126 | Time 0.080 (0.075) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 15.8573
Epoch 0127 | Time 0.080 (0.075) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 15.8168
Epoch 0128 | Time 0.083 (0.075) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 15.7812
Epoch 0129 | Time 0.085 (0.075) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 15.7500
Epoch 0130 | Time 0.080 (0.075) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 15.7226
Epoch 0131 | Time 0.086 (0.076) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 15.6985
Epoch 0132 | Time 0.077 (0.076) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 15.6775
Epoch 0133 | Time 0.080 (0.076) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 15.6590
Epoch 0134 | Time 0.083 (0.076) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 15.6429
Epoch 0135 | Time 0.081 (0.076) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 15.6288
Epoch 0136 | Time 0.082 (0.076) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 15.6165
Epoch 0137 | Time 0.081 (0.076) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 15.6056
Epoch 0138 | Time 0.083 (0.076) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 15.5961
Epoch 0139 | Time 0.082 (0.076) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 15.5877
Epoch 0140 | Time 0.080 (0.076) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 15.5803
Epoch 0141 | Time 0.083 (0.076) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 15.5737
Epoch 0142 | Time 0.081 (0.076) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 15.5678
Epoch 0143 | Time 0.079 (0.076) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 15.5626
Epoch 0144 | Time 0.080 (0.076) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 15.5578
Epoch 0145 | Time 0.081 (0.076) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 15.5534
Epoch 0146 | Time 0.081 (0.076) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 15.5494
Epoch 0147 | Time 0.079 (0.076) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 15.5457
Epoch 0148 | Time 0.085 (0.076) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 15.5422
Epoch 0149 | Time 0.081 (0.076) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 15.5388
Epoch 0150 | Time 0.082 (0.077) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 15.5357
Epoch 0151 | Time 0.084 (0.077) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 15.5326
Epoch 0152 | Time 0.082 (0.077) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 15.5296
Epoch 0153 | Time 0.080 (0.077) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 15.5267
Epoch 0154 | Time 0.077 (0.077) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 15.5238
Epoch 0155 | Time 0.080 (0.077) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 15.5209
Epoch 0156 | Time 0.082 (0.077) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 15.5181
Epoch 0157 | Time 0.082 (0.077) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 15.5152
Epoch 0158 | Time 0.080 (0.077) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 15.5123
Epoch 0159 | Time 0.080 (0.077) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 15.5094
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.052 (0.052) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 2655.2300
Epoch 0001 | Time 0.081 (0.052) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 839.8631
Epoch 0002 | Time 0.081 (0.052) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 129.0510
Epoch 0003 | Time 0.080 (0.053) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 1171.8223
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=8, lr=0.1, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): Tanh()
        (6): Linear(in_features=256, out_features=256, bias=True)
        (7): Tanh()
        (8): Linear(in_features=256, out_features=256, bias=True)
        (9): Tanh()
        (10): Linear(in_features=256, out_features=256, bias=True)
        (11): Tanh()
        (12): Linear(in_features=256, out_features=256, bias=True)
        (13): Tanh()
        (14): Linear(in_features=256, out_features=256, bias=True)
        (15): Tanh()
        (16): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 507844
Epoch 0000 | Time 0.086 (0.086) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 731.0154
Epoch 0001 | Time 0.113 (0.086) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 2016.0734
Epoch 0002 | Time 0.114 (0.086) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 5237.2349
Epoch 0003 | Time 0.109 (0.087) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 4531.1206
Epoch 0004 | Time 0.110 (0.087) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 1989.8824
Epoch 0005 | Time 0.108 (0.087) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 301.6658
Epoch 0006 | Time 0.108 (0.087) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 493.9580
Epoch 0007 | Time 0.113 (0.088) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 1748.2029
Epoch 0008 | Time 0.118 (0.088) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 2529.7563
Epoch 0009 | Time 0.113 (0.088) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 2174.6316
Epoch 0010 | Time 0.113 (0.088) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 1150.1814
Epoch 0011 | Time 0.123 (0.089) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 309.2536
Epoch 0012 | Time 0.111 (0.089) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 182.4230
Epoch 0013 | Time 0.116 (0.089) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 663.3964
Epoch 0014 | Time 0.119 (0.090) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 1192.1359
Epoch 0015 | Time 0.114 (0.090) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 1284.9496
Epoch 0016 | Time 0.118 (0.090) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 906.1856
Epoch 0017 | Time 0.114 (0.090) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 392.6099
Epoch 0018 | Time 0.112 (0.091) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 122.2978
Epoch 0019 | Time 0.111 (0.091) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 225.7960
Epoch 0020 | Time 0.116 (0.091) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 522.7018
Epoch 0021 | Time 0.119 (0.091) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 710.4362
Epoch 0022 | Time 0.116 (0.092) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 629.4308
Epoch 0023 | Time 0.113 (0.092) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 366.4654
Epoch 0024 | Time 0.112 (0.092) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 140.8226
Epoch 0025 | Time 0.113 (0.092) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 107.4657
Epoch 0026 | Time 0.110 (0.092) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 241.3730
Epoch 0027 | Time 0.108 (0.092) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 383.8287
Epoch 0028 | Time 0.110 (0.093) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 396.1730
Epoch 0029 | Time 0.110 (0.093) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 274.5717
Epoch 0030 | Time 0.113 (0.093) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 131.1252
Epoch 0031 | Time 0.115 (0.093) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 79.6387
Epoch 0032 | Time 0.111 (0.093) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 137.2011
Epoch 0033 | Time 0.115 (0.094) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 223.2363
Epoch 0034 | Time 0.112 (0.094) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 245.6334
Epoch 0035 | Time 0.116 (0.094) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 185.3743
Epoch 0036 | Time 0.113 (0.094) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 102.1742
Epoch 0037 | Time 0.110 (0.094) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 67.7542
Epoch 0038 | Time 0.111 (0.095) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 98.0614
Epoch 0039 | Time 0.111 (0.095) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 146.7148
Epoch 0040 | Time 0.110 (0.095) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 158.0894
Epoch 0041 | Time 0.109 (0.095) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 121.6764
Epoch 0042 | Time 0.121 (0.095) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 75.0557
Epoch 0043 | Time 0.113 (0.095) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 60.0309
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): Tanh()
        (6): Linear(in_features=256, out_features=256, bias=True)
        (7): Tanh()
        (8): Linear(in_features=256, out_features=256, bias=True)
        (9): Tanh()
        (10): Linear(in_features=256, out_features=256, bias=True)
        (11): Tanh()
        (12): Linear(in_features=256, out_features=256, bias=True)
        (13): Tanh()
        (14): Linear(in_features=256, out_features=256, bias=True)
        (15): Tanh()
        (16): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 507844
Epoch 0000 | Time 0.078 (0.078) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 26152.4043
Epoch 0001 | Time 0.110 (0.078) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 23588.1562
Epoch 0002 | Time 0.110 (0.078) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 21162.1816
Epoch 0003 | Time 0.108 (0.078) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 18876.9902
Epoch 0004 | Time 0.111 (0.079) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 16734.2168
Epoch 0005 | Time 0.113 (0.079) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 14734.7988
Epoch 0006 | Time 0.112 (0.079) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 12879.0420
Epoch 0007 | Time 0.110 (0.080) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 11166.5918
Epoch 0008 | Time 0.112 (0.080) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 9596.4043
Epoch 0009 | Time 0.112 (0.080) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 8166.7227
Epoch 0010 | Time 0.117 (0.081) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 6875.0317
Epoch 0011 | Time 0.129 (0.081) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 5718.0449
Epoch 0012 | Time 0.118 (0.082) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 4691.6826
Epoch 0013 | Time 0.109 (0.082) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 3791.0713
Epoch 0014 | Time 0.111 (0.082) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 3010.5569
Epoch 0015 | Time 0.115 (0.083) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 2343.7344
Epoch 0016 | Time 0.114 (0.083) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 1783.4954
Epoch 0017 | Time 0.107 (0.083) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 1322.0996
Epoch 0018 | Time 0.111 (0.083) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 951.2646
Epoch 0019 | Time 0.109 (0.084) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 662.2769
Epoch 0020 | Time 0.114 (0.084) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 446.1218
Epoch 0021 | Time 0.112 (0.084) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 293.6271
Epoch 0022 | Time 0.110 (0.084) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 195.6184
Epoch 0023 | Time 0.111 (0.085) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 143.0789
Epoch 0024 | Time 0.114 (0.085) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 127.3078
Epoch 0025 | Time 0.115 (0.085) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 140.0706
Epoch 0026 | Time 0.116 (0.086) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 173.7353
Epoch 0027 | Time 0.114 (0.086) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 221.3870
Epoch 0028 | Time 0.110 (0.086) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 276.9182
Epoch 0029 | Time 0.109 (0.086) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 335.0908
Epoch 0030 | Time 0.110 (0.087) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 391.5665
Epoch 0031 | Time 0.109 (0.087) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 442.9104
Epoch 0032 | Time 0.112 (0.087) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 486.5629
Epoch 0033 | Time 0.111 (0.087) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 520.7906
Epoch 0034 | Time 0.113 (0.088) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 544.6128
Epoch 0035 | Time 0.110 (0.088) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 557.7159
Epoch 0036 | Time 0.110 (0.088) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 560.3521
Epoch 0037 | Time 0.115 (0.088) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 553.2372
Epoch 0038 | Time 0.113 (0.089) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 537.4434
Epoch 0039 | Time 0.114 (0.089) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 514.2948
Epoch 0040 | Time 0.109 (0.089) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 485.2726
Epoch 0041 | Time 0.116 (0.089) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 451.9241
Epoch 0042 | Time 0.112 (0.089) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 415.7862
Epoch 0043 | Time 0.116 (0.090) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 378.3199
Epoch 0044 | Time 0.117 (0.090) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 340.8577
Epoch 0045 | Time 0.113 (0.090) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 304.5641
Epoch 0046 | Time 0.111 (0.090) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 270.4094
Epoch 0047 | Time 0.112 (0.091) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 239.1541
Epoch 0048 | Time 0.115 (0.091) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 211.3461
Epoch 0049 | Time 0.114 (0.091) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 187.3263
Epoch 0050 | Time 0.108 (0.091) | NFE-F 19.5 | NFE-B 0.0 | Train Loss 167.2424
Epoch 0051 | Time 0.117 (0.092) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 151.0690
Epoch 0052 | Time 0.116 (0.092) | NFE-F 19.7 | NFE-B 0.0 | Train Loss 138.6332
Epoch 0053 | Time 0.113 (0.092) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 129.6422
Epoch 0054 | Time 0.113 (0.092) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 123.7126
Epoch 0055 | Time 0.112 (0.092) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 120.4004
Epoch 0056 | Time 0.107 (0.093) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 119.2282
Epoch 0057 | Time 0.114 (0.093) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 119.7119
Epoch 0058 | Time 0.118 (0.093) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 121.3819
Epoch 0059 | Time 0.111 (0.093) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 123.8026
Epoch 0060 | Time 0.109 (0.093) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 126.5862
Epoch 0061 | Time 0.112 (0.094) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 129.4027
Epoch 0062 | Time 0.112 (0.094) | NFE-F 20.5 | NFE-B 0.0 | Train Loss 131.9863
Epoch 0063 | Time 0.112 (0.094) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 134.1366
Epoch 0064 | Time 0.111 (0.094) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 135.7179
Epoch 0065 | Time 0.118 (0.094) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 136.6545
Epoch 0066 | Time 0.111 (0.095) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 136.9240
Epoch 0067 | Time 0.109 (0.095) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 136.5495
Epoch 0068 | Time 0.129 (0.095) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 135.5902
Epoch 0069 | Time 0.124 (0.095) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 134.1317
Epoch 0070 | Time 0.110 (0.095) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 132.2771
Epoch 0071 | Time 0.118 (0.096) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 130.1377
Epoch 0072 | Time 0.116 (0.096) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 127.8259
Epoch 0073 | Time 0.110 (0.096) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 125.4479
Epoch 0074 | Time 0.111 (0.096) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 123.0990
Epoch 0075 | Time 0.111 (0.096) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 120.8596
Epoch 0076 | Time 0.109 (0.096) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 118.7933
Epoch 0077 | Time 0.116 (0.097) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 116.9454
Epoch 0078 | Time 0.111 (0.097) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 115.3436
Epoch 0079 | Time 0.113 (0.097) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 113.9986
Epoch 0080 | Time 0.117 (0.097) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 112.9065
Epoch 0081 | Time 0.114 (0.097) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 112.0512
Epoch 0082 | Time 0.109 (0.097) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 111.4070
Epoch 0083 | Time 0.110 (0.098) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 110.9418
Epoch 0084 | Time 0.113 (0.098) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 110.6197
Epoch 0085 | Time 0.110 (0.098) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 110.4035
Epoch 0086 | Time 0.115 (0.098) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 110.2573
Epoch 0087 | Time 0.110 (0.098) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 110.1482
Epoch 0088 | Time 0.112 (0.098) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 110.0476
Epoch 0089 | Time 0.112 (0.098) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 109.9322
Epoch 0090 | Time 0.115 (0.099) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 109.7841
Epoch 0091 | Time 0.112 (0.099) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 109.5916
Epoch 0092 | Time 0.110 (0.099) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 109.3483
Epoch 0093 | Time 0.114 (0.099) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 109.0524
Epoch 0094 | Time 0.111 (0.099) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 108.7065
Epoch 0095 | Time 0.117 (0.099) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 108.3162
Epoch 0096 | Time 0.113 (0.099) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 107.8894
Epoch 0097 | Time 0.115 (0.100) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 107.4355
Epoch 0098 | Time 0.113 (0.100) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 106.9641
Epoch 0099 | Time 0.111 (0.100) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 106.4846
Epoch 0100 | Time 0.109 (0.100) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 106.0056
Epoch 0101 | Time 0.113 (0.100) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 105.5345
Epoch 0102 | Time 0.111 (0.100) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 105.0769
Epoch 0103 | Time 0.114 (0.100) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 104.6369
Epoch 0104 | Time 0.115 (0.100) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 104.2168
Epoch 0105 | Time 0.119 (0.101) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 103.8174
Epoch 0106 | Time 0.109 (0.101) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 103.4381
Epoch 0107 | Time 0.112 (0.101) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 103.0771
Epoch 0108 | Time 0.120 (0.101) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 102.7319
Epoch 0109 | Time 0.112 (0.101) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 102.3995
Epoch 0110 | Time 0.110 (0.101) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 102.0765
Epoch 0111 | Time 0.111 (0.101) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 101.7598
Epoch 0112 | Time 0.111 (0.101) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 101.4462
Epoch 0113 | Time 0.112 (0.102) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 101.1331
Epoch 0114 | Time 0.110 (0.102) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 100.8185
Epoch 0115 | Time 0.109 (0.102) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 100.5007
Epoch 0116 | Time 0.110 (0.102) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 100.1788
Epoch 0117 | Time 0.112 (0.102) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 99.8523
Epoch 0118 | Time 0.108 (0.102) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 99.5211
Epoch 0119 | Time 0.113 (0.102) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 99.1857
Epoch 0120 | Time 0.112 (0.102) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 98.8467
Epoch 0121 | Time 0.115 (0.102) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 98.5050
Epoch 0122 | Time 0.109 (0.102) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 98.1615
Epoch 0123 | Time 0.118 (0.102) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 97.8173
Epoch 0124 | Time 0.113 (0.103) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 97.4731
Epoch 0125 | Time 0.114 (0.103) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 97.1298
Epoch 0126 | Time 0.110 (0.103) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 96.7878
Epoch 0127 | Time 0.113 (0.103) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 96.4478
Epoch 0128 | Time 0.111 (0.103) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 96.1098
Epoch 0129 | Time 0.112 (0.103) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 95.7741
Epoch 0130 | Time 0.116 (0.103) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 95.4405
Epoch 0131 | Time 0.107 (0.103) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 95.1089
Epoch 0132 | Time 0.111 (0.103) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 94.7790
Epoch 0133 | Time 0.114 (0.103) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 94.4505
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=8, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): Tanh()
        (2): Linear(in_features=8, out_features=8, bias=True)
        (3): Tanh()
        (4): Linear(in_features=8, out_features=8, bias=True)
        (5): Tanh()
        (6): Linear(in_features=8, out_features=8, bias=True)
        (7): Tanh()
        (8): Linear(in_features=8, out_features=8, bias=True)
        (9): Tanh()
        (10): Linear(in_features=8, out_features=8, bias=True)
        (11): Tanh()
        (12): Linear(in_features=8, out_features=8, bias=True)
        (13): Tanh()
        (14): Linear(in_features=8, out_features=8, bias=True)
        (15): Tanh()
        (16): Linear(in_features=8, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 2172
Epoch 0000 | Time 0.051 (0.051) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 1184.3640
Epoch 0001 | Time 0.079 (0.051) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 727.2823
Epoch 0002 | Time 0.079 (0.052) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 403.2333
Epoch 0003 | Time 0.082 (0.052) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 205.7345
Epoch 0004 | Time 0.080 (0.052) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 120.3673
Epoch 0005 | Time 0.080 (0.052) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 123.3114
Epoch 0006 | Time 0.080 (0.053) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 182.5055
Epoch 0007 | Time 0.079 (0.053) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 262.8900
Epoch 0008 | Time 0.082 (0.053) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 334.2912
Epoch 0009 | Time 0.079 (0.053) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 377.5869
Epoch 0010 | Time 0.081 (0.054) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 386.1079
Epoch 0011 | Time 0.082 (0.054) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 363.0370
Epoch 0012 | Time 0.083 (0.054) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 317.3429
Epoch 0013 | Time 0.079 (0.055) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 260.3018
Epoch 0014 | Time 0.081 (0.055) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 203.4306
Epoch 0015 | Time 0.080 (0.055) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 155.3506
Epoch 0016 | Time 0.083 (0.055) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 121.6054
Epoch 0017 | Time 0.081 (0.056) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 104.5790
Epoch 0018 | Time 0.080 (0.056) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 103.1159
Epoch 0019 | Time 0.079 (0.056) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 113.4392
Epoch 0020 | Time 0.081 (0.056) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 130.0931
Epoch 0021 | Time 0.082 (0.057) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 147.2605
Epoch 0022 | Time 0.085 (0.057) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 160.0721
Epoch 0023 | Time 0.083 (0.057) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 165.4980
Epoch 0024 | Time 0.083 (0.057) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 162.6834
Epoch 0025 | Time 0.079 (0.058) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 152.7443
Epoch 0026 | Time 0.083 (0.058) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 138.1801
Epoch 0027 | Time 0.084 (0.058) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 122.1068
Epoch 0028 | Time 0.084 (0.058) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 107.5015
Epoch 0029 | Time 0.080 (0.059) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 96.5960
Epoch 0030 | Time 0.081 (0.059) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 90.5101
Epoch 0031 | Time 0.080 (0.059) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 89.1650
Epoch 0032 | Time 0.081 (0.059) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 91.4650
Epoch 0033 | Time 0.080 (0.059) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 95.6848
Epoch 0034 | Time 0.083 (0.060) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 99.9561
Epoch 0035 | Time 0.076 (0.060) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 102.7251
Epoch 0036 | Time 0.081 (0.060) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 103.0729
Epoch 0037 | Time 0.080 (0.060) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 100.8368
Epoch 0038 | Time 0.080 (0.060) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 96.5297
Epoch 0039 | Time 0.081 (0.061) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 91.1079
Epoch 0040 | Time 0.078 (0.061) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 85.6675
Epoch 0041 | Time 0.078 (0.061) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 81.1518
Epoch 0042 | Time 0.079 (0.061) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 78.1384
Epoch 0043 | Time 0.081 (0.061) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 76.7481
Epoch 0044 | Time 0.082 (0.062) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 76.6810
Epoch 0045 | Time 0.085 (0.062) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 77.3557
Epoch 0046 | Time 0.081 (0.062) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 78.1029
Epoch 0047 | Time 0.082 (0.062) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 78.3547
Epoch 0048 | Time 0.078 (0.062) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 77.7797
Epoch 0049 | Time 0.080 (0.063) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 76.3349
Epoch 0050 | Time 0.080 (0.063) | NFE-F 19.5 | NFE-B 0.0 | Train Loss 74.2314
Epoch 0051 | Time 0.080 (0.063) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 71.8362
Epoch 0052 | Time 0.080 (0.063) | NFE-F 19.7 | NFE-B 0.0 | Train Loss 69.5456
Epoch 0053 | Time 0.080 (0.063) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 67.6701
Epoch 0054 | Time 0.083 (0.063) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 66.3599
Epoch 0055 | Time 0.078 (0.064) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 65.5875
Epoch 0056 | Time 0.080 (0.064) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 65.1824
Epoch 0057 | Time 0.081 (0.064) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 64.9032
Epoch 0058 | Time 0.080 (0.064) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 64.5181
Epoch 0059 | Time 0.080 (0.064) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 63.8717
Epoch 0060 | Time 0.079 (0.064) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 62.9186
Epoch 0061 | Time 0.079 (0.065) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 61.7186
Epoch 0062 | Time 0.078 (0.065) | NFE-F 20.5 | NFE-B 0.0 | Train Loss 60.4020
Epoch 0063 | Time 0.078 (0.065) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 59.1183
Epoch 0064 | Time 0.082 (0.065) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 57.9874
Epoch 0065 | Time 0.081 (0.065) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 57.0670
Epoch 0066 | Time 0.082 (0.065) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 56.3448
Epoch 0067 | Time 0.082 (0.065) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 55.7535
Epoch 0068 | Time 0.079 (0.066) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 55.2007
Epoch 0069 | Time 0.085 (0.066) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 54.6023
Epoch 0070 | Time 0.080 (0.066) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 53.9079
Epoch 0071 | Time 0.080 (0.066) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 53.1112
Epoch 0072 | Time 0.081 (0.066) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 52.2449
Epoch 0073 | Time 0.079 (0.066) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 51.3636
Epoch 0074 | Time 0.081 (0.067) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 50.5218
Epoch 0075 | Time 0.080 (0.067) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 49.7561
Epoch 0076 | Time 0.082 (0.067) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 49.0760
Epoch 0077 | Time 0.078 (0.067) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 48.4646
Epoch 0078 | Time 0.082 (0.067) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 47.8892
Epoch 0079 | Time 0.079 (0.067) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 47.3146
Epoch 0080 | Time 0.079 (0.067) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 46.7157
Epoch 0081 | Time 0.080 (0.067) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 46.0842
Epoch 0082 | Time 0.082 (0.068) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 45.4288
Epoch 0083 | Time 0.077 (0.068) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 44.7690
Epoch 0084 | Time 0.080 (0.068) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 44.1268
Epoch 0085 | Time 0.078 (0.068) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 43.5179
Epoch 0086 | Time 0.079 (0.068) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 42.9479
Epoch 0087 | Time 0.083 (0.068) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 42.4110
Epoch 0088 | Time 0.083 (0.068) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 41.8949
Epoch 0089 | Time 0.079 (0.068) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 41.3856
Epoch 0090 | Time 0.083 (0.069) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 40.8730
Epoch 0091 | Time 0.082 (0.069) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 40.3538
Epoch 0092 | Time 0.081 (0.069) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 39.8316
Epoch 0093 | Time 0.079 (0.069) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 39.3141
Epoch 0094 | Time 0.082 (0.069) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 38.8100
Epoch 0095 | Time 0.084 (0.069) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 38.3248
Epoch 0096 | Time 0.082 (0.069) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 37.8600
Epoch 0097 | Time 0.080 (0.069) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 37.4124
Epoch 0098 | Time 0.080 (0.070) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 36.9769
Epoch 0099 | Time 0.078 (0.070) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 36.5479
Epoch 0100 | Time 0.080 (0.070) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 36.1221
Epoch 0101 | Time 0.083 (0.070) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 35.6989
Epoch 0102 | Time 0.082 (0.070) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 35.2803
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=16, hyper_hidden=8, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=16, bias=True)
        (1): Tanh()
        (2): Linear(in_features=16, out_features=16, bias=True)
        (3): Tanh()
        (4): Linear(in_features=16, out_features=16, bias=True)
        (5): Tanh()
        (6): Linear(in_features=16, out_features=16, bias=True)
        (7): Tanh()
        (8): Linear(in_features=16, out_features=16, bias=True)
        (9): Tanh()
        (10): Linear(in_features=16, out_features=16, bias=True)
        (11): Tanh()
        (12): Linear(in_features=16, out_features=16, bias=True)
        (13): Tanh()
        (14): Linear(in_features=16, out_features=16, bias=True)
        (15): Tanh()
        (16): Linear(in_features=16, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 5044
Epoch 0000 | Time 0.054 (0.054) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 272.0935
Epoch 0001 | Time 0.082 (0.055) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 107.3123
Epoch 0002 | Time 0.082 (0.055) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 59.3279
Epoch 0003 | Time 0.081 (0.055) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 93.8254
Epoch 0004 | Time 0.082 (0.055) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 153.3035
Epoch 0005 | Time 0.078 (0.056) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 192.4687
Epoch 0006 | Time 0.082 (0.056) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 194.6104
Epoch 0007 | Time 0.077 (0.056) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 166.8757
Epoch 0008 | Time 0.085 (0.056) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 125.7683
Epoch 0009 | Time 0.082 (0.057) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 87.3394
Epoch 0010 | Time 0.086 (0.057) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 62.6671
Epoch 0011 | Time 0.090 (0.057) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 55.9704
Epoch 0012 | Time 0.082 (0.058) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 64.5377
Epoch 0013 | Time 0.083 (0.058) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 80.6607
Epoch 0014 | Time 0.079 (0.058) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 95.2968
Epoch 0015 | Time 0.080 (0.058) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 101.8747
Epoch 0016 | Time 0.083 (0.059) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 98.3540
Epoch 0017 | Time 0.082 (0.059) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 86.9560
Epoch 0018 | Time 0.080 (0.059) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 72.3826
Epoch 0019 | Time 0.081 (0.059) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 59.6514
Epoch 0020 | Time 0.081 (0.059) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 52.3055
Epoch 0021 | Time 0.078 (0.060) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 51.4024
Epoch 0022 | Time 0.087 (0.060) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 55.4744
Epoch 0023 | Time 0.081 (0.060) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 61.4221
Epoch 0024 | Time 0.082 (0.060) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 65.9548
Epoch 0025 | Time 0.076 (0.060) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 66.9279
Epoch 0026 | Time 0.083 (0.061) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 64.0079
Epoch 0027 | Time 0.080 (0.061) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 58.4937
Epoch 0028 | Time 0.081 (0.061) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 52.5260
Epoch 0029 | Time 0.083 (0.061) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 48.1057
Epoch 0030 | Time 0.085 (0.061) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 46.3188
Epoch 0031 | Time 0.080 (0.062) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 47.0276
Epoch 0032 | Time 0.079 (0.062) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 49.0986
Epoch 0033 | Time 0.079 (0.062) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 51.0213
Epoch 0034 | Time 0.081 (0.062) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 51.6100
Epoch 0035 | Time 0.082 (0.062) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 50.4661
Epoch 0036 | Time 0.086 (0.063) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 48.0276
Epoch 0037 | Time 0.078 (0.063) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 45.2448
Epoch 0038 | Time 0.079 (0.063) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 43.0823
Epoch 0039 | Time 0.083 (0.063) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 42.0880
Epoch 0040 | Time 0.085 (0.063) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 42.2079
Epoch 0041 | Time 0.083 (0.064) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 42.8994
Epoch 0042 | Time 0.081 (0.064) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 43.4536
Epoch 0043 | Time 0.083 (0.064) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 43.3514
Epoch 0044 | Time 0.082 (0.064) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 42.4787
Epoch 0045 | Time 0.079 (0.064) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 41.1166
Epoch 0046 | Time 0.086 (0.064) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 39.7448
Epoch 0047 | Time 0.082 (0.065) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 38.7783
Epoch 0048 | Time 0.083 (0.065) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 38.3750
Epoch 0049 | Time 0.080 (0.065) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 38.3967
Epoch 0050 | Time 0.080 (0.065) | NFE-F 19.5 | NFE-B 0.0 | Train Loss 38.5236
Epoch 0051 | Time 0.079 (0.065) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 38.4424
Epoch 0052 | Time 0.082 (0.065) | NFE-F 19.7 | NFE-B 0.0 | Train Loss 38.0043
Epoch 0053 | Time 0.080 (0.066) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 37.2746
Epoch 0054 | Time 0.082 (0.066) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 36.4650
Epoch 0055 | Time 0.079 (0.066) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 35.7999
Epoch 0056 | Time 0.079 (0.066) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 35.3965
Epoch 0057 | Time 0.082 (0.066) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 35.2196
Epoch 0058 | Time 0.086 (0.066) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 35.1245
Epoch 0059 | Time 0.080 (0.067) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 34.9516
Epoch 0060 | Time 0.082 (0.067) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 34.6154
Epoch 0061 | Time 0.085 (0.067) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 34.1390
Epoch 0062 | Time 0.080 (0.067) | NFE-F 20.5 | NFE-B 0.0 | Train Loss 33.6238
Epoch 0063 | Time 0.080 (0.067) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 33.1802
Epoch 0064 | Time 0.082 (0.067) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 32.8644
Epoch 0065 | Time 0.084 (0.067) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 32.6554
Epoch 0066 | Time 0.081 (0.068) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 32.4793
Epoch 0067 | Time 0.081 (0.068) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 32.2599
Epoch 0068 | Time 0.081 (0.068) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 31.9627
Epoch 0069 | Time 0.079 (0.068) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 31.6083
Epoch 0070 | Time 0.083 (0.068) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 31.2507
Epoch 0071 | Time 0.081 (0.068) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 30.9404
Epoch 0072 | Time 0.084 (0.068) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 30.6947
Epoch 0073 | Time 0.078 (0.068) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 30.4928
Epoch 0074 | Time 0.083 (0.069) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 30.2945
Epoch 0075 | Time 0.082 (0.069) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 30.0674
Epoch 0076 | Time 0.079 (0.069) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 29.8056
Epoch 0077 | Time 0.083 (0.069) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 29.5281
Epoch 0078 | Time 0.082 (0.069) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 29.2637
Epoch 0079 | Time 0.082 (0.069) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 29.0306
Epoch 0080 | Time 0.077 (0.069) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 28.8274
Epoch 0081 | Time 0.081 (0.069) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 28.6368
Epoch 0082 | Time 0.078 (0.070) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 28.4392
Epoch 0083 | Time 0.086 (0.070) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 28.2254
Epoch 0084 | Time 0.082 (0.070) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 28.0007
Epoch 0085 | Time 0.081 (0.070) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 27.7789
Epoch 0086 | Time 0.080 (0.070) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 27.5718
Epoch 0087 | Time 0.082 (0.070) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 27.3822
Epoch 0088 | Time 0.079 (0.070) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 27.2028
Epoch 0089 | Time 0.080 (0.070) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 27.0234
Epoch 0090 | Time 0.080 (0.070) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 26.8376
Epoch 0091 | Time 0.081 (0.071) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 26.6463
Epoch 0092 | Time 0.078 (0.071) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 26.4561
Epoch 0093 | Time 0.080 (0.071) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 26.2736
Epoch 0094 | Time 0.080 (0.071) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 26.1010
Epoch 0095 | Time 0.077 (0.071) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 25.9351
Epoch 0096 | Time 0.085 (0.071) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 25.7707
Epoch 0097 | Time 0.081 (0.071) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 25.6042
Epoch 0098 | Time 0.080 (0.071) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 25.4356
Epoch 0099 | Time 0.081 (0.071) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 25.2683
Epoch 0100 | Time 0.085 (0.071) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 25.1057
Epoch 0101 | Time 0.085 (0.072) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 24.9488
Epoch 0102 | Time 0.080 (0.072) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 24.7962
Epoch 0103 | Time 0.078 (0.072) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 24.6450
Epoch 0104 | Time 0.080 (0.072) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 24.4934
Epoch 0105 | Time 0.083 (0.072) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 24.3416
Epoch 0106 | Time 0.084 (0.072) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 24.1913
Epoch 0107 | Time 0.079 (0.072) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 24.0443
Epoch 0108 | Time 0.083 (0.072) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 23.9009
Epoch 0109 | Time 0.084 (0.072) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 23.7601
Epoch 0110 | Time 0.083 (0.072) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 23.6205
Epoch 0111 | Time 0.080 (0.073) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 23.4812
Epoch 0112 | Time 0.083 (0.073) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 23.3427
Epoch 0113 | Time 0.084 (0.073) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 23.2057
Epoch 0114 | Time 0.079 (0.073) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 23.0711
Epoch 0115 | Time 0.081 (0.073) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 22.9390
Epoch 0116 | Time 0.081 (0.073) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 22.8085
Epoch 0117 | Time 0.073 (0.073) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 22.6791
Epoch 0118 | Time 0.080 (0.073) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 22.5504
Epoch 0119 | Time 0.084 (0.073) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 22.4228
Epoch 0120 | Time 0.086 (0.073) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 22.2967
Epoch 0121 | Time 0.082 (0.073) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 22.1724
Epoch 0122 | Time 0.081 (0.073) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 22.0498
Epoch 0123 | Time 0.078 (0.073) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 21.9285
Epoch 0124 | Time 0.083 (0.074) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 21.8082
Epoch 0125 | Time 0.077 (0.074) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 21.6888
Epoch 0126 | Time 0.082 (0.074) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 21.5705
Epoch 0127 | Time 0.081 (0.074) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 21.4537
Epoch 0128 | Time 0.082 (0.074) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 21.3382
Epoch 0129 | Time 0.085 (0.074) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 21.2241
Epoch 0130 | Time 0.082 (0.074) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 21.1110
Epoch 0131 | Time 0.083 (0.074) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 20.9989
Epoch 0132 | Time 0.084 (0.074) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 20.8878
Epoch 0133 | Time 0.078 (0.074) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 20.7778
Epoch 0134 | Time 0.080 (0.074) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 20.6691
Epoch 0135 | Time 0.080 (0.074) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 20.5616
Epoch 0136 | Time 0.081 (0.074) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 20.4551
Epoch 0137 | Time 0.079 (0.074) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 20.3496
Epoch 0138 | Time 0.083 (0.075) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 20.2451
Epoch 0139 | Time 0.080 (0.075) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 20.1416
Epoch 0140 | Time 0.087 (0.075) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 20.0391
Epoch 0141 | Time 0.085 (0.075) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 19.9378
Epoch 0142 | Time 0.084 (0.075) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 19.8375
Epoch 0143 | Time 0.082 (0.075) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 19.7381
Epoch 0144 | Time 0.080 (0.075) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 19.6397
Epoch 0145 | Time 0.086 (0.075) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 19.5423
Epoch 0146 | Time 0.084 (0.075) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 19.4458
Epoch 0147 | Time 0.080 (0.075) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 19.3504
Epoch 0148 | Time 0.085 (0.075) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 19.2560
Epoch 0149 | Time 0.084 (0.075) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 19.1624
Epoch 0150 | Time 0.082 (0.076) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 19.0698
Epoch 0151 | Time 0.081 (0.076) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 18.9781
Epoch 0152 | Time 0.082 (0.076) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 18.8874
Epoch 0153 | Time 0.080 (0.076) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 18.7976
Epoch 0154 | Time 0.080 (0.076) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 18.7087
Epoch 0155 | Time 0.079 (0.076) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 18.6207
Epoch 0156 | Time 0.083 (0.076) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 18.5336
Epoch 0157 | Time 0.076 (0.076) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 18.4474
Epoch 0158 | Time 0.083 (0.076) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 18.3621
Epoch 0159 | Time 0.080 (0.076) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 18.2777
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=16, hyper_hidden=2, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=16, bias=True)
        (1): Tanh()
        (2): Linear(in_features=16, out_features=16, bias=True)
        (3): Tanh()
        (4): Linear(in_features=16, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 3412
Epoch 0000 | Time 0.047 (0.047) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 39.4678
Epoch 0001 | Time 0.079 (0.047) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 47.1884
Epoch 0002 | Time 0.071 (0.047) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 35.1912
Epoch 0003 | Time 0.074 (0.047) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 34.4672
Epoch 0004 | Time 0.072 (0.048) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 39.2334
Epoch 0005 | Time 0.072 (0.048) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 33.6470
Epoch 0006 | Time 0.075 (0.048) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 30.4710
Epoch 0007 | Time 0.075 (0.048) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 33.5817
Epoch 0008 | Time 0.072 (0.049) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 33.4069
Epoch 0009 | Time 0.073 (0.049) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 29.5242
Epoch 0010 | Time 0.069 (0.049) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 28.4943
Epoch 0011 | Time 0.072 (0.049) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 29.2873
Epoch 0012 | Time 0.088 (0.050) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 27.8980
Epoch 0013 | Time 0.076 (0.050) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 26.9021
Epoch 0014 | Time 0.068 (0.050) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 25.9813
Epoch 0015 | Time 0.079 (0.050) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 25.3770
Epoch 0016 | Time 0.070 (0.051) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 25.0259
Epoch 0017 | Time 0.081 (0.051) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 24.4300
Epoch 0018 | Time 0.084 (0.051) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 23.9473
Epoch 0019 | Time 0.072 (0.051) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 23.4395
Epoch 0020 | Time 0.079 (0.052) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 22.9285
Epoch 0021 | Time 0.075 (0.052) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 22.4484
Epoch 0022 | Time 0.075 (0.052) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 21.9863
Epoch 0023 | Time 0.077 (0.052) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 21.5161
Epoch 0024 | Time 0.077 (0.053) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 21.0627
Epoch 0025 | Time 0.078 (0.053) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 20.6105
Epoch 0026 | Time 0.070 (0.053) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 20.1944
Epoch 0027 | Time 0.074 (0.053) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 19.7708
Epoch 0028 | Time 0.072 (0.054) | NFE-F 17.8 | NFE-B 0.0 | Train Loss 19.3642
Epoch 0029 | Time 0.079 (0.054) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 18.9790
Epoch 0030 | Time 0.072 (0.054) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 18.6009
Epoch 0031 | Time 0.076 (0.054) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 18.2303
Epoch 0032 | Time 0.075 (0.054) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 17.8792
Epoch 0033 | Time 0.070 (0.055) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 17.5438
Epoch 0034 | Time 0.074 (0.055) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 17.2150
Epoch 0035 | Time 0.073 (0.055) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 16.8988
Epoch 0036 | Time 0.075 (0.055) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 16.6006
Epoch 0037 | Time 0.073 (0.055) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 16.3131
Epoch 0038 | Time 0.070 (0.055) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 16.0342
Epoch 0039 | Time 0.077 (0.056) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 15.7706
Epoch 0040 | Time 0.075 (0.056) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 15.5204
Epoch 0041 | Time 0.076 (0.056) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 15.2785
Epoch 0042 | Time 0.074 (0.056) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 15.0480
Epoch 0043 | Time 0.074 (0.056) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 14.8295
Epoch 0044 | Time 0.071 (0.057) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 14.4688
Epoch 0045 | Time 0.085 (0.057) | NFE-F 19.5 | NFE-B 0.0 | Train Loss 14.2223
Epoch 0046 | Time 0.084 (0.057) | NFE-F 19.7 | NFE-B 0.0 | Train Loss 13.8953
Epoch 0047 | Time 0.076 (0.057) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 14.1927
Epoch 0048 | Time 0.085 (0.058) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 13.9576
Epoch 0049 | Time 0.076 (0.058) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 13.5708
Epoch 0050 | Time 0.076 (0.058) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 13.6396
Epoch 0051 | Time 0.070 (0.058) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 13.4515
Epoch 0052 | Time 0.075 (0.058) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 13.2983
Epoch 0053 | Time 0.075 (0.058) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 13.2764
Epoch 0054 | Time 0.075 (0.059) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 13.1749
Epoch 0055 | Time 0.073 (0.059) | NFE-F 20.5 | NFE-B 0.0 | Train Loss 12.9442
Epoch 0056 | Time 0.093 (0.059) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 12.9023
Epoch 0057 | Time 0.076 (0.059) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 12.8876
Epoch 0058 | Time 0.077 (0.059) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 12.1900
/home/tudockho/Documents/torchdiffeq/examples/odenet_boston.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from sklearn import datasets
from sklearn import preprocessing

parser = argparse.ArgumentParser()
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval,
                    default=False, choices=[True, False])
parser.add_argument('--nepochs', type=int, default=160)
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)

parser.add_argument('--save', type=str, default='./experiment1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)

parser.add_argument('--hyper_dim', type=int, default=8)
parser.add_argument('--hyper_hidden', type=int, default=8)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


class ODEfunc(nn.Module):

    def __init__(self, dim, hypernet_dim, hypernet_hidden_layers, activation=nn.Tanh):
        super(ODEfunc, self).__init__()
        self.dim = dim
        self.params_dim = self.dim**2 + self.dim
        self.activation = activation

        print('Number of outputs in hypernet:', self.params_dim)

        layers = []
        dims = [1] + [hypernet_dim] * \
            hypernet_hidden_layers + [self.params_dim]

        for i in range(1, len(dims)):
            layers.append(nn.Linear(dims[i - 1], dims[i]))
            if i < len(dims) - 1:
                layers.append(self.activation())

        self._hypernet = nn.Sequential(*layers)
        self._hypernet.apply(weights_init)

        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1

        params = self._hypernet(t.view(1, 1)).view(-1)
        b = params[:self.dim].view(self.dim)
        w = params[self.dim:].view(self.dim, self.dim)

        out = 0.5*(nn.functional.linear(x, w, b) + nn.functional.linear(x, -w.t(), b))
        # out = nn.functional.linear(x, w, b)

        return nn.functional.tanh(out)



class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time,
                     rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


class BostonHousingTrain:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[:self.n_examples, :]
        self.y = y[:self.n_examples]

    def __len__(self):
        return self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


class BostonHousingTest:
    def __init__(self, n_examples):
        self.n_examples = n_examples

        X, y = datasets.load_boston(return_X_y=True)

        scaler = preprocessing.StandardScaler().fit(X)
        self.X = scaler.transform(X)

        self.X = X[self.n_examples:, :]
        self.y = y[self.n_examples:]

    def __len__(self):
        return 506 - self.n_examples

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])


def get_boston_housing_loaders(batch_size=128, test_train_split=16):

    train_loader = DataLoader(BostonHousingTrain(test_train_split),
                              batch_size=batch_size, shuffle=True,
                              num_workers=1, drop_last=False)

    test_loader = DataLoader(BostonHousingTest(test_train_split),
                             batch_size=batch_size, shuffle=True,
                             num_workers=1, drop_last=False)

    return train_loader, test_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def compute_loss(loss_func, model, dataset_loader):
	for x, y in dataset_loader:
		x = x.float().to(device)
		y = y.float().to(device)

	return loss_func(torch.flatten(model(x)), y)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.normal_(m.bias, 0, 0.01)


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(
        args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu)
                          if torch.cuda.is_available() else 'cpu')

    feature_layers = [ODEBlock(ODEfunc(13, args.hyper_dim, args.hyper_hidden))]
    fc_layers = [nn.Linear(13, 1)]

    model = nn.Sequential(*feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.MSELoss().to(device)

    train_loader, test_loader = get_boston_housing_loaders(args.batch_size)

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    # lr_fn = learning_rate_with_decay(
    #     args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
    #     decay_rates=[1, 0.1, 0.01, 0.001]
    # )

    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()

        x = x.float().to(device)
        y = y.float().to(device)
        loss = criterion(torch.flatten(model(x)), y)

        nfe_forward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        nfe_backward = feature_layers[0].nfe
        feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_loss = compute_loss(criterion, model, train_loader)
                # test_loss = compute_loss(criterion, model, test_loader)
                # logger.info(
                #     "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                #     "Train Loss {:.4f} | Test Loss {:.4f}".format(
                #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                #         b_nfe_meter.avg, train_loss, test_loss
                #     )
                # )

                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Loss {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_loss
                    )
                )

Namespace(adjoint=False, batch_size=128, debug=False, gpu=0, hyper_dim=256, hyper_hidden=2, lr=0.01, nepochs=160, save='./experiment1', tol=0.001)
Sequential(
  (0): ODEBlock(
    (odefunc): ODEfunc(
      (_hypernet): Sequential(
        (0): Linear(in_features=1, out_features=256, bias=True)
        (1): Tanh()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): Tanh()
        (4): Linear(in_features=256, out_features=182, bias=True)
      )
    )
  )
  (1): Linear(in_features=13, out_features=1, bias=True)
)
Number of parameters: 113092
Epoch 0000 | Time 0.056 (0.056) | NFE-F 14.0 | NFE-B 0.0 | Train Loss 66.7609
Epoch 0001 | Time 0.081 (0.056) | NFE-F 14.1 | NFE-B 0.0 | Train Loss 91.0837
Epoch 0002 | Time 0.080 (0.057) | NFE-F 14.3 | NFE-B 0.0 | Train Loss 79.3701
Epoch 0003 | Time 0.079 (0.057) | NFE-F 14.4 | NFE-B 0.0 | Train Loss 61.0886
Epoch 0004 | Time 0.078 (0.057) | NFE-F 14.6 | NFE-B 0.0 | Train Loss 59.0491
Epoch 0005 | Time 0.080 (0.057) | NFE-F 14.7 | NFE-B 0.0 | Train Loss 67.4426
Epoch 0006 | Time 0.079 (0.057) | NFE-F 14.8 | NFE-B 0.0 | Train Loss 69.2444
Epoch 0007 | Time 0.075 (0.058) | NFE-F 15.0 | NFE-B 0.0 | Train Loss 61.6205
Epoch 0008 | Time 0.077 (0.058) | NFE-F 15.1 | NFE-B 0.0 | Train Loss 53.3317
Epoch 0009 | Time 0.077 (0.058) | NFE-F 15.2 | NFE-B 0.0 | Train Loss 51.4295
Epoch 0010 | Time 0.077 (0.058) | NFE-F 15.3 | NFE-B 0.0 | Train Loss 54.6047
Epoch 0011 | Time 0.077 (0.058) | NFE-F 15.5 | NFE-B 0.0 | Train Loss 56.3178
Epoch 0012 | Time 0.082 (0.059) | NFE-F 15.6 | NFE-B 0.0 | Train Loss 53.2154
Epoch 0013 | Time 0.081 (0.059) | NFE-F 15.7 | NFE-B 0.0 | Train Loss 47.8840
Epoch 0014 | Time 0.077 (0.059) | NFE-F 15.8 | NFE-B 0.0 | Train Loss 44.6766
Epoch 0015 | Time 0.079 (0.059) | NFE-F 16.0 | NFE-B 0.0 | Train Loss 44.9811
Epoch 0016 | Time 0.082 (0.059) | NFE-F 16.1 | NFE-B 0.0 | Train Loss 46.2941
Epoch 0017 | Time 0.082 (0.060) | NFE-F 16.2 | NFE-B 0.0 | Train Loss 45.5470
Epoch 0018 | Time 0.078 (0.060) | NFE-F 16.3 | NFE-B 0.0 | Train Loss 42.5180
Epoch 0019 | Time 0.079 (0.060) | NFE-F 16.4 | NFE-B 0.0 | Train Loss 39.4926
Epoch 0020 | Time 0.078 (0.060) | NFE-F 16.5 | NFE-B 0.0 | Train Loss 38.4018
Epoch 0021 | Time 0.077 (0.060) | NFE-F 16.7 | NFE-B 0.0 | Train Loss 38.8376
Epoch 0022 | Time 0.077 (0.060) | NFE-F 16.8 | NFE-B 0.0 | Train Loss 38.8475
Epoch 0023 | Time 0.078 (0.061) | NFE-F 16.9 | NFE-B 0.0 | Train Loss 37.3366
Epoch 0024 | Time 0.077 (0.061) | NFE-F 17.0 | NFE-B 0.0 | Train Loss 35.1271
Epoch 0025 | Time 0.079 (0.061) | NFE-F 17.1 | NFE-B 0.0 | Train Loss 33.7226
Epoch 0026 | Time 0.078 (0.061) | NFE-F 17.2 | NFE-B 0.0 | Train Loss 33.5351
Epoch 0027 | Time 0.078 (0.061) | NFE-F 17.3 | NFE-B 0.0 | Train Loss 33.5820
Epoch 0028 | Time 0.079 (0.062) | NFE-F 17.4 | NFE-B 0.0 | Train Loss 32.8215
Epoch 0029 | Time 0.078 (0.062) | NFE-F 17.5 | NFE-B 0.0 | Train Loss 31.3671
Epoch 0030 | Time 0.077 (0.062) | NFE-F 17.6 | NFE-B 0.0 | Train Loss 30.1667
Epoch 0031 | Time 0.078 (0.062) | NFE-F 17.7 | NFE-B 0.0 | Train Loss 29.7627
Epoch 0032 | Time 0.078 (0.062) | NFE-F 17.9 | NFE-B 0.0 | Train Loss 29.7093
Epoch 0033 | Time 0.078 (0.062) | NFE-F 18.0 | NFE-B 0.0 | Train Loss 29.2541
Epoch 0034 | Time 0.077 (0.062) | NFE-F 18.1 | NFE-B 0.0 | Train Loss 28.2963
Epoch 0035 | Time 0.075 (0.063) | NFE-F 18.2 | NFE-B 0.0 | Train Loss 27.4052
Epoch 0036 | Time 0.078 (0.063) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 27.0130
Epoch 0037 | Time 0.077 (0.063) | NFE-F 18.3 | NFE-B 0.0 | Train Loss 26.8969
Epoch 0038 | Time 0.077 (0.063) | NFE-F 18.4 | NFE-B 0.0 | Train Loss 26.5566
Epoch 0039 | Time 0.075 (0.063) | NFE-F 18.5 | NFE-B 0.0 | Train Loss 25.8920
Epoch 0040 | Time 0.076 (0.063) | NFE-F 18.6 | NFE-B 0.0 | Train Loss 25.2663
Epoch 0041 | Time 0.076 (0.063) | NFE-F 18.7 | NFE-B 0.0 | Train Loss 24.9626
Epoch 0042 | Time 0.079 (0.064) | NFE-F 18.8 | NFE-B 0.0 | Train Loss 24.8245
Epoch 0043 | Time 0.078 (0.064) | NFE-F 18.9 | NFE-B 0.0 | Train Loss 24.5285
Epoch 0044 | Time 0.078 (0.064) | NFE-F 19.0 | NFE-B 0.0 | Train Loss 24.0390
Epoch 0045 | Time 0.068 (0.064) | NFE-F 19.1 | NFE-B 0.0 | Train Loss 23.6084
Epoch 0046 | Time 0.080 (0.064) | NFE-F 19.2 | NFE-B 0.0 | Train Loss 23.3888
Epoch 0047 | Time 0.074 (0.064) | NFE-F 19.3 | NFE-B 0.0 | Train Loss 23.2385
Epoch 0048 | Time 0.080 (0.064) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 22.9617
Epoch 0049 | Time 0.078 (0.064) | NFE-F 19.4 | NFE-B 0.0 | Train Loss 22.5878
Epoch 0050 | Time 0.081 (0.065) | NFE-F 19.5 | NFE-B 0.0 | Train Loss 22.2928
Epoch 0051 | Time 0.080 (0.065) | NFE-F 19.6 | NFE-B 0.0 | Train Loss 22.1261
Epoch 0052 | Time 0.076 (0.065) | NFE-F 19.7 | NFE-B 0.0 | Train Loss 21.9594
Epoch 0053 | Time 0.080 (0.065) | NFE-F 19.8 | NFE-B 0.0 | Train Loss 21.6977
Epoch 0054 | Time 0.078 (0.065) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 21.4107
Epoch 0055 | Time 0.078 (0.065) | NFE-F 19.9 | NFE-B 0.0 | Train Loss 21.2034
Epoch 0056 | Time 0.076 (0.065) | NFE-F 20.0 | NFE-B 0.0 | Train Loss 21.0555
Epoch 0057 | Time 0.079 (0.066) | NFE-F 20.1 | NFE-B 0.0 | Train Loss 20.8708
Epoch 0058 | Time 0.080 (0.066) | NFE-F 20.2 | NFE-B 0.0 | Train Loss 20.6324
Epoch 0059 | Time 0.077 (0.066) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 20.4142
Epoch 0060 | Time 0.076 (0.066) | NFE-F 20.3 | NFE-B 0.0 | Train Loss 20.2540
Epoch 0061 | Time 0.073 (0.066) | NFE-F 20.4 | NFE-B 0.0 | Train Loss 20.1014
Epoch 0062 | Time 0.075 (0.066) | NFE-F 20.5 | NFE-B 0.0 | Train Loss 19.9089
Epoch 0063 | Time 0.075 (0.066) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 19.7046
Epoch 0064 | Time 0.079 (0.066) | NFE-F 20.6 | NFE-B 0.0 | Train Loss 19.5356
Epoch 0065 | Time 0.079 (0.066) | NFE-F 20.7 | NFE-B 0.0 | Train Loss 19.3909
Epoch 0066 | Time 0.079 (0.067) | NFE-F 20.8 | NFE-B 0.0 | Train Loss 19.2282
Epoch 0067 | Time 0.081 (0.067) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 19.0458
Epoch 0068 | Time 0.081 (0.067) | NFE-F 20.9 | NFE-B 0.0 | Train Loss 18.8785
Epoch 0069 | Time 0.082 (0.067) | NFE-F 21.0 | NFE-B 0.0 | Train Loss 18.7354
Epoch 0070 | Time 0.082 (0.067) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 18.5896
Epoch 0071 | Time 0.078 (0.067) | NFE-F 21.1 | NFE-B 0.0 | Train Loss 18.4277
Epoch 0072 | Time 0.079 (0.067) | NFE-F 21.2 | NFE-B 0.0 | Train Loss 18.2694
Epoch 0073 | Time 0.077 (0.067) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 18.1296
Epoch 0074 | Time 0.078 (0.068) | NFE-F 21.3 | NFE-B 0.0 | Train Loss 17.9943
Epoch 0075 | Time 0.079 (0.068) | NFE-F 21.4 | NFE-B 0.0 | Train Loss 17.8489
Epoch 0076 | Time 0.080 (0.068) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 17.7026
Epoch 0077 | Time 0.081 (0.068) | NFE-F 21.5 | NFE-B 0.0 | Train Loss 17.5689
Epoch 0078 | Time 0.075 (0.068) | NFE-F 21.6 | NFE-B 0.0 | Train Loss 17.4420
Epoch 0079 | Time 0.076 (0.068) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 17.3100
Epoch 0080 | Time 0.082 (0.068) | NFE-F 21.7 | NFE-B 0.0 | Train Loss 17.1762
Epoch 0081 | Time 0.080 (0.068) | NFE-F 21.8 | NFE-B 0.0 | Train Loss 17.0508
Epoch 0082 | Time 0.079 (0.068) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 16.9321
Epoch 0083 | Time 0.084 (0.069) | NFE-F 21.9 | NFE-B 0.0 | Train Loss 16.8114
Epoch 0084 | Time 0.076 (0.069) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 16.6894
Epoch 0085 | Time 0.080 (0.069) | NFE-F 22.0 | NFE-B 0.0 | Train Loss 16.5733
Epoch 0086 | Time 0.079 (0.069) | NFE-F 22.1 | NFE-B 0.0 | Train Loss 16.4630
Epoch 0087 | Time 0.081 (0.069) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 16.3524
Epoch 0088 | Time 0.080 (0.069) | NFE-F 22.2 | NFE-B 0.0 | Train Loss 16.2413
Epoch 0089 | Time 0.080 (0.069) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 16.1346
Epoch 0090 | Time 0.078 (0.069) | NFE-F 22.3 | NFE-B 0.0 | Train Loss 16.0327
Epoch 0091 | Time 0.080 (0.069) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 15.9313
Epoch 0092 | Time 0.076 (0.069) | NFE-F 22.4 | NFE-B 0.0 | Train Loss 15.8302
Epoch 0093 | Time 0.076 (0.070) | NFE-F 22.5 | NFE-B 0.0 | Train Loss 15.7326
Epoch 0094 | Time 0.076 (0.070) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 15.6389
Epoch 0095 | Time 0.079 (0.070) | NFE-F 22.6 | NFE-B 0.0 | Train Loss 15.5460
Epoch 0096 | Time 0.078 (0.070) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 15.4539
Epoch 0097 | Time 0.080 (0.070) | NFE-F 22.7 | NFE-B 0.0 | Train Loss 15.3648
Epoch 0098 | Time 0.075 (0.070) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 15.2788
Epoch 0099 | Time 0.074 (0.070) | NFE-F 22.8 | NFE-B 0.0 | Train Loss 15.1937
Epoch 0100 | Time 0.081 (0.070) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 15.1098
Epoch 0101 | Time 0.082 (0.070) | NFE-F 22.9 | NFE-B 0.0 | Train Loss 15.0285
Epoch 0102 | Time 0.083 (0.070) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 14.9496
Epoch 0103 | Time 0.077 (0.070) | NFE-F 23.0 | NFE-B 0.0 | Train Loss 14.8718
Epoch 0104 | Time 0.078 (0.070) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 14.7953
Epoch 0105 | Time 0.081 (0.071) | NFE-F 23.1 | NFE-B 0.0 | Train Loss 14.7211
Epoch 0106 | Time 0.085 (0.071) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 14.6488
Epoch 0107 | Time 0.079 (0.071) | NFE-F 23.2 | NFE-B 0.0 | Train Loss 14.5776
Epoch 0108 | Time 0.076 (0.071) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 14.5078
Epoch 0109 | Time 0.077 (0.071) | NFE-F 23.3 | NFE-B 0.0 | Train Loss 14.4400
Epoch 0110 | Time 0.081 (0.071) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 14.3738
Epoch 0111 | Time 0.078 (0.071) | NFE-F 23.4 | NFE-B 0.0 | Train Loss 14.3087
Epoch 0112 | Time 0.078 (0.071) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 14.2450
Epoch 0113 | Time 0.079 (0.071) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 14.1830
Epoch 0114 | Time 0.079 (0.071) | NFE-F 23.5 | NFE-B 0.0 | Train Loss 14.1223
Epoch 0115 | Time 0.082 (0.071) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 14.0627
Epoch 0116 | Time 0.084 (0.072) | NFE-F 23.6 | NFE-B 0.0 | Train Loss 14.0045
Epoch 0117 | Time 0.083 (0.072) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 13.9477
Epoch 0118 | Time 0.081 (0.072) | NFE-F 23.7 | NFE-B 0.0 | Train Loss 13.8921
Epoch 0119 | Time 0.081 (0.072) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 13.8376
Epoch 0120 | Time 0.079 (0.072) | NFE-F 23.8 | NFE-B 0.0 | Train Loss 13.7844
Epoch 0121 | Time 0.076 (0.072) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 13.7324
Epoch 0122 | Time 0.082 (0.072) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 13.6815
Epoch 0123 | Time 0.079 (0.072) | NFE-F 23.9 | NFE-B 0.0 | Train Loss 13.6316
Epoch 0124 | Time 0.079 (0.072) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 13.5829
Epoch 0125 | Time 0.077 (0.072) | NFE-F 24.0 | NFE-B 0.0 | Train Loss 13.5353
Epoch 0126 | Time 0.080 (0.072) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 13.4886
Epoch 0127 | Time 0.079 (0.072) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 13.4430
Epoch 0128 | Time 0.081 (0.072) | NFE-F 24.1 | NFE-B 0.0 | Train Loss 13.3984
Epoch 0129 | Time 0.082 (0.073) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 13.3547
Epoch 0130 | Time 0.077 (0.073) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 13.3119
Epoch 0131 | Time 0.078 (0.073) | NFE-F 24.2 | NFE-B 0.0 | Train Loss 13.2701
Epoch 0132 | Time 0.080 (0.073) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 13.2292
Epoch 0133 | Time 0.078 (0.073) | NFE-F 24.3 | NFE-B 0.0 | Train Loss 13.1891
Epoch 0134 | Time 0.080 (0.073) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 13.1499
Epoch 0135 | Time 0.078 (0.073) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 13.1116
Epoch 0136 | Time 0.079 (0.073) | NFE-F 24.4 | NFE-B 0.0 | Train Loss 13.0740
Epoch 0137 | Time 0.083 (0.073) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 13.0373
Epoch 0138 | Time 0.080 (0.073) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 13.0013
Epoch 0139 | Time 0.079 (0.073) | NFE-F 24.5 | NFE-B 0.0 | Train Loss 12.9661
Epoch 0140 | Time 0.079 (0.073) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 12.9316
Epoch 0141 | Time 0.077 (0.073) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 12.8979
Epoch 0142 | Time 0.078 (0.073) | NFE-F 24.6 | NFE-B 0.0 | Train Loss 12.8648
Epoch 0143 | Time 0.078 (0.073) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 12.8325
Epoch 0144 | Time 0.078 (0.073) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 12.8008
Epoch 0145 | Time 0.073 (0.073) | NFE-F 24.7 | NFE-B 0.0 | Train Loss 12.7697
Epoch 0146 | Time 0.081 (0.074) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 12.7393
Epoch 0147 | Time 0.076 (0.074) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 12.7095
Epoch 0148 | Time 0.077 (0.074) | NFE-F 24.8 | NFE-B 0.0 | Train Loss 12.6804
Epoch 0149 | Time 0.076 (0.074) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 12.6518
Epoch 0150 | Time 0.077 (0.074) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 12.6238
Epoch 0151 | Time 0.076 (0.074) | NFE-F 24.9 | NFE-B 0.0 | Train Loss 12.5964
Epoch 0152 | Time 0.075 (0.074) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 12.5695
Epoch 0153 | Time 0.078 (0.074) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 12.5432
Epoch 0154 | Time 0.074 (0.074) | NFE-F 25.0 | NFE-B 0.0 | Train Loss 12.5173
Epoch 0155 | Time 0.079 (0.074) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 12.4920
Epoch 0156 | Time 0.076 (0.074) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 12.4672
Epoch 0157 | Time 0.083 (0.074) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 12.4429
Epoch 0158 | Time 0.076 (0.074) | NFE-F 25.1 | NFE-B 0.0 | Train Loss 12.4191
Epoch 0159 | Time 0.078 (0.074) | NFE-F 25.2 | NFE-B 0.0 | Train Loss 12.3957
